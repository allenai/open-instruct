# MFU (Model FLOPs Utilization) Support for grpo_fast

This document describes the MFU (Model FLOPs Utilization) functionality that has been added to the `grpo_fast.py` training script.

## Overview

MFU is a metric that measures how efficiently a model is utilizing the available computational resources. It is calculated as:

```
MFU = (Actual FLOPs per second) / (Theoretical Peak FLOPs per second) Ã— 100%
```

## Features Added

### 1. FLOPs Calculation
- **Function**: `calculate_model_flops_per_token(model, tokenizer)`
- **Purpose**: Calculates the exact FLOPs for a single token forward pass using torch's flops counter
- **Dependencies**: Requires `fvcore` library for FLOPs counting

### 2. MFU Calculation
- **Function**: `calculate_mfu(flops_per_token, tokens_per_second, model_name)`
- **Purpose**: Calculates MFU percentage based on FLOPs per token and tokens per second
- **Model Size Detection**: Automatically detects model size from model name (7b, 8b, 13b, 32b, 70b)
- **Theoretical Peak FLOPs**: Uses A100 GPU theoretical peak of 312 TFLOPS

### 3. Token Tracking
- **Location**: `vllm_generate_thread` function
- **Purpose**: Tracks tokens generated by actors for MFU calculation
- **Storage**: Tokens are stored in a shared list for periodic MFU calculation

### 4. MFU Logging
- **Location**: `one_training_step` function
- **Metrics Logged**:
  - `mfu/tokens_per_second`: Tokens generated per second by actors
  - `mfu/flops_per_token`: FLOPs per token for the model
  - `mfu/mfu_percentage`: Calculated MFU percentage
  - `mfu/total_tokens_generated`: Total tokens generated in the period
  - `mfu/elapsed_time`: Time elapsed for the calculation period

## Configuration

### New Arguments Added to Args Class

```python
# MFU (Model FLOPs Utilization) settings
enable_mfu_tracking: bool = True
"""Whether to enable MFU tracking"""
mfu_calculation_freq: int = 10
"""How often to calculate MFU (in training steps)"""
```

### Usage

1. **Enable MFU Tracking** (default: True):
   ```bash
   python open_instruct/grpo_fast.py --enable_mfu_tracking True
   ```

2. **Disable MFU Tracking**:
   ```bash
   python open_instruct/grpo_fast.py --enable_mfu_tracking False
   ```

3. **Set MFU Calculation Frequency**:
   ```bash
   python open_instruct/grpo_fast.py --mfu_calculation_freq 5
   ```

## Dependencies

### Required Package
- `fvcore>=0.1.5.post20221221`: For FLOPs counting functionality

### Installation
```bash
pip install fvcore>=0.1.5.post20221221
```

## Implementation Details

### 1. FLOPs Calculation Process
1. Creates a dummy input tensor with shape `(1, 1)` (single token)
2. Uses `FlopCountMode` from fvcore to count FLOPs
3. Performs a forward pass with the dummy input
4. Extracts total FLOPs from the flop counter

### 2. Token Tracking Process
1. In `vllm_generate_thread`, after each generation:
   - Counts total tokens generated across all responses
   - Appends to shared `mfu_tokens_generated` list
2. In `one_training_step`, periodically:
   - Calculates tokens per second from accumulated tokens
   - Computes MFU using the calculated FLOPs per token
   - Logs metrics to wandb
   - Clears the token list for next period

### 3. Model Size Detection
The system automatically detects model size from the model name:
- Searches for patterns: "70b", "32b", "13b", "8b", "7b"
- Defaults to "7b" if no size is detected
- Uses appropriate theoretical peak FLOPs for the detected size

## Example Output

When MFU tracking is enabled, you'll see logs like:
```
Calculating FLOPs per token for MFU tracking...
FLOPs per token: 1234567890.0
```

And wandb metrics like:
```
mfu/tokens_per_second: 150.5
mfu/flops_per_token: 1234567890.0
mfu/mfu_percentage: 45.2
mfu/total_tokens_generated: 1505
mfu/elapsed_time: 10.0
```

## Troubleshooting

### Common Issues

1. **fvcore not available**:
   ```
   fvcore not available, MFU calculation will be disabled
   ```
   - **Solution**: Install fvcore: `pip install fvcore>=0.1.5.post20221221`

2. **Model size not detected**:
   ```
   Could not determine model size from model_name, using 7b as default
   ```
   - **Solution**: Ensure model name contains size information (e.g., "llama-7b", "qwen-13b")

3. **Zero FLOPs per token**:
   - **Cause**: Model not properly loaded or device mismatch
   - **Solution**: Check model loading and device placement

## Testing

Run the test script to verify MFU functionality:
```bash
python test_mfu.py
```

## Notes

- MFU calculation is performed on the first model in the policy group
- FLOPs calculation is done once at the start of training
- Token tracking is thread-safe using a shared list
- MFU metrics are logged to wandb with the "mfu/" prefix
- The system gracefully handles missing dependencies by disabling MFU tracking