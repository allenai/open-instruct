{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Open Instruct","text":"<p>This repo serves as an open effort on instruction-tuning and post-training popular pretrained language models on publicly available datasets. We release this repo and will keep updating it with:</p> <ol> <li>Code for finetuning language models with latest techniques and instruction datasets in a unified format.</li> <li>Code for DPO, preference finetuning and reinforcement learning with verifiable rewards (RLVR).</li> <li>Checkpoints or other useful artifacts that we build in our exploration.</li> </ol> <p>We also support some evaluations natively in the codebase, but these are now unmaintained and instead we suggest using OLMES, which we used for T\u00dcLU 3. Below are some of our papers:</p> <ul> <li> <p>T\u00dcLU 3: Pushing Frontiers in Open Language Model Post-Training</p> <ul> <li>Latest research on open post-training techniques and methodologies</li> <li>Comprehensive details on our most recent model training approaches</li> </ul> </li> <li> <p>How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources</p> <ul> <li>Our first paper introducing the project's foundation and vision</li> <li>Presents initial findings and exploration of instruction tuning with open resources</li> </ul> </li> <li> <p>Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2</p> <ul> <li>Second paper focusing on Llama-2 model adaptations</li> <li>Details our work with direct preference optimization (DPO) techniques</li> </ul> </li> <li> <p>Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback</p> <ul> <li>Most recent research comparing reinforcement learning approaches</li> <li>Analyzes best practices for both DPO and PPO methodologies</li> </ul> </li> </ul> <p> </p> <p>Try some of the models we train with Open Instruct. There is a free demo or download them from HuggingFace:</p> Stage Llama 3.1 8B Llama 3.1 70B OLMo-2 7B OLMo-2 13B Base Model meta-llama/Llama-3.1-8B meta-llama/Llama-3.1-70B allenai/OLMo2-7B-1124 allenai/OLMo-2-13B-1124 SFT allenai/Llama-3.1-Tulu-3-8B-SFT allenai/Llama-3.1-Tulu-3-70B-SFT allenai/OLMo-2-1124-7B-SFT allenai/OLMo-2-1124-13B-SFT DPO allenai/Llama-3.1-Tulu-3-8B-DPO allenai/Llama-3.1-Tulu-3-70B-DPO allenai/OLMo-2-1124-7B-DPO allenai/OLMo-2-1124-13B-DPO Final Models (RLVR) allenai/Llama-3.1-Tulu-3-8B allenai/Llama-3.1-Tulu-3-70B allenai/OLMo-2-1124-7B-Instruct allenai/OLMo-2-1124-13B-Instruct Final Models (RLVR) (\ud83d\udd25 New, trained with GRPO) allenai/Llama-3.1-Tulu-3.1-8B Reward Model (RM) allenai/Llama-3.1-Tulu-3-8B-RM (Same as 8B) allenai/OLMo-2-1124-7B-RM (Same as 7B)"},{"location":"#news","title":"News","text":"<ul> <li>[2025-02-12] We released the <code>allenai/Llama-3.1-Tulu-3.1-8B</code> model, which is trained with our GRPO recipe and outperforms the old <code>allenai/Llama-3.1-Tulu-3-8B</code> model in almost all of our evals.</li> <li>[2024-11-22] We released T\u00dcLU 3: Pushing Frontiers in Open Language Model Post-Training and updated our entire stack of open post-training recipes with both Llama 3.1 and OLMo 2.</li> <li>[2024-07-01] We released Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback and have majorly updated our codebase to support new models and package versions.</li> <li>[2023-11-27] We released Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2. Check out our models here. We have added a DPO finetuning script for replicating our results.</li> <li>[2023-09-26] We switched to use the official alpaca-eval library to run AlpacaFarm evaluation but use regenerated longer reference outputs. This will change our numbers reported in the paper. We will update the paper soon.</li> <li>[2023-09-25] Supported using vLLM for our evaluations, which speeds up the evaluation by 10x.</li> <li>[2023-09-17] Supported LoRA and QLoRA finetuning. See here for more details.</li> <li>[2023-08-18] Added support for ToxiGen/TruthfulQA evaluation. Check our <code>scripts/eval/</code> for examples of running them.</li> <li>[2023-08-08] Supported several new instruction dataset, including LIMA / WizardLM / Open-Orca. See the preparation script for details. Performance hasn't been evaluated yet.</li> <li>[2023-08-06] Supported LLaMa 2 finetuning and FlashAttention-2 by bumping the version of transformers and many other dependencies.</li> <li>[2023-06-29] Added licensing info for our released models.</li> <li>[2023-06-09] Released T\u00fclu (a suite of LLaMa models fully-finetuned on a strong mix of datasets) and many other checkpoints on HuggingFace [Links].</li> <li>[2023-06-09] Initial release of the codebase containing the training and evaluation code for our arxiv paper.</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you used this repository or our models, please cite our work:</p> <p>Tulu 1: <pre><code>@misc{wang2023far,\n   title={How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources}, \n   author={Yizhong Wang and Hamish Ivison and Pradeep Dasigi and Jack Hessel and Tushar Khot and Khyathi Raghavi Chandu and David Wadden and Kelsey MacMillan and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},\n   year={2023},\n   eprint={2306.04751},\n   archivePrefix={arXiv},\n   primaryClass={cs.CL}\n}\n</code></pre></p> <p>Tulu 2: <pre><code>@misc{ivison2023camels,\n      title={Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2}, \n      author={Hamish Ivison and Yizhong Wang and Valentina Pyatkin and Nathan Lambert and Matthew Peters and Pradeep Dasigi and Joel Jang and David Wadden and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},\n      year={2023},\n      eprint={2311.10702},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre></p> <p>Tulu 2.5: <pre><code>@misc{ivison2024unpacking,\n      title={Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback}, \n      author={Hamish Ivison and Yizhong Wang and Jiacheng Liu and Zeqiu Wu and Valentina Pyatkin and Nathan Lambert and Noah A. Smith and Yejin Choi and Hannaneh Hajishirzi},\n      year={2024},\n      eprint={2406.09279},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n}\n</code></pre></p> <p>Tulu 3: <pre><code>@article{lambert2024tulu3,\n  title = {T\u00fclu 3: Pushing Frontiers in Open Language Model Post-Training},\n  author = {\n    Nathan Lambert and Jacob Morrison and Valentina Pyatkin and Shengyi Huang and Hamish Ivison and Faeze Brahman and Lester James V. Miranda and Alisa Liu and Nouha Dziri and Shane Lyu and Yuling Gu and Saumya Malik and Victoria Graf and Jena D. Hwang and Jiangjiang Yang and Ronan Le Bras and Oyvind Tafjord and Chris Wilhelm and Luca Soldaini and Noah A. Smith and Yizhong Wang and Pradeep Dasigi and Hannaneh Hajishirzi\n  },\n  year = {2024},\n  email = {tulu@allenai.org}\n}\n</code></pre></p>"},{"location":"ai2_internal/","title":"Ai2 internal","text":"<p>Deprecated. Check out get_started/ai2_internal_setup.md instead.</p>"},{"location":"olmo2/","title":"OLMo 2 Commands","text":"<p>Here we'll add commands and references to the training runs of OLMo 2.  We'll prioritize the smaller models where more people are hoping to study and reproduce them.</p> <p>Core to training OLMo models (version 1 and 2) at least are to include the following flags: <code>--add_bos</code> and <code>--use_slow_tokenizer False</code> because of the tokenizer used.</p> <p>For more details on how to convert these to standard launch commands (without ai2 <code>mason.py</code>) see the <code>tulu3.md</code> docs.</p>"},{"location":"olmo2/#insturction-finetuning","title":"Insturction Finetuning","text":""},{"location":"olmo2/#1b","title":"1B","text":"<p>We ran training for the 1B model in SFT on 1 node of 8 NVIDIA H100 GPUs.</p> <p>The command used internally is: <pre><code>python mason.py \\\n    --cluster ai2/augusta \\\n    --workspace ai2/olmo-instruct \\\n    --priority high \\\n    --image nathanl/open_instruct_auto --pure_docker_mode \\\n    --preemptible \\\n    --num_nodes 1 \\\n    --budget ai2/oe-adapt \\\n    --gpus 8 -- accelerate launch \\\n    --mixed_precision bf16 \\\n    --num_processes 8 \\\n    --use_deepspeed \\\n    --deepspeed_config_file configs/ds_configs/stage3_no_offloading_accelerate.conf \\\n    --deepspeed_multinode_launcher standard \\\n    open_instruct/finetune.py \\\n    --exp_name olmo2_1b_sft \\\n    --model_name_or_path allenai/OLMo-2-0425-1B \\\n    --model_revision main \\\n    --tokenizer_name allenai/OLMo-2-1124-7B \\\n    --tokenizer_revision main \\\n    --use_slow_tokenizer False \\\n    --add_bos \\\n    --dataset_mixer_list allenai/tulu-3-sft-olmo-2-mixture-0225 1.0 \\\n    --use_flash_attn \\\n    --max_seq_length 4096 \\\n    --per_device_train_batch_size 2 \\\n    --gradient_accumulation_steps 8 \\\n    --learning_rate 3e-5 \\\n    --lr_scheduler_type linear \\\n    --warmup_ratio 0.03 \\\n    --weight_decay 0.0 \\\n    --num_train_epochs 2 \\\n    --report_to wandb \\\n    --with_tracking \\\n    --logging_steps 1 \\\n    --seed 1\n</code></pre> Which reduces to roughly: <pre><code>accelerate launch \\\n    --mixed_precision bf16 \\\n    --num_processes 8 \\\n    --use_deepspeed \\\n    --deepspeed_config_file configs/ds_configs/stage3_no_offloading_accelerate.conf \\\n    --deepspeed_multinode_launcher standard \\\n    open_instruct/finetune.py \\\n    --exp_name olmo2_1b_v2_sft_lr3e-5_seed1  \\\n    --model_name_or_path allenai/OLMo-2-0425-1B \\\n    --model_revision main \\\n    --tokenizer_name allenai/OLMo-2-1124-7B \\\n    --tokenizer_revision main \\\n    --use_slow_tokenizer False \\\n    --add_bos \\\n    --dataset_mixer_list allenai/tulu-3-sft-olmo-2-mixture-0225 1.0 \\\n    --use_flash_attn \\\n    --max_seq_length 4096 \\\n    --per_device_train_batch_size 2 \\\n    --gradient_accumulation_steps 8 \\\n    --learning_rate 3e-5 \\\n    --lr_scheduler_type linear \\\n    --warmup_ratio 0.03 \\\n    --weight_decay 0.0 \\\n    --num_train_epochs 2 \\\n    --report_to wandb \\\n    --with_tracking \\\n    --logging_steps 1 \\\n    --seed 1\n</code></pre> For those internal to Ai2, see the wandb logs or the beaker job.</p>"},{"location":"olmo2/#preference-tuning-dpo","title":"Preference Tuning (DPO)","text":""},{"location":"olmo2/#1b_1","title":"1B","text":"<p>We ran training for the 1B model in DPO on 1 node of 8 NVIDIA H100 GPUs. The command reduces to: <pre><code>accelerate launch \\\n    --mixed_precision bf16 \\\n    --num_processes 8 \\\n    --use_deepspeed \\\n    --deepspeed_config_file configs/ds_configs/stage2_accelerate.conf \\\n    --deepspeed_multinode_launcher standard \\\n    open_instruct/dpo_tune_cache.py \\\n    --exp_name 0424_1B_dpo_onpol_lr_2.5e-6_seed_111 \\\n    --learning_rate 2.5e-6 \\\n    --seed 111 \\\n    --model_name_or_path allenai/OLMo-2-0425-1B-SFT \\\n    --model_revision main \\\n    --use_flash_attn \\\n    --tokenizer_name_or_path allenai/OLMo-2-1124-13B \\\n    --tokenizer_revision main \\\n    --max_seq_length 2048 \\\n    --per_device_train_batch_size 8 \\\n    --gradient_accumulation_steps 2 \\\n    --lr_scheduler_type linear \\\n    --warmup_ratio 0.1 \\\n    --weight_decay 0.0 \\\n    --num_train_epochs 1 \\\n    --output_dir /output \\\n    --with_tracking \\\n    --report_to wandb \\\n    --logging_steps 1 \\\n    --gradient_checkpointing \\\n    --dataset_mixer_list allenai/olmo-2-0425-1b-preference-mix \\\n    --use_slow_tokenizer False \\\n    --add_bos \\\n    --use_lora False \\\n    --dpo_loss_type dpo_norm \\\n    --dpo_beta 5 \n</code></pre></p> <p>For those internal to Ai2, see the wandb logs or the beaker job.</p> <p>Example with DeepSpeed Stage 2:  <pre><code>python mason.py \\\n    --cluster ai2/augusta \\\n    --workspace ai2/olmo-instruct \\\n    --priority urgent \\\n    --image nathanl/open_instruct_auto --pure_docker_mode \\\n    --preemptible \\\n    --num_nodes 1 \\\n    --budget ai2/oe-adapt \\\n    --gpus 8 -- accelerate launch \\\n    --mixed_precision bf16 \\\n    --num_processes 8 \\\n    --use_deepspeed \\\n    --deepspeed_config_file configs/ds_configs/stage2_accelerate.conf \\\n    --deepspeed_multinode_launcher standard \\\n    open_instruct/dpo_tune_cache.py \\\n    --exp_name \"0424_1B_dpo_onpol_lr_2.5e-6_seed_111\" \\\n    --learning_rate 2.5e-6 \\\n    --seed 111 \\\n    --model_name_or_path allenai/open_instruct_dev \\\n    --model_revision \"olmo2_1b_v2_sft_lr3e-5_seed1__1__1744989064\" \\\n    --use_flash_attn \\\n    --tokenizer_name_or_path allenai/OLMo-2-1124-13B \\\n    --tokenizer_revision main \\\n    --max_seq_length 2048 \\\n    --per_device_train_batch_size 8 \\\n    --gradient_accumulation_steps 2 \\\n    --lr_scheduler_type linear \\\n    --warmup_ratio 0.1 \\\n    --weight_decay 0.0 \\\n    --num_train_epochs 1 \\\n    --output_dir /output \\\n    --with_tracking \\\n    --report_to wandb \\\n    --logging_steps 1 \\\n    --gradient_checkpointing \\\n    --dataset_mixer_list \\\n        allenai/olmo-2-1b-pref-mix-v0 1.0 \\\n    --use_slow_tokenizer False \\\n    --add_bos \\\n    --use_lora False \\\n    --dpo_loss_type dpo_norm \\\n    --dpo_beta 5\n</code></pre></p> <p>Example run with DeepSpeed Stage 3 (slower than stage 2): <pre><code>for lr in 2e-6; do\npython mason.py \\\n    --cluster ai2/jupiter \\\n    --workspace ai2/olmo-instruct \\\n    --priority high \\\n    --image nathanl/open_instruct_auto --pure_docker_mode \\\n    --preemptible \\\n    --num_nodes 1 \\\n    --budget ai2/oe-adapt \\\n    --gpus 8 -- accelerate launch \\\n    --mixed_precision bf16 \\\n    --num_processes 8 \\\n    --use_deepspeed \\\n    --deepspeed_config_file configs/ds_configs/stage3_no_offloading_accelerate.conf \\\n    --deepspeed_multinode_launcher standard \\\n    open_instruct/dpo_tune_cache.py \\\n    --exp_name \"0421_1B_dpo_lr_${lr}\" \\\n    --learning_rate $lr \\\n    --model_name_or_path allenai/open_instruct_dev \\\n    --model_revision \"olmo2_1b_v2_sft_lr3e-5_seed1__1__1744989064\" \\\n    --use_flash_attn \\\n    --tokenizer_name_or_path allenai/OLMo-2-1124-13B \\\n    --tokenizer_revision main \\\n    --max_seq_length 2048 \\\n    --per_device_train_batch_size 1 \\\n    --gradient_accumulation_steps 16 \\\n    --lr_scheduler_type linear \\\n    --warmup_ratio 0.1 \\\n    --weight_decay 0.0 \\\n    --num_train_epochs 1 \\\n    --output_dir /output \\\n    --with_tracking \\\n    --report_to wandb \\\n    --logging_steps 1 \\\n    --gradient_checkpointing \\\n    --dataset_mixer_list \\\n        allenai/olmo-2-32b-pref-mix-v0-filter-datecutoff 1.0 \\\n    --use_slow_tokenizer False \\\n    --add_bos \\\n    --use_lora False \\\n    --dpo_loss_type dpo_norm \\\n    --dpo_beta 5 \\\n    --add_bos \\\n    --use_slow_tokenizer False\ndone\n</code></pre></p>"},{"location":"olmo2/#rlvr","title":"RLVR","text":""},{"location":"olmo2/#1b_2","title":"1B","text":"<p>The 1B OLMo 2 model has two RL stages run in sequence. The first is on MATH, GSM8K, and IF constraints: <pre><code>python open_instruct/grpo_vllm_thread_ray_gtrl.py \\\n    --exp_name 0423_grpo_seed_1_lr_7e-7 \\\n    --beta 0.01 \\\n    --local_mini_batch_size 32 \\\n    --number_samples_per_prompt 16 \\\n    --local_rollout_batch_size 4 \\\n    --kl_estimator kl3 \\\n    --learning_rate 5e-7 \\\n    --dataset_mixer_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 1.0 \\\n    --dataset_mixer_list_splits train \\\n    --dataset_mixer_eval_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 16 \\\n    --dataset_mixer_eval_list_splits train \\\n    --max_token_length 2048 \\\n    --max_prompt_token_length 2048 \\\n    --response_length 2048 \\\n    --model_name_or_path allenai/OLMo-2-0425-1B-DPO \\\n    --model_revision main \\\n    --tokenizer_name allenai/OLMo-2-1124-7B-DPO \\\n    --tokenizer_revision main \\\n    --use_slow_tokenizer False \\\n    --add_bos \\\n    --non_stop_penalty \\\n    --stop_token eos \\\n    --temperature 1.0 \\\n    --ground_truths_key ground_truth \\\n    --chat_template_name tulu \\\n    --sft_messages_key messages \\\n    --total_episodes 2000000 \\\n    --penalty_reward_value 0.0 \\\n    --deepspeed_stage 2 \\\n    --per_device_train_batch_size 1 \\\n    --local_rollout_forward_batch_size 2 \\\n    --actor_num_gpus_per_node 4 8 \\\n    --num_epochs 1 \\\n    --vllm_tensor_parallel_size 4 \\\n    --lr_scheduler_type constant \\\n    --apply_verifiable_reward true \\\n    --seed 1 \\\n    --num_evals 100 \\\n    --save_freq 200 \\\n    --reward_model_multiplier 0.0 \\\n    --no_try_launch_beaker_eval_jobs \\\n    --try_launch_beaker_eval_jobs_on_weka \\\n    --gradient_checkpointing \\\n    --with_tracking \\\n    --tokenizer_name_or_path allenai/OLMo-2-1124-7B-DPO\n</code></pre> For those internal to Ai2, see the wandb logs or the beaker job.</p> <p>Next, on MATH only: <pre><code>python open_instruct/grpo_vllm_thread_ray_gtrl.py \\\n--exp_name 0427_grpo_seed_1_lr_9e-7 \\\n--beta 0.01 \\\n--local_mini_batch_size 32 \\\n--number_samples_per_prompt 16 \\\n--local_rollout_batch_size 4 \\\n--kl_estimator kl3 \\\n--learning_rate 5e-7 \\\n--dataset_mixer_list allenai/RLVR-MATH 1.0 \\\n--dataset_mixer_list_splits train \\\n--dataset_mixer_eval_list allenai/RLVR-MATH 16 \\\n--dataset_mixer_eval_list_splits train \\\n--max_token_length 2048 \\\n--max_prompt_token_length 2048 \\\n--response_length 2048 \\\n--model_name_or_path allenai/OLMo-2-0425-1B-RLVR1 \\\n--model_revision main \\\n--use_slow_tokenizer False \\\n--add_bos \\\n--non_stop_penalty \\\n--stop_token eos \\\n--temperature 1.0 \\\n--ground_truths_key ground_truth \\\n--chat_template_name tulu \\\n--sft_messages_key messages \\\n--total_episodes 2000000 \\\n--penalty_reward_value 0.0 \\\n--deepspeed_stage 2 \\\n--per_device_train_batch_size 1 \\\n--local_rollout_forward_batch_size 2 \\\n--actor_num_gpus_per_node 4 8 \\\n--num_epochs 1 \\\n--vllm_tensor_parallel_size 4 \\\n--lr_scheduler_type constant \\\n--apply_verifiable_reward true \\\n--seed 1 \\\n--num_evals 100 \\\n--save_freq 200 \\\n--reward_model_multiplier 0.0 \\\n--no_try_launch_beaker_eval_jobs \\\n--try_launch_beaker_eval_jobs_on_weka \\\n--gradient_checkpointing \\\n--with_tracking \\\n--tokenizer_name_or_path allenai/OLMo-2-1124-7B-DPO \\\n--tokenizer_revision main\n</code></pre> For those internal to Ai2, see the wandb logs or the beaker job.</p>"},{"location":"safety/","title":"Safety Evaluations","text":"<p>We are using the Ai2 Safety Evaluation suite for safety evals. This contains a bunch of sub-evals, and you can learn more by looking at the eval-safety fork.</p>"},{"location":"safety/#running-at-ai2","title":"Running at Ai2","text":"<p>This should be the most relevant thing for internal Ai2 users of open-instruct. To run evals, use the task suite <code>SAFETY_EVAL</code> or <code>SAFETY_EVAL_REASONING</code> when calling <code>submit_eval_jobs.py</code>. This will create a job that uploads and runs the safety evaluations (and uploads to the leaderboard if the appropriate flag is set). </p> <p>An example command on a reasoning model would be: <pre><code>python scripts/submit_eval_jobs.py \\\n    --model_name &lt;model name&gt; \\\n      --location &lt;beaker id&gt; \\\n      --is_tuned --workspace tulu-3-results \\\n      --preemptible \\\n      --use_hf_tokenizer_template \\\n      --beaker_image nathanl/open_instruct_auto \\\n      --upload_to_hf allenai/tulu-3-evals \\\n      --run_oe_eval_experiments \\\n      --oe_eval_task_suite \"SAFETY_EVAL_REASONING\"\n</code></pre></p> <p>An example command on a non-reasoning model would be: <pre><code>python scripts/submit_eval_jobs.py \\\n    --model_name &lt;model name&gt; \\\n      --location &lt;beaker id&gt; \\\n      --is_tuned --workspace tulu-3-results \\\n      --preemptible \\\n      --use_hf_tokenizer_template \\\n      --beaker_image nathanl/open_instruct_auto \\\n      --upload_to_hf allenai/tulu-3-evals \\\n      --run_oe_eval_experiments \\\n      --oe_eval_task_suite \"SAFETY_EVAL\"\n</code></pre></p>"},{"location":"safety/#running-on-an-interactive-session","title":"Running on an interactive session","text":"<p>Clone the fork and run from that location.</p>"},{"location":"safety/#safety-benchmarks","title":"Safety benchmarks","text":"<p>For all benchmarks requiring safety evaluation unless noted otherwise, as a default, we use the WildGuard classifier to evaluate the safety of model outputs.</p> <ul> <li>WildGuardTest</li> <li>Harmbench</li> <li>ToxiGen</li> <li>XSTest</li> <li>JailbreakTrigger (in TrustLLM)</li> <li>Do-anything-now</li> <li>WildJailbreak (both harmful and benign contrast sets)</li> </ul> <pre><code>PYTHONPATH=safety-eval python evaluation/run_all_generation_benchmarks.py    \\\n --model_name_or_path allenai/tulu-2-dpo-7b     --model_input_template_path_or_name tulu2    \\\n  --report_output_path ./generation_results/metrics.json     --save_individual_results_path ./generation_results/all.json \\\n  --hf_upload_name {HF upload name} --upload_to_hf {HF repo ID} --min_gpus_per_task {num. GPUs available}\n</code></pre> <p>Changing classifiers for safety benchmarks:</p> <p>You can change the safety classifier used for evaluation by specifying the <code>classifier_model_name</code> in the yaml file. For example, when you want to use the HarmBench's classifiers for evaluation on HarmBench, you can use <code>HarmbenchClassifier</code> as the <code>classifier_model_name</code>. Please check out the <code>evaluation/tasks/generation/harmbench/default.yaml</code> and <code>evaluation/tasks/classification/harmbench/harmbench_classsifier.yaml</code> to see the classifier's specification.</p>"},{"location":"tulu1_tulu2/","title":"Tulu1 tulu2","text":""},{"location":"tulu1_tulu2/#tulu-1-and-tulu-2-documentation","title":"Tulu 1 and Tulu 2 Documentation","text":"<p>Note Tulu 1/2 results used an ealier version of Open Instruct with a pinned version of Transformers. If you are looking to replicate these results, refer to this commit or older.</p>"},{"location":"tulu1_tulu2/#released-checkpoints","title":"Released Checkpoints","text":"<p>Our checkpoints can be found:</p> <ul> <li>Here for all Tulu v1 models.</li> <li>Here for all Tulu v2 models.</li> <li>OLMo 7B SFT and Instruct, along with a 2048 sequence length version of Tulu 2.</li> </ul>"},{"location":"tulu1_tulu2/#weight-diff-script","title":"Weight diff script","text":"<p>Our Tulu V1 models were released as weight diffs (due to LLaMa 1 license). We use a slightly modified form of the Alpaca weight diff script, which runs the same.</p> <p>To merge a model: 1. Download the relevant LLaMa model and convert it to Hugging Face format (see above). 2. Download our repository and install the right dependencies (see above). 3. Download the model diff you want. 4. Run the command below:</p> <pre><code>python scripts/weights/weight_diff.py recover --path_raw ${hf_llama_path} --path_tuned ${output_path} --path_diff ${diff_location}\n</code></pre>"},{"location":"tulu1_tulu2/#evaluation","title":"Evaluation","text":""},{"location":"tulu1_tulu2/#benchmark-based-eval","title":"Benchmark-based eval","text":"<p>We provide the scripts for running evaluation of Huggingface/OpenAI models on a list of standard benchmarks targeting for the core capabilities of large language models. These benchmakrs include:</p> <ul> <li>MMLU</li> <li>Grade School Math (GSM)</li> <li>MATH</li> <li>Big-Bench Hard (BBH)</li> <li>TydiQA</li> <li>Codex HumanEval</li> <li>HumanEval+ and MBPP+</li> <li>IFEval</li> <li>ToxiGen</li> <li>XSTest</li> <li>TruthfulQA</li> <li>AlpacaEval 1 and 2</li> </ul> <p>We are working on including more promising benchmarks into this list. Please stay tuned!</p> <p>You can use the following script to download all the evaluation data:</p> <pre><code>./scripts/data/prepare_eval_data.sh\n</code></pre> <p>Evaluation scripts for different datasets are put under <code>./scripts</code>. For example, you can use the following command to run the MMLU evaluation script:</p> <pre><code>./scripts/eval/mmlu.sh\n</code></pre>"},{"location":"tulu1_tulu2/#human-evaluation","title":"Human evaluation","text":"<p>We release our human evaluation interface and collected annotations in the <code>./human_eval</code> folder. Please see the corresponding README for more details.</p>"},{"location":"tulu1_tulu2/#training","title":"Training","text":""},{"location":"tulu1_tulu2/#dataset-preparation","title":"Dataset preparation","text":"<p>We include a collection of representative instruction datasets in our exploration and are adding new ones to our list. We unify them into the same chatting format. To download and prepare these datasets, simply run the following command:</p> <pre><code>./scripts/data/prepare_train_data.sh\n</code></pre> <p>Please check these datasets for licenses and restrictions around their use!</p> <p>You can also find the processed Tulu v1 and Tulu v2 SFT datasets on HuggingFace. Note that the train data preparation script will not precisely recreate the Tulu v2 mixture due to randomness in the generation and shifts in data availability - see this PR for some details. If you need exactly yhe training data used, the HuggingFace mixture is exactly this - the exact same data used during model training.</p>"},{"location":"tulu1_tulu2/#model-preparation","title":"Model preparation","text":"<p>Generally, most huggingface-compatible causal language models should work fine with our codebase, potentially with some adjusting for different tokenizers etc. Some models may require addtional requests to download. E.g., for LLaMa 1 and 2, please consult the Hugging Face documentation for requesting access and converting them to a huggingface-compatible format.</p>"},{"location":"tulu1_tulu2/#finetuning","title":"Finetuning","text":"<p>You can use the following command to run instruction tuning (finetuning a pretrained model to follow instructions):</p> <pre><code>./scripts/finetune_with_accelerate.sh\n</code></pre> <p>Make sure to adjust <code>model_name_or_path</code>, <code>tokenizer_name</code>, <code>train_file</code>, and <code>output_dir</code> to your models / data / setting. By default, this uses <code>deepspeed</code> with <code>accelerate</code>.</p>"},{"location":"tulu1_tulu2/#parameter-efficient-finetuning","title":"Parameter-Efficient Finetuning","text":"<p>We support LoRA finetuning, wherein only a small number of parameters are updated, resulting in faster and cheaper training. For even more efficiency, we also support QLoRA finetuning, wherein the non-trained (underlying) model parameters are quantised during 4-bit training. This means you can train a 70b Llama model on a single 80GB A100! Please refer to the respective papers for more details.</p> <p>Please also note you cannot currently run QLoRA with model parallelism - only data-parallel training is supported, so you cannot train a model that does not fit on one GPU. For LoRA, you can use deepspeed + zero-3 to achieve model parallelism (and FSDP is not currently supported).</p> <p>Please see <code>./scripts/finetune_lora_with_accelerate.sh</code> and <code>./scripts/finetune_qlora_with_accelerate.sh</code> for example hyperparameters. We found a larger rank (e.g. 256) and higher learning rate (e.g. 2e-4) worked best. Additionally, we found that QLoRA tended to always achieve similar results to LoRA, while LoRA itself sometimes fell behind full-finetuning, especially in long, complex generation tasks. However, for most purposes, LoRA training essentially matches full-finetuning performance. We recommend merging modules learnt with QLoRA into a dequantised model (run our merge script with the <code>--qlora</code> flag).</p>"},{"location":"tulu1_tulu2/#dpo-finetuning","title":"DPO Finetuning","text":"<p>For an example of how to fully finetune a model with DPO, see <code>scripts/dpo_train_with_accelerate.sh</code>. Note you will require at least 8 80GB A100s to be able to train a 7b size model, and will require more compute for anything larger. We have not tested multi-node training with this script, but it should work.</p> <p>Our script also supports PEFT training with QLoRA. See <code>scripts/dpo_train_with_qlora.sh</code> for an example. We have not trained models with this, so it may require additional hyperparameter tuning to achieve reasonable results.</p>"},{"location":"tulu3/","title":"Tulu3 Reproduction","text":"<p>This document details the commands and configs to reproduce the tulu3 models.</p>"},{"location":"tulu3/#finetuning","title":"Finetuning","text":""},{"location":"tulu3/#llama-31-tulu-3-8b-sft-reproduction","title":"Llama-3.1-Tulu-3-8B-SFT Reproduction","text":"<p>Below is (almost) the exact command which produced Llama-3.1-Tulu-3-8B-SFT. We deployed the command across 8 machines, each equipped with 8 NVIDIA H100 GPUs, for a total of 64 GPUs in the our setup.</p> <pre><code># modify the following `MACHINE_RANK`, `MAIN_PROCESS_IP`,\n# `NUM_MACHINES`, `NUM_PROCESSES`, `PER_DEVICE_TRAIN_BATCH_SIZE`,\n# `GRADIENT_ACCUMULATION_STEPS` according to your setup\nMACHINE_RANK=0\nMAIN_PROCESS_IP=localhost\nNUM_MACHINES=8\nNUM_PROCESSES=64\nPER_DEVICE_TRAIN_BATCH_SIZE=1\nGRADIENT_ACCUMULATION_STEPS=2\naccelerate launch \\\n    --mixed_precision bf16 \\\n    --num_machines 8 \\\n    --num_processes 64 \\\n    --machine_rank $MACHINE_RANK \\\n    --main_process_ip $MAIN_PROCESS_IP \\\n    --main_process_port 29400 \\\n    --use_deepspeed \\\n    --deepspeed_config_file configs/ds_configs/stage3_no_offloading_accelerate.conf \\\n    --deepspeed_multinode_launcher standard open_instruct/finetune.py \\\n    --model_name_or_path meta-llama/Llama-3.1-8B \\\n    --tokenizer_name meta-llama/Llama-3.1-8B \\\n    --use_slow_tokenizer \\\n    --use_flash_attn \\\n    --max_seq_length 4096 \\\n    --preprocessing_num_workers 128 \\\n    --per_device_train_batch_size $PER_DEVICE_TRAIN_BATCH_SIZE \\\n    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \\\n    --learning_rate 5e-06 \\\n    --lr_scheduler_type linear \\\n    --warmup_ratio 0.03 \\\n    --weight_decay 0.0 \\\n    --num_train_epochs 2 \\\n    --output_dir output/sft_8b \\\n    --with_tracking \\\n    --report_to wandb \\\n    --logging_steps 1 \\\n    --model_revision main \\\n    --dataset_mixer_list allenai/tulu-3-sft-mixture 1.0 \\\n    --checkpointing_steps epoch \\\n    --dataset_mix_dir output/sft_8b \\\n    --exp_name tulu-3-8b-sft \\\n    --seed 123\n# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JBNTPW8TKG09B2XR832YB5S8\n</code></pre> <p>[!NOTE] If you have different number of GPUs, please adjust the <code>NUM_MACHINES</code>, <code>NUM_PROCESSES</code>, <code>PER_DEVICE_TRAIN_BATCH_SIZE</code>, and <code>GRADIENT_ACCUMULATION_STEPS</code> accordingly to reproduce the same effective batch size. The effective batch size is calculated by multiplying: - Number of GPUs / processes (NUM_PROCESSES) - Train batch size per GPU (PER_DEVICE_TRAIN_BATCH_SIZE)  - Gradient accumulation steps (GRADIENT_ACCUMULATION_STEPS) so we have <pre><code>64 GPUs: 64 * 1 * 2 = 128 # from the example above\n8 GPUs:   8 * 1 * 16 = 128 # if you only \n</code></pre> You can achieve the same effective batch size with fewer GPUs by increasing gradient accumulation steps proportionally (e.g., <code>NUM_PROCESSES=8, PER_DEVICE_TRAIN_BATCH_SIZE=1, and GRADIENT_ACCUMULATION_STEPS=16</code>)</p>"},{"location":"tulu3/#llama-31-tulu-3-70b-sft-reproduction","title":"Llama-3.1-Tulu-3-70B-SFT Reproduction","text":"<p>This is (almost) the exact command which produced allenai/Llama-3.1-Tulu-3-70B-SFT</p> <pre><code># modify the following `MACHINE_RANK`, `MAIN_PROCESS_IP`,\n# `NUM_MACHINES`, `NUM_PROCESSES`, `PER_DEVICE_TRAIN_BATCH_SIZE`,\n# `GRADIENT_ACCUMULATION_STEPS` according to your setup\nMACHINE_RANK=0\nMAIN_PROCESS_IP=localhost\nNUM_MACHINES=8\nNUM_PROCESSES=64\nPER_DEVICE_TRAIN_BATCH_SIZE=1\nGRADIENT_ACCUMULATION_STEPS=2\naccelerate launch \\\n    --mixed_precision bf16 \\\n    --num_machines $NUM_MACHINES \\\n    --num_processes $NUM_PROCESSES \\\n    --machine_rank $MACHINE_RANK \\\n    --main_process_ip $MAIN_PROCESS_IP \\\n    --main_process_port 29400 \\\n    --use_deepspeed \\\n    --deepspeed_config_file configs/ds_configs/stage3_no_offloading_accelerate.conf \\\n    --deepspeed_multinode_launcher standard open_instruct/finetune.py \\\n    --model_name_or_path meta-llama/Llama-3.1-70B \\\n    --tokenizer_name meta-llama/Llama-3.1-70B \\\n    --use_slow_tokenizer \\\n    --use_flash_attn \\\n    --max_seq_length 4096 \\\n    --preprocessing_num_workers 128 \\\n    --per_device_train_batch_size $PER_DEVICE_TRAIN_BATCH_SIZE \\\n    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \\\n    --learning_rate 2e-06 \\\n    --lr_scheduler_type linear \\\n    --warmup_ratio 0.03 \\\n    --weight_decay 0.0 \\\n    --num_train_epochs 2 \\\n    --output_dir output/sft_70B \\\n    --with_tracking \\\n    --report_to wandb \\\n    --logging_steps 1 \\\n    --model_revision main \\\n    --dataset_mixer_list allenai/tulu-3-sft-mixture 1.0 \\\n    --dataset_mix_dir output/sft_70B \\\n    --checkpointing_steps 1000 \\\n    --keep_last_n_checkpoints 20 \\\n    --gradient_checkpointing \\\n    --exp_name tulu-3-70b-sft \\\n    --seed 456\n# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JC5J4R80M18XQTDH47JSFRJY/\n</code></pre>"},{"location":"tulu3/#preference-tuning","title":"Preference Tuning","text":""},{"location":"tulu3/#llama-31-tulu-3-8b-dpo-reproduction","title":"Llama-3.1-Tulu-3-8B-DPO Reproduction","text":"<p>This is (almost) the exact command which produced allenai/Llama-3.1-Tulu-3-8B-DPO</p> <pre><code>accelerate launch \\\n    --mixed_precision bf16 \\\n    --num_machines 1 \\\n    --num_processes 8 \\\n    --use_deepspeed \\\n    --deepspeed_config_file configs/ds_configs/stage3_no_offloading_accelerate.conf open_instruct/dpo_tune.py \\\n    --model_name_or_path allenai/Llama-3.1-Tulu-3-8B-SFT \\\n    --use_flash_attn \\\n    --tokenizer_name allenai/Llama-3.1-Tulu-3-8B-SFT \\\n    --max_seq_length 2048 \\\n    --preprocessing_num_workers 16 \\\n    --per_device_train_batch_size 1 \\\n    --gradient_accumulation_steps 16 \\\n    --learning_rate 5e-07 \\\n    --lr_scheduler_type linear \\\n    --warmup_ratio 0.1 \\\n    --weight_decay 0.0 \\\n    --num_train_epochs 1 \\\n    --output_dir output/dpo_8b \\\n    --with_tracking \\\n    --report_to wandb \\\n    --logging_steps 1 \\\n    --model_revision main \\\n    --gradient_checkpointing \\\n    --dataset_mixer_list allenai/llama-3.1-tulu-3-8b-preference-mixture 1.0 \\\n    --use_slow_tokenizer \\\n    --use_lora False \\\n    --dpo_loss_type dpo_norm \\\n    --dpo_beta 5 \\\n    --checkpointing_steps 1000 \\\n    --exp_name tulu-3-8b-dpo\n# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JCRXP0AR5312S8MD3XGCN0J7/\n</code></pre>"},{"location":"tulu3/#llama-31-tulu-3-70b-dpo-reproduction","title":"Llama-3.1-Tulu-3-70B-DPO Reproduction","text":"<p>This is (almost) the exact command which produced allenai/Llama-3.1-Tulu-3-70B-DPO</p> <pre><code># modify the following `MACHINE_RANK`, `MAIN_PROCESS_IP`,\n# `NUM_MACHINES`, `NUM_PROCESSES`, `PER_DEVICE_TRAIN_BATCH_SIZE`,\n# `GRADIENT_ACCUMULATION_STEPS` according to your setup\nMACHINE_RANK=0\nMAIN_PROCESS_IP=localhost\nNUM_MACHINES=8\nNUM_PROCESSES=64\nPER_DEVICE_TRAIN_BATCH_SIZE=1\nGRADIENT_ACCUMULATION_STEPS=2\naccelerate launch \\\n    --mixed_precision bf16 \\\n    --num_machines $NUM_MACHINES \\\n    --num_processes $NUM_PROCESSES \\\n    --machine_rank $MACHINE_RANK \\\n    --main_process_ip $MAIN_PROCESS_IP \\\n    --main_process_port 29400 \\\n    --use_deepspeed \\\n    --deepspeed_config_file configs/ds_configs/stage3_offloading_accelerate.conf \\\n    --deepspeed_multinode_launcher standard open_instruct/dpo_tune_cache.py \\\n    --model_name_or_path allenai/Llama-3.1-Tulu-3-70B-SFT \\\n    --tokenizer_name allenai/Llama-3.1-Tulu-3-70B-SFT \\\n    --use_flash_attn \\\n    --max_seq_length 2048 \\\n    --preprocessing_num_workers 16 \\\n    --per_device_train_batch_size $PER_DEVICE_TRAIN_BATCH_SIZE \\\n    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \\\n    --learning_rate 2e-07 \\\n    --lr_scheduler_type linear \\\n    --warmup_ratio 0.1 \\\n    --weight_decay 0.0 \\\n    --num_train_epochs 1 \\\n    --output_dir output/dpo_70b \\\n    --with_tracking \\\n    --report_to wandb \\\n    --logging_steps 1 \\\n    --model_revision main \\\n    --gradient_checkpointing \\\n    --dataset_mixer_list allenai/llama-3.1-tulu-3-70b-preference-mixture \\\n    --use_slow_tokenizer \\\n    --use_lora False \\\n    --dpo_loss_type dpo_norm \\\n    --dpo_beta 5 \\\n    --checkpointing_steps epoch \\\n    --exp_name tulu-3-70b-dpo\n# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JCSAYYHQYF9QDQDCV6KJ53M9/\n</code></pre>"},{"location":"tulu3/#llama-31-tulu-3-405b-dpo-reproduction","title":"Llama-3.1-Tulu-3-405B-DPO Reproduction","text":"<p>This is (almost) the exact command which produced allenai/Llama-3.1-Tulu-3-405B-DPO</p> <pre><code># modify the following `MACHINE_RANK`, `MAIN_PROCESS_IP`,\n# `NUM_MACHINES`, `NUM_PROCESSES`, `PER_DEVICE_TRAIN_BATCH_SIZE`,\n# `GRADIENT_ACCUMULATION_STEPS` according to your setup\nMACHINE_RANK=0\nMAIN_PROCESS_IP=localhost\nNUM_MACHINES=8\nNUM_PROCESSES=64\nPER_DEVICE_TRAIN_BATCH_SIZE=1\nGRADIENT_ACCUMULATION_STEPS=2\naccelerate launch --mixed_precision bf16 \\\n    --num_machines 32 \\\n    --num_processes 256 \\\n    --machine_rank $BEAKER_REPLICA_RANK \\\n    --main_process_ip $BEAKER_LEADER_REPLICA_HOSTNAME \\\n    --main_process_port 29400 \\\n    --use_deepspeed \\\n    --deepspeed_config_file configs/ds_configs/stage3_no_offloading_accelerate.conf \\\n    --deepspeed_multinode_launcher standard open_instruct/dpo_tune_cache.py \\\n    --model_name_or_path allenai/Llama-3.1-Tulu-3-405B-SFT \\\n    --tokenizer_name allenai/Llama-3.1-Tulu-3-70B-SFT \\\n    --use_flash_attn \\\n    --max_seq_length 2048 \\\n    --preprocessing_num_workers 16 \\\n    --per_device_train_batch_size 1 \\\n    --gradient_accumulation_steps 1 \\\n    --learning_rate 2e-07 \\\n    --lr_scheduler_type linear \\\n    --warmup_ratio 0.1 \\\n    --weight_decay 0.0 \\\n    --num_train_epochs 1 \\\n    --output_dir output_405b \\\n    --with_tracking \\\n    --report_to wandb \\\n    --logging_steps 1 \\\n    --model_revision main \\\n    --gradient_checkpointing \\\n    --dataset_mixer_list ai2-adapt-dev/405b_preference_mix 1.0 \\\n    --use_slow_tokenizer \\\n    --use_lora False \\\n    --dpo_loss_type dpo_norm \\\n    --dpo_beta 5 \\\n    --checkpointing_steps 1000\n# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JJ4QRZ31SH79AHVM6WWDVJB4/\n</code></pre>"},{"location":"tulu3/#rlvr","title":"RLVR","text":""},{"location":"tulu3/#rlvr-for-if-note","title":"RLVR for IF Note:","text":"<p>We have since updated the RLVR verifier functions and judge for precise IF. If you want to reproduce Tulu3 results, please use the IFEvalVerifierOld class in ground_truth_utils.py. The new IFEvalVerifier class is not compatible with the old data format, so please use the new IF data format for the new verifier. The new verifier and the new data will give better results.</p>"},{"location":"tulu3/#llama-31-tulu-3-8b-rm-reproduction","title":"Llama-3.1-Tulu-3-8B-RM Reproduction","text":"<p>This is (almost) the exact command which produced allenai/Llama-3.1-Tulu-3-8B-RM</p> <pre><code>accelerate launch \\\n    --config_file configs/ds_configs/deepspeed_zero3.yaml open_instruct/reward_modeling.py \\\n    --dataset_mixer '{\"allenai/llama-3.1-tulu-3-8b-preference-mixture\": 1.0}' \\\n    --dataset_train_splits train \\\n    --dataset_eval_mixer '{\"allenai/ultrafeedback_binarized_cleaned\": 1.0}' \\\n    --dataset_eval_splits test_prefs \\\n    --model_name_or_path allenai/Llama-3.1-Tulu-3-8B-SFT \\\n    --chat_template tulu \\\n    --learning_rate 3e-6 \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 32 \\\n    --max_token_length 2048 \\\n    --max_prompt_token_length 2048 \\\n    --num_train_epochs 1 \\\n    --output_dir output/rm_8b \\\n    --gradient_checkpointing \\\n    --push_to_hub \\\n    --with_tracking\n# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JCS01RFBQGFE5F1W3W96FFVM/\n</code></pre>"},{"location":"tulu3/#llama-31-tulu-3-8b-reproduction","title":"Llama-3.1-Tulu-3-8B Reproduction","text":"<p>This is (almost) the exact command which produced allenai/Llama-3.1-Tulu-3-8B</p> <pre><code>python open_instruct/ppo_vllm_thread_ray_gtrl.py \\\n    --exp_name tulu-3-8b-rlvr \\\n    --dataset_mixer_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 1.0 \\\n    --dataset_mixer_list_splits train \\\n    --dataset_mixer_eval_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 16 \\\n    --dataset_mixer_eval_list_splits train \\\n    --max_token_length 2048 \\\n    --max_prompt_token_length 2048 \\\n    --response_length 2048 \\\n    --model_name_or_path allenai/Llama-3.1-Tulu-3-8B-DPO \\\n    --reward_model_path allenai/Llama-3.1-Tulu-3-8B-RM \\\n    --non_stop_penalty \\\n    --stop_token eos \\\n    --temperature 1.0 \\\n    --chat_template_name tulu \\\n    --learning_rate 3e-7 \\\n    --total_episodes 10000000 \\\n    --penalty_reward_value -10.0 \\\n    --deepspeed_stage 3 \\\n    --per_device_train_batch_size 2 \\\n    --local_rollout_forward_batch_size 2 \\\n    --local_mini_batch_size 32 \\\n    --local_rollout_batch_size 32 \\\n    --actor_num_gpus_per_node 7 \\\n    --vllm_tensor_parallel_size 1 \\\n    --beta 0.05 \\\n    --apply_verifiable_reward true \\\n    --output_dir output/rlvr_8b \\\n    --seed 3 \\\n    --num_evals 3 \\\n    --save_freq 100 \\\n    --reward_model_multiplier 0.0 \\\n    --gradient_checkpointing \\\n    --with_tracking\n# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JCVTA10BQDVGGQKFYWEZ6KCQ/\n</code></pre>"},{"location":"tulu3/#llama-31-tulu-3-70b-reproduction","title":"Llama-3.1-Tulu-3-70B Reproduction","text":"<p>This is (almost) the exact command which produced allenai/Llama-3.1-Tulu-3-70B</p> <p>Couple of notes: * Make sure to modify <code>configs/beaker_configs/ray_node_setup.sh</code> in our own cluster setup. The idea is to have the replicas join the main machines via <code>ray</code>. * We had to use <code>--vllm_tensor_parallel_size 4</code> because <code>--vllm_tensor_parallel_size 8</code> errors out for some strange reason. This is a temporary workaround. * Here the effective batch size is <code>sum(actor_num_gpus_per_node) * local_mini_batch_size = 40 * 16 = 640</code>. If you have less GPUs, you can adjust <code>actor_num_gpus_per_node</code> and <code>local_mini_batch_size</code> accordingly.</p> <pre><code>source configs/beaker_configs/ray_node_setup.sh &amp;&amp; python open_instruct/ppo_vllm_thread_ray_gtrl.py \\\n    --dataset_mixer_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 1.0 \\\n    --dataset_mixer_list_splits train \\\n    --dataset_mixer_eval_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 16 \\\n    --dataset_mixer_eval_list_splits train \\\n    --max_token_length 2048 \\\n    --max_prompt_token_length 2048 \\\n    --response_length 2048 \\\n    --model_name_or_path allenai/Llama-3.1-Tulu-3-70B-DPO \\\n    --exp_name tulu-3-70b-rlvr \\\n    --reward_model_path allenai/Llama-3.1-Tulu-3-8B-RM \\\n    --beta 0.07 \\\n    --warmup_ratio 0.1 \\\n    --seed 8 \\\n    --output_dir output/rlvr_70b \\\n    --non_stop_penalty \\\n    --stop_token eos \\\n    --temperature 1.0 \\\n    --chat_template_name tulu \\\n    --learning_rate 1e-7 \\\n    --total_episodes 400000 \\\n    --penalty_reward_value -10.0 \\\n    --deepspeed_stage 3 \\\n    --per_device_train_batch_size 1 \\\n    --local_rollout_forward_batch_size 1 \\\n    --local_mini_batch_size 16 \\\n    --local_rollout_batch_size 16 \\\n    --actor_num_gpus_per_node 8 8 8 8 8 \\\n    --vllm_num_engines 1 \\\n    --vllm_tensor_parallel_size 4 \\\n    --apply_verifiable_reward true \\\n    --reward_model_multiplier 0.0 \\\n    --no_gather_whole_model \\\n    --num_evals 3 \\\n    --save_freq 40 \\\n    --gradient_checkpointing \\\n    --with_tracking\n# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JD3YEM4XGH2F2H10Y49GK441/\n</code></pre>"},{"location":"tulu3/#llama-31-tulu-3-405b-reproduction","title":"Llama-3.1-Tulu-3-405B Reproduction","text":"<p>This is (almost) the exact command which produced allenai/Llama-3.1-Tulu-3-405B</p> <p>Couple of notes: * We had to set <code>TORCH_NCCL_ENABLE_MONITORING=0</code> to turn off NCCL heartbeat monitoring and avoid timeouts. Feel free to remove this. * Make sure to modify <code>configs/beaker_configs/ray_node_setup.sh</code> in our own cluster setup. The idea is to have the replicas join the main machines via <code>ray</code>. * Here the effective batch size is <code>sum(actor_num_gpus_per_node) * local_mini_batch_size = 40 * 16 = 640</code>. If you have less GPUs, you can adjust <code>actor_num_gpus_per_node</code> and <code>local_mini_batch_size</code> accordingly.</p> <pre><code>TORCH_NCCL_ENABLE_MONITORING=0 python mason.py \\\n    --cluster ai2/jupiter --pure_docker_mode \\\n    --workspace ai2/tulu-3-dev \\\n    --priority urgent \\\n    --preemptible \\\n    --num_nodes 32 \\\n    --image nathanl/open_instruct_auto \\\n    --budget ai2/oe-adapt \\\n    --gpus 8 -- source configs/beaker_configs/ray_node_setup.sh \\&amp;\\&amp; TORCH_DISTRIBUTED_DEBUG=DETAIL python open_instruct/ppo_vllm_thread_ray_gtrl.py \\\n    --dataset_mixer_list allenai/RLVR-MATH 1.0 \\\n    --dataset_mixer_list_splits train \\\n    --dataset_mixer_eval_list allenai/RLVR-MATH 128 \\\n    --dataset_mixer_eval_list_splits train \\\n    --max_token_length 2048 \\\n    --max_prompt_token_length 2048 \\\n    --response_length 1024 \\\n    --model_name_or_path /weka/oe-adapt-default/hamishi/405b_dpo_v4 \\\n    --exp_name \"405b_rlvr_math_only_8b_valu_on_v4\" \\\n    --reward_model_path allenai/Llama-3.1-Tulu-3-8B-RM \\\n    --beta 0.05 \\\n    --output_dir \"/weka/oe-adapt-default/hamishi/405b_rlvr_math_only_8b_valu_on_v4\" \\\n    --non_stop_penalty \\\n    --stop_token eos \\\n    --temperature 1.0 \\\n    --chat_template tulu \\\n    --learning_rate 1e-7 \\\n    --total_episodes 400000 \\\n    --num_epochs 4 \\\n    --penalty_reward_value -10.0 \\\n    --deepspeed_stage 3 \\\n    --per_device_train_batch_size 1 \\\n    --local_rollout_forward_batch_size 1 \\\n    --local_mini_batch_size 8 \\\n    --local_rollout_batch_size 8 \\\n    --actor_num_gpus_per_node 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 \\\n    --vllm_num_engines 1 \\\n    --vllm_tensor_parallel_size 16 \\\n    --vllm_enforce_eager true \\\n    --apply_verifiable_reward true \\\n    --reward_model_multiplier 0.0 \\\n    --no_gather_whole_model \\\n    --seed 3 \\\n    --num_evals 3 \\\n    --no_try_launch_beaker_eval_jobs \\\n    --save_freq 25 \\\n    --try_launch_beaker_eval_jobs_on_weka \\\n    --gradient_checkpointing \\\n    --with_tracking\n# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JJA31S20XAFR82YPFKSMMYZV/\n</code></pre>"},{"location":"tulu3/#new-llama-31-tulu-31-8b-reproduction","title":"(NEW) Llama-3.1-Tulu-3.1-8B Reproduction","text":"<p>This is the exact command which produced allenai/Llama-3.1-Tulu-3.1-8B, which uses 2 nodes (16 GPUs)</p> <pre><code>for learning_rate in 5e-7; do\nfor beta in 0.01; do\nfor nspp in 16; do\nfor m in half-m ; do\nfor kl_estimator in kl3; do\nlocal_rollout_batch_size=4\nif [ $m == \"half-m\" ]; then\n    local_mini_batch_size=$(($local_rollout_batch_size * $nspp / 2))\nelse\n    local_mini_batch_size=$(($local_rollout_batch_size * $nspp))\nfi\nexp_name=\"0204_lr_scan_grpo_math_lr_${learning_rate}_${kl_estimator}_${beta}_${nspp}_${m}_${RANDOM}\"\nfull_bsz=$(($local_rollout_batch_size * nspp * (7) * 2))\necho $exp_name:\necho --- local_mini_batch_size=$local_mini_batch_size\necho --- full_bsz=$full_bsz\necho --- num_gradient_updates=$(($local_rollout_batch_size * $nspp / $local_mini_batch_size))\npython mason.py \\\n    --cluster ai2/jupiter \\\n    --workspace ai2/tulu-3-dev \\\n    --priority high \\\n    --preemptible \\\n    --num_nodes 2 \\\n    --max_retries 1 \\\n    --budget ai2/oe-adapt \\\n    --gpus 8 -- source configs/beaker_configs/ray_node_setup.sh \\&amp;\\&amp; uv run python open_instruct/grpo_vllm_thread_ray_gtrl.py \\\n    --exp_name $exp_name \\\n    --beta $beta \\\n    --local_mini_batch_size $local_mini_batch_size \\\n    --number_samples_per_prompt $nspp \\\n    --output_dir /weka/oe-adapt-default/costah/models/$exp_name \\\n    --local_rollout_batch_size $local_rollout_batch_size \\\n    --kl_estimator $kl_estimator \\\n    --learning_rate $learning_rate \\\n    --dataset_mixer_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 1.0 \\\n    --dataset_mixer_list_splits train \\\n    --dataset_mixer_eval_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 16 \\\n    --dataset_mixer_eval_list_splits train \\\n    --max_token_length 2048 \\\n    --max_prompt_token_length 2048 \\\n    --response_length 2048 \\\n    --model_name_or_path allenai/Llama-3.1-Tulu-3-8B-DPO \\\n    --non_stop_penalty \\\n    --stop_token eos \\\n    --temperature 1.0 \\\n    --chat_template_name tulu \\\n    --total_episodes 10000000 \\\n    --penalty_reward_value 0.0 \\\n    --deepspeed_stage 2 \\\n    --per_device_train_batch_size 2 \\\n    --local_rollout_forward_batch_size 2 \\\n    --actor_num_gpus_per_node 4 8 \\\n    --num_epochs 1 \\\n    --vllm_tensor_parallel_size 4 \\\n    --lr_scheduler_type constant \\\n    --apply_verifiable_reward true \\\n    --seed 1 \\\n    --num_evals 30 \\\n    --save_freq 40 \\\n    --reward_model_multiplier 0.0 \\\n    --no_try_launch_beaker_eval_jobs \\\n    --try_launch_beaker_eval_jobs_on_weka \\\n    --gradient_checkpointing \\\n    --with_tracking\ndone\ndone\ndone\ndone\ndone\n# For Ai2 internal members, this was the experiment URL: https://beaker.allen.ai/orgs/ai2/workspaces/tulu-3-dev/work/01JKA7CSDGG3YA84X89C5HJPXR?taskId=01JKA7CSDQMVBDNAWF5T7ZXDSA&amp;jobId=01JKH4KYJTR2Y2NYNCCQ63ZQHE\n</code></pre> <p>If you are running on a single node (8 GPUs), consider adjusting the commands as follows. Basically, the idea is to simulate the same batch size. In the two nodes setup, we used <code>--actor_num_gpus_per_node 4 8</code> (12 GPUs) for training, so we multiply it with <code>local_rollout_batch_size=4</code> to get the rollout batch size <code>12 * 4 = 48</code>. Now assume we used <code>--actor_num_gpus_per_node 6</code> (6 GPUs) for training, so we get <code>48 / 6 = 8</code>, which is the new <code>local_rollout_batch_size</code>.</p> <pre><code> for learning_rate in 5e-7; do\n for beta in 0.01; do\n for nspp in 16; do\n for m in half-m ; do\n for kl_estimator in kl3; do\n-local_rollout_batch_size=4\n+local_rollout_batch_size=8\n if [ $m == \"half-m\" ]; then\n     local_mini_batch_size=$(($local_rollout_batch_size * $nspp / 2))\n else\n     local_mini_batch_size=$(($local_rollout_batch_size * $nspp))\n fi\n exp_name=\"0204_lr_scan_grpo_math_lr_${learning_rate}_${kl_estimator}_${beta}_${nspp}_${m}_${RANDOM}\"\n full_bsz=$(($local_rollout_batch_size * nspp * (7) * 2))\n echo $exp_name:\n echo --- local_mini_batch_size=$local_mini_batch_size\n echo --- full_bsz=$full_bsz\n echo --- num_gradient_updates=$(($local_rollout_batch_size * $nspp / $local_mini_batch_size))\n python mason.py \\\n     --cluster ai2/jupiter \\\n     --workspace ai2/tulu-3-dev \\\n     --priority high \\\n     --preemptible \\\n     --num_nodes 2 \\\n     --max_retries 1 \\\n     --budget ai2/oe-adapt \\\n     --gpus 8 -- source configs/beaker_configs/ray_node_setup.sh \\&amp;\\&amp; uv run python open_instruct/grpo_vllm_thread_ray_gtrl.py \\\n     --exp_name $exp_name \\\n     --beta $beta \\\n     --local_mini_batch_size $local_mini_batch_size \\\n     --number_samples_per_prompt $nspp \\\n     --output_dir /weka/oe-adapt-default/costah/models/$exp_name \\\n     --local_rollout_batch_size $local_rollout_batch_size \\\n     --kl_estimator $kl_estimator \\\n     --learning_rate $learning_rate \\\n     --dataset_mixer_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 1.0 \\\n     --dataset_mixer_list_splits train \\\n     --dataset_mixer_eval_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 16 \\\n     --dataset_mixer_eval_list_splits train \\\n     --max_token_length 2048 \\\n     --max_prompt_token_length 2048 \\\n     --response_length 2048 \\\n     --model_name_or_path allenai/Llama-3.1-Tulu-3-8B-DPO \\\n     --non_stop_penalty \\\n     --stop_token eos \\\n     --temperature 1.0 \\\n     --chat_template_name tulu \\\n     --total_episodes 10000000 \\\n     --penalty_reward_value 0.0 \\\n-    --deepspeed_stage 2 \\\n+    --deepspeed_stage 3 \\\n     --per_device_train_batch_size 2 \\\n     --local_rollout_forward_batch_size 2 \\\n-    --actor_num_gpus_per_node 4 8 \\\n+    --actor_num_gpus_per_node 6 \\\n     --num_epochs 1 \\\n-    --vllm_tensor_parallel_size 4 \\\n+    --vllm_tensor_parallel_size 2 \\\n     --lr_scheduler_type constant \\\n     --apply_verifiable_reward true \\\n     --seed 1 \\\n     --num_evals 30 \\\n     --save_freq 40 \\\n     --reward_model_multiplier 0.0 \\\n     --no_try_launch_beaker_eval_jobs \\\n     --try_launch_beaker_eval_jobs_on_weka \\\n     --gradient_checkpointing \\\n     --with_tracking\n done\n done\n done\n done\n done\n</code></pre>"},{"location":"algorithms/dataset_transformation/","title":"Dataset Transformations","text":"<p>Dataset transformations are a key part of the training process. Typically, we are given some text dataset, and we tokenize and filter it to be used for training. </p> <p>Open Instruct includes a <code>dataset_transformation.py</code> utility which </p> <ul> <li>handles dataset mixing</li> <li>handles different tokenization functions</li> <li>caches the tokenized dataset so we don't have to re-tokenize every time<ul> <li>This is especially important when we have 405B SFT models: 32 nodes are just spending like 5 minutes to tokenize the dataset. This translates to 32 * 5 * 8 = 1280 minutes = 21 hours of wasted H100 time.</li> <li>Sometimes we also launch on places that don't have a shared cache, so we would download individual datasets 32 times, and wait for concatenation and tokenization (actually  twice because the <code>with accelerator.main_process_first()</code> function assumes a shared cache)</li> <li>Using a cache like this also minimizes the time to get first training output, making debug cycles faster.</li> </ul> </li> </ul>"},{"location":"algorithms/dataset_transformation/#sft-dataset-format","title":"SFT Dataset Format","text":"<p>We expect the dataset to have a <code>messages</code> key, which is a list of dictionaries with <code>role</code> and <code>content</code> keys. For example,</p> <ul> <li>allenai/tulu-3-sft-personas-instruction-following</li> <li>allenai/tulu-3-sft-personas-code</li> </ul> <p>Below is a minimal example of how <code>dataset_transformation.py</code> was used in the <code>finetune.py</code> script to mix, tokenize, and filter a dataset for SFT.</p> <p>You can run <code>python scripts/data/finetune_dataset_transformation.py</code> to see the output.</p> scripts/data/finetune_dataset_transformation.py<pre><code>from open_instruct.dataset_transformation import TokenizerConfig, get_cached_dataset_tulu, visualize_token\n\ntc = TokenizerConfig(\n    tokenizer_name_or_path=\"meta-llama/Llama-3.1-8B\",\n    tokenizer_revision=\"main\",\n    use_fast=True,\n    chat_template_name=\"tulu\",\n)\ndataset_mixer_list = [\n    \"allenai/tulu-3-sft-personas-instruction-following\", \"1.0\",\n    \"allenai/tulu-3-sft-personas-code\", \"1.0\",\n]\ndataset_mixer_list_splits = [\"train\"]\ndataset_transform_fn = [\"sft_tulu_tokenize_and_truncate_v1\", \"sft_tulu_filter_v1\"]\ntransform_fn_args = [\n    {\"max_seq_length\": 4096},\n    {},\n]\ntrain_dataset = get_cached_dataset_tulu(\n    dataset_mixer_list,\n    dataset_mixer_list_splits,\n    tc,\n    dataset_transform_fn,\n    transform_fn_args,\n    target_columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n    dataset_cache_mode=\"local\",\n)\nprint(train_dataset)\nvisualize_token(train_dataset[0][\"input_ids\"], tc.tokenizer)\n</code></pre> <p></p> <p>You can also use a different <code>chat_template_name</code>. For example,</p> <pre><code>tc = TokenizerConfig(\n    # ...\n    chat_template_name=\"simple_chat\",\n)\n#...\n</code></pre> <p>would give us</p> <p></p>"},{"location":"algorithms/dpo/","title":"Direct Preference Optimization (DPO)","text":"<p>We support Direct Preference Optimization (DPO) training on a variety of datasets.</p>"},{"location":"algorithms/dpo/#implemented-variants","title":"Implemented Variants","text":"<ul> <li><code>dpo_tune_cache.py</code> is the DPO implementation that directly optimizes model outputs based on human preferences.</li> </ul>"},{"location":"algorithms/dpo/#dpo_tune_cachepy","title":"<code>dpo_tune_cache.py</code>","text":"<p>This implementation has the following key features:</p> <ul> <li>Auto save the trained checkpoint to HuggingFace Hub</li> <li>Supports LigerKernel for optimized training with fused operations</li> <li>Implements the DPO algorithm for direct preference optimization</li> </ul> <p>There are several relevant implementation details:</p> <ol> <li>To save memory, we 1) cache the logprobs of the reference model on the dataset, 2) remove the reference model from the memory after the logprobs are computed. This means that you won't see the initial training losses for a while until the logprobs are computed.</li> <li>We use the <code>dpo_norm</code> loss type by default, which is a length-normalized loss. See the SimPO paper for more details.</li> </ol>"},{"location":"algorithms/dpo/#debug-single-gpu","title":"Debug (Single GPU)","text":"<p>You can run the script in a single GPU mode to debug the training process.</p> <pre><code>bash scripts/train/debug/dpo.sh\n</code></pre>"},{"location":"algorithms/dpo/#reproduce-allenaillama-31-tulu-3-8b-dpo-4-nodes","title":"Reproduce <code>allenai/Llama-3.1-Tulu-3-8B-DPO</code> (4 Nodes)","text":"<p>You can reproduce our <code>allenai/Llama-3.1-Tulu-3-8B-DPO</code> model by running the following command:</p> <pre><code>bash scripts/train/tulu3/dpo_8b.sh\n</code></pre> <p> </p> \ud83d\udc49 Tracked WandB Experiments (Click to expand) <p></p> Info <p>Based on our internal evaluation, the DPO model is roughly on par with the original <code>allenai/Llama-3.1-Tulu-3-8B-DPO</code> model, though there are some slight differences. Note that your results may vary slightly due to the random seeds used in the training. </p> <p></p> <p>For example, DROP is lower than the reference, but DROP can be quite brittle due to parsing issues (see below). </p> <p></p> Info <p>We haven't quite figured out how to make our internal evaluation toolchains more open yet. Stay tuned!</p>"},{"location":"algorithms/dpo/#reproduce-allenaiolmo-2-1124-7b-dpo-4-nodes","title":"Reproduce <code>allenai/OLMo-2-1124-7B-DPO</code> (4 Nodes)","text":"<p>You can reproduce our <code>allenai/OLMo-2-1124-7B-DPO</code> model by running the following command:</p> <pre><code>bash scripts/train/olmo2/dpo_7b.sh\n</code></pre> Info <p>If you are an external user, <code>mason.py</code> will print out the actual command being executed on our internal server, so you can modify the command as needed.</p> <p> </p> \ud83d\udc49 Tracked WandB Experiments (Click to expand) <p></p> Info <p>Based on our internal evaluation, the DPO model is roughly on par with the original <code>allenai/OLMo-2-1124-7B-DPO</code> model, though there are some slight differences. Note that your results may vary slightly due to the random seeds used in the training. </p> <p></p> Info <p>We haven't quite figured out how to make our internal evaluation toolchains more open yet. Stay tuned!</p>"},{"location":"algorithms/dpo/#reproduce-allenaiolmo-2-1124-13b-dpo-4-nodes","title":"Reproduce <code>allenai/OLMo-2-1124-13B-DPO</code> (4 Nodes)","text":"<p>You can reproduce our <code>allenai/OLMo-2-1124-13B-DPO</code> model by running the following command:</p> <pre><code>bash scripts/train/olmo2/dpo_13b.sh\n</code></pre> <p> </p> \ud83d\udc49 Tracked WandB Experiments (Click to expand) <p></p> Info <p>Based on our internal evaluation, the DPO model is roughly on par with the original <code>allenai/OLMo-2-1124-13B-DPO</code> model, though there are some slight differences. Note that your results may vary slightly due to the random seeds used in the training. </p> <p></p> Info <p>We haven't quite figured out how to make our internal evaluation toolchains more open yet. Stay tuned!</p>"},{"location":"algorithms/dpo/#training-metrics","title":"Training Metrics","text":"<p>During training, the following metrics are logged:</p> <ul> <li><code>training_step</code>: Current training step</li> <li><code>learning_rate</code>: The current learning rate from the learning rate scheduler</li> <li><code>epoch</code>: Current epoch (as a fraction of total dataset)</li> <li><code>train_loss</code>: The average training loss over the logged steps</li> <li><code>logps/chosen</code>: Average log probabilities for chosen responses</li> <li><code>logps/rejected</code>: Average log probabilities for rejected responses</li> </ul> <p>For DPO and DPO-norm loss types, additional metrics are logged:</p> <ul> <li><code>rewards/chosen</code>: Average rewards for chosen responses</li> <li><code>rewards/rejected</code>: Average rewards for rejected responses</li> <li><code>rewards/average</code>: Average of chosen and rejected rewards</li> <li><code>rewards/accuracy</code>: Accuracy of preference prediction</li> <li><code>rewards/margin</code>: Margin between chosen and rejected rewards</li> </ul> <p>When using load balancing loss (for OLMoE), the following metric is also logged:</p> <ul> <li><code>aux_loss</code>: Auxiliary loss for load balancing</li> </ul> <p>The metrics are logged every <code>logging_steps</code> steps (if specified) and provide insights into:</p> <ul> <li>Training progress (loss, learning rate, epoch)</li> <li>Model behavior (log probabilities, rewards)</li> <li>Preference learning (accuracy, margin)</li> <li>Resource utilization (auxiliary losses)</li> </ul>"},{"location":"algorithms/dpo/#acknowledgements","title":"Acknowledgements","text":"<p>We would like to thank the following projects for general infrastructure:</p> <ul> <li>DeepSpeedAI/DeepSpeed</li> <li>HuggingFace/Transformers </li> </ul>"},{"location":"algorithms/finetune/","title":"Supervised finetuning (SFT)","text":"<p>We support Supervised finetuning (SFT) on a variety of datasets.</p>"},{"location":"algorithms/finetune/#implemented-variants","title":"Implemented Variants","text":"<ul> <li><code>finetune.py</code> is the original SFT implementation.</li> </ul>"},{"location":"algorithms/finetune/#finetunepy","title":"<code>finetune.py</code>","text":"<p>This implementation has the following key features:</p> <ul> <li>Auto save the trained checkpoint to HuggingFace Hub</li> <li>Supports LigerKernel for optimized training with fused operations</li> </ul>"},{"location":"algorithms/finetune/#debug-single-gpu","title":"Debug (Single GPU)","text":"<p>You can run the script in a single GPU mode to debug the training process.</p> <pre><code>bash scripts/train/debug/finetune.sh\n</code></pre> <p></p>"},{"location":"algorithms/finetune/#reproduce-allenaillama-31-tulu-3-8b-sft-8-nodes","title":"Reproduce <code>allenai/Llama-3.1-Tulu-3-8B-SFT</code> (8 Nodes)","text":"<p>You can reproduce our <code>allenai/Llama-3.1-Tulu-3-8B-SFT</code> model by running the following command:</p> <pre><code>bash scripts/train/tulu3/finetune_8b.sh\n</code></pre> Info <p>If you are an external user, <code>mason.py</code> will print out the actual command being executed on our internal server, so you can modify the command as needed.</p> <p></p> <p> </p> \ud83d\udc49 Tracked WandB Experiments (Click to expand) <p></p> Info <p>Based on our internal evaluation, the SFT model is roughly on par with the original <code>allenai/Llama-3.1-Tulu-3-8B</code> model, though there are some slight differences. Note that your results may vary slightly due to the random seeds used in the training.</p> <p></p> Info <p>We haven't quite figured out how to make our internal evaluation toolchains more open yet. Stay tuned!</p>"},{"location":"algorithms/finetune/#reproduce-allenaiolmo-2-1124-7b-sft-8-nodes","title":"Reproduce <code>allenai/OLMo-2-1124-7B-SFT</code> (8 Nodes)","text":"<p>You can reproduce our <code>allenai/OLMo-2-1124-7B-SFT</code> model by running the following command:</p> <pre><code>bash scripts/train/olmo2/finetune_7b.sh\n</code></pre> <p> </p> \ud83d\udc49 Tracked WandB Experiments (Click to expand) <p></p> Info <p>Based on our internal evaluation, the SFT model is roughly on par with the original <code>allenai/OLMo-2-1124-7B</code> model, though there are some slight differences. Note that your results may vary slightly due to the random seeds used in the training.</p> <p></p> Info <p>We haven't quite figured out how to make our internal evaluation toolchains more open yet. Stay tuned!</p>"},{"location":"algorithms/finetune/#reproduce-allenaiolmo-2-1124-13b-sft-8-nodes","title":"Reproduce <code>allenai/OLMo-2-1124-13B-SFT</code> (8 Nodes)","text":"<p>You can reproduce our <code>allenai/OLMo-2-1124-13B-SFT</code> model by running the following command:</p> <pre><code>bash scripts/train/olmo2/finetune_13b.sh\n</code></pre> <p> </p> \ud83d\udc49 Tracked WandB Experiments (Click to expand) <p></p> Info <p>Based on our internal evaluation, the SFT model is roughly on par with the original <code>allenai/OLMo-2-1124-7B</code> model, though there are some slight differences. Note that your results may vary slightly due to the random seeds used in the training.</p> <p></p> Info <p>We haven't quite figured out how to make our internal evaluation toolchains more open yet. Stay tuned!</p>"},{"location":"algorithms/finetune/#reproduce-allenaiolmo-2-1124-32b-sft-8-nodes","title":"Reproduce <code>allenai/OLMo-2-1124-32B-SFT</code> (8 Nodes)","text":"<p>You can reproduce our <code>allenai/OLMo-2-1124-32B-SFT</code> model by running the following command:</p> <pre><code>bash scripts/train/olmo2/finetune_32b.sh\n</code></pre> <p> </p> \ud83d\udc49 Tracked WandB Experiments (Click to expand) <p></p> Info <p>Based on our internal evaluation, the SFT model is roughly on par with the original <code>allenai/OLMo-2-1124-7B</code> model, though there are some slight differences. Note that your results may vary slightly due to the random seeds used in the training.</p> <p></p> Info <p>We haven't quite figured out how to make our internal evaluation toolchains more open yet. Stay tuned!</p>"},{"location":"algorithms/finetune/#training-metrics","title":"Training Metrics","text":"<p>During training, the following metrics are logged:</p> <ul> <li><code>learning_rate</code>: The current learning rate from the learning rate scheduler</li> <li><code>train_loss</code>: The average training loss over the logged steps</li> <li><code>total_tokens</code>: Total number of tokens processed (excluding padding)</li> <li><code>per_device_tps</code>: Tokens per second processed per device (excluding padding)</li> <li><code>total_tokens_including_padding</code>: Total number of tokens including padding tokens</li> <li><code>per_device_tps_including_padding</code>: Tokens per second processed per device (including padding)</li> </ul> <p>The metrics are logged every <code>logging_steps</code> steps (if specified) and provide insights into: - Training progress (loss, learning rate) - Training efficiency (tokens per second) - Resource utilization (padding vs non-padding tokens)</p>"},{"location":"algorithms/finetune/#acknowledgements","title":"Acknowledgements","text":"<p>We would like to thank the following projects for general infrastructure:</p> <ul> <li>DeepSpeedAI/DeepSpeed</li> <li>HuggingFace/Transformers</li> </ul>"},{"location":"algorithms/grpo/","title":"Grouped Relative Policy Optimization (GRPO)","text":"<p>GRPO is an online RL method used in DeepSeek R1 paper and its first appearance is in DeepSeekMath</p>"},{"location":"algorithms/grpo/#implemented-variants","title":"Implemented Variants","text":"<ul> <li><code>grpo_fast.py</code> is a faster variant using packing techniques.</li> <li><code>grpo_vllm_thread_ray_gtrl.py</code> is a more vanilla GRPO implementation, using vLLM and Ray.</li> </ul>"},{"location":"algorithms/grpo/#grpo_fastpy","title":"<code>grpo_fast.py</code>","text":"<p>This implementation has the following features:</p> <ul> <li>Uses packing techniques to speed up the training process, inspired by Open-Reasoner-Zero/Open-Reasoner-Zero</li> <li>Uses a thread-based approach to parallelize the training and inference processes, based on Asynchronous RLHF.</li> <li>Uses a data preparation thread to prepare the data for the training process.</li> </ul> <p>In simpler tasks, we see 2x faster training, and even 10x faster for more complex tasks. With <code>grpo_fast.py</code>, we can run crank up <code>number_samples_per_prompt</code> and train on really large batch sizes.</p> <p>It implements additional optimizations:</p> <ul> <li><code>grpo_fast.py</code> also implements an optimization to skip zero gradient batches. If we solve a prompt 100% correct or 0% correct, the std of the group is 0. So <code>adv = (score - score.mean()) / (score.std + 1e-5) = 0 / 1e-5 = 0</code>, causing 0 gradients. <code>grpo_fast.py</code> will skip these batches before packing the sequences.</li> </ul> <p></p> <p>Figure taken from this discord thread by @the_real_jrb</p> <ul> <li><code>grpo_fast.py</code> only applies the verification reward if the format reward is enabled (via <code>--additive_format_reward False</code> by default). See (allenai/open-instruct/pull/659). A direct additive format reward is undesirable. In GRPO, the scale of the rewards is not relevant due to group normalization. For example, a group of [0, 0, 0, 0, 10], [0, 0, 0, 0, 11], [0, 0, 0, 0, 1] reward will have the same advantage.</li> </ul> <p>Now imagine there are cases where the model generates a really long response (8k) gen length, but only get the format reward right, GRPO will push up the probs for this long response even though the response is not really correct. As a result, when using the format reward directly, we see the response length of unsolved prompts to fluctuate significantly, causing stability issues.</p> <p></p>"},{"location":"algorithms/grpo/#debug-single-gpu","title":"Debug (Single GPU)","text":"<p>You can run the script in a single GPU mode to debug the training process.</p> <pre><code># single GPU\nbash scripts/train/debug/grpo_fast.sh\n# 3 GPU: 2 for training, 1 for inference (a more realistic setting for async training)\nbash scripts/train/debug/grpo_fast_3_gpu.sh\n</code></pre>"},{"location":"algorithms/grpo/#reproduce-allenaillama-31-tulu-31-8b-1-nodes","title":"Reproduce <code>allenai/Llama-3.1-Tulu-3.1-8B</code> (1 Nodes)","text":"<p>You can reproduce our <code>allenai/Llama-3.1-Tulu-3.1-8B</code> model by running the following command:</p> <pre><code>bash scripts/train/tulu3/grpo_fast_8b_single_node.sh\n</code></pre> Info <p>Here the <code>grpo_fast.py</code> actually use 6 GPUs for training and 2 GPUs for inference, so it's using less hardware but runs faster than <code>grpo_vllm_thread_ray_gtrl.py</code> which uses 2 nodes (12 GPUs for training and 4 GPUs for inference).</p> <p> </p> \ud83d\udc49 Tracked WandB Experiments (Click to expand) <p></p> Info <p>Below are some learning curves for the evaluation metrics during training. Basically, ifeval, gsm8k, and math:flex all go up. </p> <p></p> Info <p>Based on our internal evaluation, the GRPO model is roughly on par with the original <code>allenai/Llama-3.1-Tulu-3.1-8B</code> model, though there are some slight differences. Note that your results may vary slightly due to the random seeds used in the training. </p> <p></p> Info <p>We haven't quite figured out how to make our internal evaluation toolchains more open yet. Stay tuned!</p>"},{"location":"algorithms/grpo/#experimental-qwen-25-7b-grpo-fast-zero-style","title":"(\ud83e\uddea Experimental) Qwen 2.5 7B GRPO Fast Zero-style","text":"<p>We have</p> <pre><code>bash scripts/train/qwen/grpo_fast_7b.sh\n</code></pre> <p> </p> \ud83d\udc49 Tracked WandB Experiments (Click to expand) <p></p> Info <p>Below are some learning curves for the evaluation metrics during training. Basically, ifeval, gsm8k, and math:flex all go up. </p> <p></p> Info <p>We haven't quite figured out how to make our internal evaluation toolchains more open yet. Stay tuned!</p>"},{"location":"algorithms/grpo/#experimental-olmo2-7b-grpo-fast-zero-style","title":"(\ud83e\uddea Experimental) Olmo2 7B GRPO Fast Zero-style","text":"<p>We have</p> <pre><code>bash scripts/train/olmo2/grpo_fast_7b_zero.sh\n</code></pre> <p> </p> \ud83d\udc49 Tracked WandB Experiments (Click to expand) <p></p> Info <p>Below are some learning curves for the evaluation metrics during training. Basically, ifeval, gsm8k, and math:flex all go up. </p> <p></p> Info <p>We haven't quite figured out how to make our internal evaluation toolchains more open yet. Stay tuned!</p>"},{"location":"algorithms/grpo/#experimental-olmo2-13b-grpo-fast-zero-style","title":"(\ud83e\uddea Experimental) Olmo2 13B GRPO Fast Zero-style","text":"<p>We have</p> <pre><code>bash scripts/train/olmo2/grpo_fast_13b_zero.sh\n</code></pre> <p> </p> \ud83d\udc49 Tracked WandB Experiments (Click to expand) <p></p> Info <p>Below are some learning curves for the evaluation metrics during training. Basically, ifeval, gsm8k, and math:flex all go up. </p> <p></p> Info <p>We haven't quite figured out how to make our internal evaluation toolchains more open yet. Stay tuned!</p>"},{"location":"algorithms/grpo/#training-metrics","title":"Training Metrics","text":"<p>See the Training Metrics for <code>grpo_vllm_thread_ray_gtrl.py</code> below for general metrics. <code>grpo_fast.py</code> includes the following additional metrics:</p> <ul> <li><code>other/real_batch_size_ratio</code>: In GRPO, as we train we actually get smaller and smaller batch sizes. This is because if we solve a prompt 100% correct or 0% correct, the std of the group is 0. So <code>adv = (score - score.mean()) / (score.std + 1e-5) = 0 / 1e-5 = 0</code>, causing 0 gradients. This metric is the ratio of the samples that have gradients vs the total number of samples,</li> <li><code>other/packed_ratio</code>: The ratio of the packed sequences vs the total number of sequences. The lower the ratio, the more efficiently we have packed the sequences. E.g., if we have 100 sequences and the ratio is 0.1, it means we only have to do 10% of the forward passes than if we didn't pack.</li> </ul>"},{"location":"algorithms/grpo/#grpo_vllm_thread_ray_gtrlpy","title":"<code>grpo_vllm_thread_ray_gtrl.py</code>","text":"<p>This implementation has the following features:</p> <ul> <li>Uses a thread-based approach to parallelize the training and inference processes, based on Asynchronous RLHF.</li> <li>Uses vLLM and Ray to parallelize the training process, based on how OpenRLHF does it</li> </ul>"},{"location":"algorithms/grpo/#debug-single-gpu_1","title":"Debug (Single GPU)","text":"<p>You can run the script in a single GPU mode to debug the training process.</p> <pre><code>bash scripts/train/debug/grpo.sh\n</code></pre>"},{"location":"algorithms/grpo/#reproduce-allenaillama-31-tulu-31-8b-2-nodes","title":"Reproduce <code>allenai/Llama-3.1-Tulu-3.1-8B</code> (2 Nodes)","text":"<p>You can reproduce our <code>allenai/Llama-3.1-Tulu-3.1-8B</code> model by running the following command:</p> <pre><code>bash scripts/train/tulu3/grpo_8b.sh\n</code></pre> <p> </p> \ud83d\udc49 Tracked WandB Experiments (Click to expand) <p></p> Info <p>Below are some learning curves for the evaluation metrics during training. Basically, ifeval, gsm8k, and math:flex all go up. </p> <p></p> Info <p>Based on our internal evaluation, the GRPO model is roughly on par with the original <code>allenai/Llama-3.1-Tulu-3.1-8B</code> model, though there are some slight differences. Note that your results may vary slightly due to the random seeds used in the training. </p> <p></p>"},{"location":"algorithms/grpo/#reproduce-allenaiolmo-2-1124-7b-instruct-but-better-2-nodes","title":"Reproduce <code>allenai/OLMo-2-1124-7B-Instruct</code> but better (2 Nodes)","text":"<p>You can reproduce our <code>allenai/OLMo-2-1124-7B-Instruct</code> model by running the following command:</p> <pre><code>bash scripts/train/olmo2/grpo_7b.sh\n</code></pre> <p> </p> \ud83d\udc49 Tracked WandB Experiments (Click to expand) <p></p> Info <p>Below are some learning curves for the evaluation metrics during training. Basically, ifeval, gsm8k, and math:flex all go up. </p> <p></p> Info <p>Based on our internal evaluation, the GRPO model actually outperforms the original <code>allenai/OLMo-2-1124-7B-Instruct</code> model. This is mostly because the original <code>allenai/OLMo-2-1124-7B-Instruct</code> was trained with PPO, which may suffer from not using a outcome reward model to initialize the value model (since it uses a genreal RM to initialize the value model). Note that your results may vary slightly due to the random seeds used in the training. </p> <p></p>"},{"location":"algorithms/grpo/#experimental-qwen-25-7b-zero-style","title":"(\ud83e\uddea Experimental) Qwen 2.5 7B Zero-style","text":"<p>Here is a command to run GRPO on the <code>Qwen/Qwen2.5-7B</code> on ai2-adapt-dev/math_ground_truth_zs, which is simply a zero-shot version of the RLVR MATH dataset. The training is done starting from a base model, similar to how DeepSeek R1 does it.</p> <pre><code>bash scripts/train/qwen/grpo_7b.sh\n</code></pre> <p> </p> \ud83d\udc49 Tracked WandB Experiments (Click to expand) <p></p> Info <p>Below are some learning curves for the evaluation metrics during training. Basically, ifeval, gsm8k, and math:flex all go up. </p> <p></p> Info <p>We haven't quite figured out how to make our internal evaluation toolchains more open yet. Stay tuned!</p>"},{"location":"algorithms/grpo/#training-metrics_1","title":"Training Metrics","text":"<p>During training, the following metrics are logged:</p> <ul> <li><code>episode</code>: the global episode number training has gone through (e.g., <code>3000</code> means we have trained on 3000 data points already -- in the case of RLVR that is prompts, which can repeat)</li> <li><code>lr</code>: the current learning rate</li> <li><code>epoch</code>: the fraction or multiple of the epoch (e.g., <code>2.7</code> means we have trained on the dataset for 2 epochs and 70% of the third epoch)</li> <li><code>objective/kl</code>: the KL divergence between the current policy and the reference policy (sum of the KL divergence of each response token)</li> <li><code>objective/scores</code>: the scores of the current response, rated by a combination of reward model and other rewards (e.g., R1 style format reward, verifiable reward, etc.)</li> <li><code>objective/rlhf_reward</code>: the RLHF reward, which is <code>objective/scores</code> - <code>beta</code> * <code>objective/kl</code></li> <li><code>objective/non_score_reward</code>: <code>beta</code> * <code>objective/kl</code></li> <li><code>objective/entropy</code>: the entropy of the current policy</li> <li><code>objective/loss</code>: the GRPO loss</li> <li><code>objective/kl2</code>: the second variant of KL divergence used in the training process, calculated similarly to <code>objective/kl</code></li> <li><code>objective/kl3</code>: the third variant of KL divergence used in the training process, providing additional insights into policy divergence</li> <li><code>objective/scores_mean</code>: the mean of the scores of the current response, providing an average measure of response quality</li> <li><code>objective/reward_std</code>: the standard deviation of the rewards, indicating the variability in the reward distribution</li> <li><code>objective/verifiable_correct_rate</code>: the rate at which responses are verifiably correct, providing a measure of response accuracy</li> <li><code>loss/policy_avg</code>: the average policy loss, indicating the mean loss incurred during policy updates</li> <li><code>policy/approxkl_avg</code>: the average approximate KL divergence, used to monitor policy stability</li> <li><code>policy/clipfrac_avg</code>: the average fraction of updates where the policy was clipped, indicating how often clipping occurs</li> <li><code>policy/entropy_avg</code>: the average entropy of the policy, providing a measure of policy randomness</li> <li><code>time/from_scratch</code>: the time taken to train the model from scratch</li> <li><code>time/training</code>: the time taken to do one training step</li> <li><code>val/sequence_lengths</code>: the length of the sequences in the generated responses</li> <li><code>val/num_stop_token_ids</code>: the number of stop tokens in the generated responses</li> <li><code>val/ratio</code>: the mean ratio of the new policy to the old policy, used to assess policy updates</li> <li><code>val/ratio_var</code>: the variance of the ratio of the new policy to the old policy, indicating the variability in policy updates</li> <li><code>val/stop_token_rate</code>: the rate at which stop tokens appear in the responses, providing a measure of response termination</li> <li><code>val/format_scores</code>: the mean format scores, indicating the quality of response formatting (only logged if <code>add_r1_style_format_reward</code> is enabled)</li> <li><code>other/real_batch_size_ratio</code>: In GRPO, as we train we actually get smaller and smaller batch sizes. This is because if we solve a prompt 100% correct or 0% correct, the std of the group is 0. So <code>adv = (score - score.mean()) / (score.std + 1e-5) = 0 / 1e-5 = 0</code>, causing 0 gradients. This metric is the ratio of the samples that have gradients vs the total number of samples,</li> <li><code>other/packed_ratio</code>: The ratio of the packed sequences vs the total number of sequences. The lower the ratio, the more efficiently we have packed the sequences. E.g., if we have 100 sequences and the ratio is 0.1, it means we only have to do 10% of the forward passes than if we didn't pack.</li> </ul>"},{"location":"algorithms/grpo/#acknowledgements","title":"Acknowledgements","text":"<p>We would like to thank the following resources for GRPO theory:</p> <ul> <li>DeepSeek R1</li> <li>DeepSeekMath</li> <li>Asynchronous RLHF</li> </ul> <p>We would like to thank the following resources for GRPO implementation and Ray usage:</p> <ul> <li>Packing Techniques</li> <li>OpenRLHF/OpenRLHF</li> <li>Open-Reasoner-Zero/Open-Reasoner-Zero</li> </ul> <p>We would like to thank the following projects for general infrastructure:</p> <ul> <li>vLLM</li> <li>Ray</li> <li>DeepSpeedAI/DeepSpeed</li> <li>HuggingFace/Transformers</li> </ul>"},{"location":"algorithms/online_dpo/","title":"Reward model training","text":"<p><code>open_instruct/online_dpo.py</code> contains the script for training online DPO models.</p>"},{"location":"algorithms/online_dpo/#get-started","title":"Get started","text":"<p>In the sections below, we will include some examples on how to train models and demonstrating different features. A couple of notes:</p> <ul> <li>You should adjust your <code>per_device_train_batch_size</code> and <code>gradient_accumulation_steps</code> accordingly to maximize throughput on a particular GPU type.</li> <li>If you set <code>take_top_bottom_generation</code>, you can use a <code>num_generation_per_prompt</code> larger than 2 -- it just takes the top and bottom scoring generations for each prompt.</li> <li>For the examples below, we use <code>mason.py</code> to invoke experiment orchastration on Ai2's cluster. For external users, you can copy the command after the <code>--</code> and run it on your system or debug locally. For example: the documentation will have commands like the following, but you can just run <code>$YOUR_COMMAND</code> on your system and make sure it matches <code>$NUM_GPUS</code>.<ul> <li>You can you <code>--image costah/open_instruct_onlinedpo2</code> to specify a custom image or if you don't specify any it's going to use the default image.</li> <li>If you installed your python on NFS you can run a debug mode by not toggling <code>--pure_docker_mode</code> and it will mount your python environment on the docker container.</li> </ul> </li> </ul> <pre><code>python mason.py \\\n    --cluster ai2/jupiter \\\n    --image costah/open_instruct_onlinedpo2 --pure_docker_mode \\\n    --priority preemptible \\\n    --budget ai2/jupiter \\\n    --gpus $NUM_GPUS -- $YOUR_COMMAND\n</code></pre> <p>WARNING: This script is not battle-tested. There may be bugs and issues -- please report them! Use at your own risk.</p>"},{"location":"algorithms/online_dpo/#level-0-single-gpu-quick-debug-should-take-less-than-10-minutes-to-finish","title":"Level 0: single GPU; quick debug. Should take less than 10 minutes to finish","text":"<pre><code>python open_instruct/online_dpo_vllm_thread.py \\\n    --dataset_mixer_list trl-internal-testing/tldr-preference-sft-trl-style 1.0 \\\n    --dataset_mixer_eval_list trl-internal-testing/tldr-preference-sft-trl-style 1.0 \\\n    --dataset_mixer_list_splits train \\\n    --dataset_mixer_eval_list_splits validation \\\n    --max_token_length 1024 \\\n    --max_prompt_token_length 512 \\\n    --model_name_or_path cleanrl/EleutherAI_pythia-1b-deduped__sft__tldr \\\n    --reward_model_path cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr \\\n    --non_stop_penalty \\\n    --stop_token eos \\\n    --chat_template simple_concat_with_space \\\n    --learning_rate 3e-6 \\\n    --total_episodes 3000 \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 64 \\\n    --max_token_length 1024 \\\n    --max_prompt_token_length 512 \\\n    --beta 0.1 \\\n    --output_dir models/rm/rm_sentiment_1b \\\n    --single_gpu_mode \\\n    --hf_metadata_dataset \"\" \\\n    --no_try_launch_beaker_eval_jobs \\\n    --gradient_checkpointing \\\n    --with_tracking \\\n    --push_to_hub \\\n    --vllm_gpu_memory_utilization 0.5 \\\n    --actor_num_gpus_per_node 1 \\\n    --local_mini_batch_size 32 \\\n    --num_mini_batches 1 \\\n    --vllm_sync_backend gloo\n\n# LEVEL 0.1: two GPU; quick debug; using 1 GPU for training and 1 GPU for vllm generation via --vllm_device cuda:1\npython open_instruct/online_dpo_vllm_thread.py \\\n    --dataset_mixer_list trl-internal-testing/tldr-preference-sft-trl-style 1.0 \\\n    --dataset_mixer_eval_list trl-internal-testing/tldr-preference-sft-trl-style 1.0 \\\n    --dataset_mixer_list_splits train \\\n    --dataset_mixer_eval_list_splits validation \\\n    --max_token_length 1024 \\\n    --max_prompt_token_length 512 \\\n    --model_name_or_path cleanrl/EleutherAI_pythia-1b-deduped__sft__tldr \\\n    --reward_model_path cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr \\\n    --non_stop_penalty \\\n    --stop_token eos \\\n    --chat_template simple_concat_with_space \\\n    --learning_rate 3e-6 \\\n    --total_episodes 3000 \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 2 \\\n    --gradient_accumulation_steps 64 \\\n    --max_token_length 1024 \\\n    --max_prompt_token_length 512 \\\n    --num_train_epochs 1 \\\n    --beta 0.1 \\\n    --output_dir models/rm/rm_sentiment_1b \\\n    --single_gpu_mode \\\n    --vllm_gpu_memory_utilization 0.5 \\\n    --actor_num_gpus_per_node 1 \\\n    --local_mini_batch_size 32 \\\n    --num_mini_batches 1 \\\n    --vllm_sync_backend gloo\n    --no_try_launch_beaker_eval_jobs \\\n    --gradient_checkpointing \\\n    --with_tracking \\\n    --push_to_hub\n</code></pre>"},{"location":"algorithms/online_dpo/#old-examples","title":"Old examples","text":"<p>These examples use the older form of the script available at https://github.com/allenai/open-instruct/blob/efa36849bd65db7614e6729344df94ace83b7228/open_instruct/online_dpo_vllm_thread.py. These require older package versions, use at your own risk.</p>"},{"location":"algorithms/online_dpo/#level-1-8-gpu-tldr-summarization","title":"LEVEL 1: 8 GPU; TL;DR summarization","text":"<p>Here we are using --vllm_device cuda:7 to say we want to launch the vllm generation engine on the 8th GPU (or GPU_7 using 0 index) <pre><code># for running TL;DR you can likely use GPUs with less memory\npython mason.py \\\n    --image nathanl/open_instruct_auto --pure_docker_mode \\\n    --cluster ai2/jupiter \\\n    --priority normal \\\n    --resumable \\\n    --preemptible \\\n    --budget ai2/jupiter \\\n    --gpus 8 -- accelerate launch --num_processes 7 --config_file configs/ds_configs/deepspeed_zero3.yaml \\\n     open_instruct/online_dpo_vllm_thread.py \\\n    --dataset_mixer '{\"trl-internal-testing/tldr-preference-sft-trl-style\": 1.0}' \\\n    --dataset_train_splits train \\\n    --dataset_eval_mixer '{\"trl-internal-testing/tldr-preference-sft-trl-style\": 1.0}' \\\n    --dataset_eval_splits validation \\\n    --max_token_length 1024 \\\n    --max_prompt_token_length 512 \\\n    --learning_rate 3e-6 \\\n    --output_dir models/minimal/online_dpo_vllm_thread_tldr \\\n    --per_device_train_batch_size 16 \\\n    --local_rollout_forward_batch_size 32 \\\n    --gradient_accumulation_steps 4 \\\n    --num_epochs 1 \\\n    --num_mini_batches 1 \\\n    --total_episodes 1000000 \\\n    --model_name_or_path cleanrl/EleutherAI_pythia-1b-deduped__sft__tldr  \\\n    --reward_model_path cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr \\\n    --non_stop_penalty \\\n    --stop_token eos \\\n    --beta 0.1 \\\n    --response_length 53 \\\n    --with_tracking \\\n    --push_to_hub \\\n    --hf_metadata_dataset '\"\"' \\\n    --no_try_launch_beaker_eval_jobs \\\n    --single_gpu_mode\n</code></pre></p> <ul> <li>Tracked experiment: https://wandb.ai/ai2-llm/open_instruct_internal/runs/fub45jhm</li> <li>Trained model: https://huggingface.co/vwxyzjn/online_dpo_vllm_thread__cleanrl_EleutherAI_pythia-1b-deduped__sft__tldr/tree/online_dpo_vllm_thread__1__1726080959</li> </ul>"},{"location":"algorithms/online_dpo/#level-2-8-gpu-huggingface-no-robot","title":"LEVEL 2: 8 GPU; Huggingface no robot","text":"<pre><code># for running chat based models you should use an 8xH100 node.\npython mason.py \\\n    --cluster ai2/jupiter \\\n    --image nathanl/open_instruct_auto --pure_docker_mode \\\n    --workspace ai2/tulu-3-dev \\\n    --priority high \\\n    --preemptible \\\n    --budget ai2/jupiter \\\n    --gpus 8 -- accelerate launch --num_processes 7 --config_file configs/ds_configs/deepspeed_zero3.yaml \\\n    open_instruct/online_dpo_vllm_thread.py \\\n    --exp_name \"online_dpo_vllm_thread_beta_0.03\" \\\n    --dataset_mixer '{\"HuggingFaceH4/no_robots\": 1.0}' \\\n    --dataset_train_splits train \\\n    --dataset_eval_mixer '{\"HuggingFaceH4/no_robots\": 1.0}' \\\n    --dataset_eval_splits test \\\n    --max_token_length 1024 \\\n    --max_prompt_token_length 512 \\\n    --learning_rate 8e-7 \\\n    --output_dir /output/ \\\n    --chat_template tulu \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 32 \\\n    --local_rollout_forward_batch_size 1 \\\n    --vllm_device cuda:7 \\\n    --num_epochs 1 \\\n    --num_mini_batches 1 \\\n    --total_episodes 100000 \\\n    --model_name_or_path allenai/open_instruct_dev  \\\n    --model_revision costa_finetune_tulu3_8b_norobot__meta-llama_Meta-Llama-3.1-8B__42__1725559869 \\\n    --reward_model_path vwxyzjn/reward_modeling__allenai_open_instruct_dev \\\n    --reward_model_revision reward_modeling__1__1725760619 \\\n    --non_stop_penalty \\\n    --stop_token eos \\\n    --penalty_reward_value -10.0 \\\n    --beta 0.03 \\\n    --num_evals 3 \\\n    --seed 3 \\\n    --response_length 1024 \\\n    --gradient_checkpointing \\\n    --with_tracking \\\n    --push_to_hub\n</code></pre> <ul> <li>Tracked experiment: https://wandb.ai/ai2-llm/open_instruct_internal/runs/do4nuqhh</li> <li>Trained model: https://huggingface.co/vwxyzjn/online_dpo_vllm_thread_beta_0.03__allenai_open_instruct_dev/tree/online_dpo_vllm_thread_beta_0.03__3__1726200312</li> </ul>"},{"location":"algorithms/online_dpo/#level-3-8-gpu-training-on-ultrafeedback-rm","title":"LEVEL 3: 8 GPU; Training on ultrafeedback RM","text":"<pre><code># for running chat based models you should use an 8xH100 node.\npython mason.py \\\n    --cluster ai2/jupiter \\\n    --image nathanl/open_instruct_auto --pure_docker_mode \\\n    --workspace ai2/tulu-3-dev \\\n    --priority high \\\n    --preemptible \\\n    --budget ai2/jupiter \\\n    --gpus 8 -- accelerate launch --num_processes 7 --config_file configs/ds_configs/deepspeed_zero3.yaml \\\n    open_instruct/online_dpo_vllm_thread.py \\\n    --exp_name \"online_dpo_vllm_thread_beta_0.03\" \\\n    --dataset_mixer '{\"allenai/ultrafeedback_binarized_cleaned\": 1.0}' \\\n    --sft_messages_key chosen \\\n    --dataset_train_splits train_prefs \\\n    --dataset_eval_mixer '{\"allenai/ultrafeedback_binarized_cleaned\": 1.0}' \\\n    --dataset_eval_splits test_prefs \\\n    --max_token_length 1024 \\\n    --max_prompt_token_length 512 \\\n    --learning_rate 8e-7 \\\n    --output_dir /output/ \\\n    --chat_template tulu \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 32 \\\n    --local_rollout_forward_batch_size 1 \\\n    --vllm_device cuda:7 \\\n    --num_epochs 1 \\\n    --num_mini_batches 1 \\\n    --total_episodes 300000 \\\n    --model_name_or_path allenai/open_instruct_dev  \\\n    --model_revision finetune__meta-llama_Meta-Llama-3.1-8B__42__1725751338 \\\n    --reward_model_path vwxyzjn/reward_modeling__allenai_llama-3-tulu-2-8b \\\n    --reward_model_revision reward_modeling__1__1726175049 \\\n    --non_stop_penalty \\\n    --stop_token eos \\\n    --penalty_reward_value -10.0 \\\n    --beta 0.03 \\\n    --num_evals 3 \\\n    --response_length 1024 \\\n    --gradient_checkpointing \\\n    --with_tracking \\\n    --push_to_hub\n</code></pre> <ul> <li>Tracked experiment: https://wandb.ai/ai2-llm/open_instruct_internal/runs/le8luk2u</li> <li>Trained model: https://huggingface.co/vwxyzjn/online_dpo_vllm_thread_beta_0.03__allenai_open_instruct_dev/tree/online_dpo_vllm_thread_beta_0.03__1__1726282895</li> </ul>"},{"location":"algorithms/online_dpo/#if-you-want-to-use-beaker-datasets","title":"If you want to use beaker datasets","text":"<p>If you want to use beaker datasets, you need to mount your datasets using --beaker_datasets. An example command with beaker datasets models:</p> <pre><code>python mason.py \\\n    --cluster ai2/jupiter \\\n    --image nathanl/open_instruct_auto \\\n    --pure_docker_mode \\\n    --workspace ai2/tulu-3-dev \\\n    --priority high \\\n    --preemptible \\\n    --budget ai2/jupiter \\\n    --beaker_datasets /model:01J6DC8YQ291QA3QEYQTM3CBHE /reward_model:01J834TT3SB6PTB3QYPH33YJ6M \\\n    --gpus 8 -- accelerate launch --num_processes 7 --config_file configs/ds_configs/deepspeed_zero3.yaml \\\n    open_instruct/online_dpo_vllm_thread.py \\\n    --exp_name \"online_dpo_vllm_thread_beta_0.03\" \\\n    --dataset_mixer '{\"allenai/ultrafeedback_binarized_cleaned\": 1.0}' \\\n    --sft_messages_key chosen \\\n    --dataset_train_splits train_prefs \\\n    --dataset_eval_mixer '{\"allenai/ultrafeedback_binarized_cleaned\": 1.0}' \\\n    --dataset_eval_splits test_prefs \\\n    --max_token_length 1024 \\\n    --max_prompt_token_length 512 \\\n    --learning_rate 8e-7 \\\n    --output_dir /output/ \\\n    --chat_template tulu \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 32 \\\n    --local_rollout_forward_batch_size 1 \\\n    --vllm_device cuda:7 \\\n    --num_epochs 1 \\\n    --num_mini_batches 1 \\\n    --total_episodes 300000 \\\n    --model_name_or_path /model \\\n    --reward_model_path /reward_model \\\n    --non_stop_penalty \\\n    --stop_token eos \\\n    --penalty_reward_value -10.0 \\\n    --beta 0.03 \\\n    --num_evals 3 \\\n    --response_length 1024 \\\n    --gradient_checkpointing \\\n    --with_tracking \\\n    --push_to_hub\n</code></pre>"},{"location":"algorithms/online_dpo/#quality-of-life-tools","title":"Quality of life tools","text":"<p>Note that when running with <code>--push_to_hub</code> and <code>--with_tracking</code>, the HF repo is automatically tracked to wandb, so we link the tracked run and the trained model.</p> <p></p> <p>Furthermore, we also track the dataset length visualization in wandb (see detail in here)</p> <p></p> <p>Finally, we also include samples</p> <p></p>"},{"location":"algorithms/online_dpo/#explanation-of-the-logged-metrics","title":"Explanation of the logged metrics","text":"<ul> <li><code>episode</code>: the global episode number training has gone through (e.g., <code>3000</code> means we have trained on 3000 data points already)</li> <li><code>lr</code>: the current learning rate</li> <li><code>epoch</code>: the fraction or multiple of the epoch (e.g., <code>2.7</code> means we have trained on the dataset for 2 epochs and 70% of the third epoch)</li> <li><code>objective/kl</code>: the KL divergence between the current policy and the reference policy (sum of the KL divergence of each response token)</li> <li><code>objective/scores</code>: the scores of the current response, rated by a reward model</li> <li><code>objective/rlhf_reward</code>: the RLHF reward, which is <code>objective/scores</code> - <code>beta</code> * <code>objective/kl</code></li> <li><code>objective/non_score_reward</code>: <code>beta</code> * <code>objective/kl</code></li> <li><code>objective/entropy</code>: the entropy of the current policy</li> <li><code>objective/scores_margin</code>: the difference between the chosen response scores and the rejected response scores. We pick the chosen response to be the response with higher scores, and the rejected response to be the response with lower scores</li> <li><code>objective/loss</code>: the DPO loss</li> <li><code>logps/chosen</code>: the log probability of the chosen response</li> <li><code>logps/rejected</code>: the log probability of the rejected response</li> <li><code>reward/chosen</code>: the implicit DPO reward of the chosen response</li> <li><code>reward/rejected</code>: the implicit DPO reward of the rejected response</li> <li><code>reward_margin</code>: the difference between the implicit PDO chosen reward and the implicit rejected reward</li> <li><code>time/from_scratch</code>: the time taken to train the model from scratch</li> <li><code>time/training</code>: the time taken to do one training step</li> <li><code>val/sequence_lengths</code>: the length of the sequences in the generated responses</li> <li><code>val/num_stop_token_ids</code>: the number of stop tokens in the generated responses</li> </ul>"},{"location":"algorithms/online_dpo/#implementation-details","title":"Implementation details","text":"<p>These are relevant implementation details on reward modeling:</p> <ol> <li>The tokenizer pads from the left, so it's straightforward to do generations.</li> <li>Disable dropout in the model: this is an implementation detail in PPO training (see p.3. in https://arxiv.org/pdf/1909.08593).</li> <li>Layer initialization: we initialize the score's weight according to <code>std=1 / np.sqrt(model.config.hidden_size + 1)</code> (see p. 11 in https://arxiv.org/abs/2009.01325)</li> <li>Vocab size for RM and Policy: we use the same vocab size for the reward model and the policy model. This is to ensure that the reward model can score all the tokens in the policy model. We added a <code>ValueError</code> for situations when <code>policy.config.vocab_size != reward_model.config.vocab_size</code>.</li> <li>Retrain on the same prompts: say we only have 10k prompts but we specified <code>--episodes 100k</code>, we will shuffle the prompts at every 10k episodes and retrain on them.</li> <li>Truncate responses at the stop token: we truncate the responses at the <code>--stop_token eos</code> to ensure the generation is stopped at the stop token.</li> <li>Non-stop penalty: we use a non-stop penalty to the reward model to penalize the model for not stopping at the stop token. For example, if the model does not end at the stop token, we penalize the model by <code>-10.0</code> (see <code>--penalty_reward_value -10.0</code>).</li> <li>Async training and generation: we follow the architecture in https://arxiv.org/abs/2310.00036 to do rollout and training asynchronously. This is to ensure that the training is not bottlenecked by the generation.</li> <li>We also optimizes online DPO runtime by re-using the model training logprob to save an additional forward pass; notice that this does impact KL calculation and causes some numerical issues. See https://github.com/allenai/open-instruct/pull/364 for more detail.</li> </ol> <p><pre><code>import queue\nimport threading\nimport time\n\nclass Agent():\n    def __init__(self):\n        self.param = 1\n\n    def learn(self, data):\n        self.param += 1\n\ndef query_generator_fn():\n    for i in range(1, 100):\n        yield i\n\n\nITER = 7\nbatch_size = 32\nagent = Agent()\ndata_Q = queue.Queue(maxsize=1)\nparam_and_query_Q = queue.Queue(maxsize=1)\ndef actor():\n    for i in range(1, ITER + 1):\n        params, query = param_and_query_Q.get()\n        data = params\n        print(f\"[actor] generating data \u03c0_{params} -&gt; p_{query} D_\u03c0_{data}\")\n        time.sleep(1) # simulate data generation\n        data_Q.put((query, data))\n\nactor_thread = threading.Thread(target=actor)\nactor_thread.start()\n\n# initial param put\ngenerator = query_generator_fn()\nnext_queries = next(generator)\nparam_and_query_Q.put((agent.param, next_queries))\n\n# cleanba style stuff\nasync_mode = True\nstart_time = time.time()\nfor g in range(1, ITER + 1):\n    queries = next_queries\n    if async_mode:\n        if g != 1:\n            next_queries = next(generator)\n        param_and_query_Q.put((agent.param, queries))\n    else:\n        if g != 1:\n            next_queries = next(generator)\n            param_and_query_Q.put((agent.param, next_queries)) # note the indent here is different\n    _, data = data_Q.get()\n    old_param = agent.param\n    agent.learn(data)\n    time.sleep(1) # simulate training\n    print(f\"--[leaner] get \u03c0_{old_param} -&gt;  p_{queries} D_\u03c0_{data} -&gt; \u03c0_{agent.param}, time: {time.time() - start_time}\")\nactor_thread.join()\n</code></pre> <pre><code>[actor] generating data \u03c0_1 -&gt; p_1 D_\u03c0_1\n[actor] generating data \u03c0_1 -&gt; p_1 D_\u03c0_1\n--[leaner] get \u03c0_1 -&gt;  p_1 D_\u03c0_1 -&gt; \u03c0_2, time: 2.0022709369659424\n[actor] generating data \u03c0_2 -&gt; p_1 D_\u03c0_2\n--[leaner] get \u03c0_2 -&gt;  p_1 D_\u03c0_1 -&gt; \u03c0_3, time: 3.003502607345581\n[actor] generating data \u03c0_3 -&gt; p_2 D_\u03c0_3\n--[leaner] get \u03c0_3 -&gt;  p_2 D_\u03c0_2 -&gt; \u03c0_4, time: 4.004725933074951\n[actor] generating data \u03c0_4 -&gt; p_3 D_\u03c0_4\n--[leaner] get \u03c0_4 -&gt;  p_3 D_\u03c0_3 -&gt; \u03c0_5, time: 5.005916118621826\n[actor] generating data \u03c0_5 -&gt; p_4 D_\u03c0_5\n--[leaner] get \u03c0_5 -&gt;  p_4 D_\u03c0_4 -&gt; \u03c0_6, time: 6.007085800170898\n[actor] generating data \u03c0_6 -&gt; p_5 D_\u03c0_6\n--[leaner] get \u03c0_6 -&gt;  p_5 D_\u03c0_5 -&gt; \u03c0_7, time: 7.007669448852539\n--[leaner] get \u03c0_7 -&gt;  p_6 D_\u03c0_6 -&gt; \u03c0_8, time: 8.009439706802368\n</code></pre></p>"},{"location":"algorithms/ppo/","title":"Proximal Policy Optimization (PPO)","text":""},{"location":"algorithms/ppo/#implemented-variants","title":"Implemented Variants","text":"<ul> <li><code>open_instruct/grpo_vllm_thread_ray_gtrl.py</code> contains the script for training PPO models.</li> </ul>"},{"location":"algorithms/ppo/#ppo_vllm_thread_ray_gtrlpy","title":"<code>ppo_vllm_thread_ray_gtrl.py</code>","text":"<p>This implementation has the following features:</p> <ul> <li>Uses a thread-based approach to parallelize the training and inference processes, based on Asynchronous RLHF.</li> <li>Uses vLLM and Ray to parallelize the training process, based on how OpenRLHF does it</li> </ul>"},{"location":"algorithms/ppo/#debug-single-gpu","title":"Debug (Single GPU)","text":"<p>You can run the script in a single GPU mode to debug the training process.</p> <pre><code>bash scripts/train/debug/ppo.sh\n</code></pre>"},{"location":"algorithms/ppo/#reproduce-allenaillama-31-tulu-31-8b-2-nodes","title":"Reproduce <code>allenai/Llama-3.1-Tulu-3.1-8B</code> (2 Nodes)","text":"<p>You can reproduce our <code>allenai/Llama-3.1-Tulu-3-8B</code> model by running the following command:</p> <pre><code>bash scripts/train/tulu3/ppo_8b.sh\n</code></pre>"},{"location":"algorithms/ppo/#quality-of-life-tools","title":"Quality of life tools","text":"<p>Note that when running with <code>--push_to_hub</code> and <code>--with_tracking</code>, the HF repo is automatically tracked to wandb, so we link the tracked run and the trained model.</p> <p></p> <p>Furthermore, we also track the dataset length visualization in wandb (see detail in here)</p> <p></p> <p>Finally, we also include samples </p> <p></p>"},{"location":"algorithms/ppo/#training-metrics","title":"Training Metrics","text":"<p>During training, the following metrics are logged:</p> <ul> <li><code>episode</code>: the global episode number training has gone through (e.g., <code>3000</code> means we have trained on 3000 data points already -- in the case of RLVR that is prompts, which can repeat)</li> <li><code>lr</code>: the current learning rate</li> <li><code>epoch</code>: the fraction or multiple of the epoch (e.g., <code>2.7</code> means we have trained on the dataset for 2 epochs and 70% of the third epoch)</li> <li><code>objective/kl</code>: the KL divergence between the current policy and the reference policy (sum of the KL divergence of each response token)</li> <li><code>objective/scores</code>: the scores of the current response, rated by a combination of reward model and other rewards (e.g., R1 style format reward, verifiable reward, etc.)</li> <li><code>objective/rlhf_reward</code>: the RLHF reward, which is <code>objective/scores</code> - <code>beta</code> * <code>objective/kl</code></li> <li><code>objective/non_score_reward</code>: <code>beta</code> * <code>objective/kl</code></li> <li><code>objective/entropy</code>: the entropy of the current policy</li> <li><code>objective/loss</code>: the PPO loss</li> <li><code>objective/verifiable_correct_rate</code>: the rate at which responses are verifiably correct, providing a measure of response accuracy</li> <li><code>loss/policy_avg</code>: the average policy loss, indicating the mean loss incurred during policy updates</li> <li><code>policy/approxkl_avg</code>: the average approximate KL divergence, used to monitor policy stability</li> <li><code>policy/clipfrac_avg</code>: the average fraction of updates where the policy was clipped, indicating how often clipping occurs</li> <li><code>policy/entropy_avg</code>: the average entropy of the policy, providing a measure of policy randomness</li> <li><code>time/from_scratch</code>: the time taken to train the model from scratch</li> <li><code>time/training</code>: the time taken to do one training step</li> <li><code>val/sequence_lengths</code>: the length of the sequences in the generated responses</li> <li><code>val/num_stop_token_ids</code>: the number of stop tokens in the generated responses</li> </ul>"},{"location":"algorithms/ppo/#implementation-details","title":"Implementation details","text":"<p>These are relevant implementation details on reward modeling:</p> <ol> <li>The tokenizer pads from the left, so it's straightforward to do generations.</li> <li>Disable dropout in the model: this is an implementation detail in PPO training (see p.3. in https://arxiv.org/pdf/1909.08593).</li> <li>Layer initialization: we initialize the score's weight according to <code>std=1 / np.sqrt(model.config.hidden_size + 1)</code> (see p. 11 in https://arxiv.org/abs/2009.01325)</li> <li>Vocab size for RM and Policy: we use the same vocab size for the reward model and the policy model. This is to ensure that the reward model can score all the tokens in the policy model. We added a <code>ValueError</code> for situations when <code>policy.config.vocab_size != reward_model.config.vocab_size</code>.</li> <li>Retrain on the same prompts: say we only have 10k prompts but we specified <code>--episodes 100k</code>, we will shuffle the prompts at every 10k episodes and retrain on them.</li> <li>Truncate responses at the stop token: we truncate the responses at the <code>--stop_token eos</code> to ensure the generation is stopped at the stop token.</li> <li>Non-stop penalty: we use a non-stop penalty to the reward model to penalize the model for not stopping at the stop token. For example, if the model does not end at the stop token, we penalize the model by <code>-10.0</code> (see <code>--penalty_reward_value -10.0</code>).</li> <li>Async training and generation: we follow the architecture in https://arxiv.org/abs/2310.00036 to do rollout and training asynchronously. This is to ensure that the training is not bottlenecked by the generation.</li> </ol> <p><pre><code>import queue\nimport threading\nimport time\n\nclass Agent():\n    def __init__(self):\n        self.param = 1\n\n    def learn(self, data):\n        self.param += 1\n\ndef query_generator_fn():\n    for i in range(1, 100):\n        yield i\n\n\nITER = 7\nbatch_size = 32\nagent = Agent()\ndata_Q = queue.Queue(maxsize=1)\nparam_and_query_Q = queue.Queue(maxsize=1)\ndef actor():\n    for i in range(1, ITER + 1):\n        params, query = param_and_query_Q.get()\n        data = params\n        print(f\"[actor] generating data \u03c0_{params} -&gt; p_{query} D_\u03c0_{data}\")\n        time.sleep(1) # simulate data generation\n        data_Q.put((query, data))\n\nactor_thread = threading.Thread(target=actor)\nactor_thread.start()\n\n# initial param put\ngenerator = query_generator_fn()\nnext_queries = next(generator)\nparam_and_query_Q.put((agent.param, next_queries))\n\n# cleanba style stuff\nasync_mode = True\nstart_time = time.time()\nfor g in range(1, ITER + 1):\n    queries = next_queries\n    if async_mode:\n        if g != 1:\n            next_queries = next(generator)\n        param_and_query_Q.put((agent.param, next_queries))\n    else:\n        if g != 1:\n            next_queries = next(generator)\n            param_and_query_Q.put((agent.param, next_queries)) # note the indent here is different\n            queries = next_queries\n    _, data = data_Q.get()\n    old_param = agent.param\n    agent.learn(data)\n    time.sleep(1) # simulate training\n    print(f\"--[leaner] get \u03c0_{old_param} -&gt;  p_{queries} D_\u03c0_{data} -&gt; \u03c0_{agent.param}, time: {time.time() - start_time}\")\nactor_thread.join()\n</code></pre> <pre><code># async_mode = True\n[actor] generating data \u03c0_1 -&gt; p_1 D_\u03c0_1\n[actor] generating data \u03c0_1 -&gt; p_1 D_\u03c0_1\n--[leaner] get \u03c0_1 -&gt;  p_1 D_\u03c0_1 -&gt; \u03c0_2, time: 2.0003671646118164\n[actor] generating data \u03c0_2 -&gt; p_2 D_\u03c0_2\n--[leaner] get \u03c0_2 -&gt;  p_1 D_\u03c0_1 -&gt; \u03c0_3, time: 3.0012056827545166\n[actor] generating data \u03c0_3 -&gt; p_3 D_\u03c0_3\n--[leaner] get \u03c0_3 -&gt;  p_2 D_\u03c0_2 -&gt; \u03c0_4, time: 4.001934766769409\n[actor] generating data \u03c0_4 -&gt; p_4 D_\u03c0_4\n--[leaner] get \u03c0_4 -&gt;  p_3 D_\u03c0_3 -&gt; \u03c0_5, time: 5.002779722213745\n[actor] generating data \u03c0_5 -&gt; p_5 D_\u03c0_5\n--[leaner] get \u03c0_5 -&gt;  p_4 D_\u03c0_4 -&gt; \u03c0_6, time: 6.003664970397949\n[actor] generating data \u03c0_6 -&gt; p_6 D_\u03c0_6\n--[leaner] get \u03c0_6 -&gt;  p_5 D_\u03c0_5 -&gt; \u03c0_7, time: 7.004390716552734\n--[leaner] get \u03c0_7 -&gt;  p_6 D_\u03c0_6 -&gt; \u03c0_8, time: 8.00534439086914\n\n# async_mode = False\n[actor] generating data \u03c0_1 -&gt; p_1 D_\u03c0_1\n--[leaner] get \u03c0_1 -&gt;  p_1 D_\u03c0_1 -&gt; \u03c0_2, time: 2.000866174697876\n[actor] generating data \u03c0_2 -&gt; p_2 D_\u03c0_2\n--[leaner] get \u03c0_2 -&gt;  p_2 D_\u03c0_2 -&gt; \u03c0_3, time: 4.002583980560303\n[actor] generating data \u03c0_3 -&gt; p_3 D_\u03c0_3\n--[leaner] get \u03c0_3 -&gt;  p_3 D_\u03c0_3 -&gt; \u03c0_4, time: 6.003793239593506\n[actor] generating data \u03c0_4 -&gt; p_4 D_\u03c0_4\n--[leaner] get \u03c0_4 -&gt;  p_4 D_\u03c0_4 -&gt; \u03c0_5, time: 8.005346775054932\n[actor] generating data \u03c0_5 -&gt; p_5 D_\u03c0_5\n--[leaner] get \u03c0_5 -&gt;  p_5 D_\u03c0_5 -&gt; \u03c0_6, time: 10.00696587562561\n[actor] generating data \u03c0_6 -&gt; p_6 D_\u03c0_6\n--[leaner] get \u03c0_6 -&gt;  p_6 D_\u03c0_6 -&gt; \u03c0_7, time: 12.00776195526123\n[actor] generating data \u03c0_7 -&gt; p_7 D_\u03c0_7\n--[leaner] get \u03c0_7 -&gt;  p_7 D_\u03c0_7 -&gt; \u03c0_8, time: 14.009297132492065\n</code></pre></p>"},{"location":"algorithms/ppo/#acknowledgements","title":"Acknowledgements","text":"<p>We would like to thank the following resources for PPO theory:</p> <ul> <li>Proximal Policy Optimization Algorithms</li> <li>The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization</li> <li>Asynchronous RLHF</li> </ul> <p>We would like to thank the following resources for distributed Ray usage:</p> <ul> <li>OpenRLHF/OpenRLHF</li> </ul> <p>We would like to thank the following projects for general infrastructure:</p> <ul> <li>vLLM</li> <li>Ray</li> <li>DeepSpeedAI/DeepSpeed</li> <li>HuggingFace/Transformers</li> </ul>"},{"location":"algorithms/rejection_sampling/","title":"Rejection sampling","text":"<p>This is a technique used in the Llama 3 paper. The basic idea is to sample <code>n</code> (typically between 10 and 30) outputs from the latest chat model policy (usually the best performing checkpoint of some kind) and use a reward model to select the best candidate. In the following script, we can vary the <code>--num_completions</code> to generate different number of completions per prompt.</p>"},{"location":"algorithms/rejection_sampling/#debug-run-use-an-interactive-session","title":"Debug run (use an interactive session)","text":"<p>This code supports HF models, local models and also API-based models (e.g., <code>gpt-4</code>). For generating completions, the code now accepts one model at a time, but we're working on adding an ensemble of models. Stay tuned. </p> <pre><code># 1. first sample a bunch of completions given prompts\n# Here is an example created dataset: https://huggingface.co/datasets/vwxyzjn/generation_1727879425\npython open_instruct/rejection_sampling/generation.py \\\n    --dataset_mixer_list allenai/tulu-v2-sft-mixture 100 \\\n    --dataset_splits train \\\n    --model_name_or_path allenai/llama-3-tulu-2-8b \\\n    --num_completions 3 \\\n    --save_filename output/completions.jsonl \\\n    --push_to_hub\n</code></pre>"},{"location":"algorithms/rejection_sampling/#scoring-completions","title":"Scoring completions","text":"<p>You can use either a single RM to score responses or a list of RMs. In the latter case, we will take the majority vote to compute the final score. The RMs can be models explicitly trained as RMs, HF LMs, or API-based models.</p> <p>Note that by default we include the reference completion in the list of completions to perform rejection sampling. This can be disabled by setting <code>--no_include_reference_completion_for_rejection_sampling</code></p> <pre><code># 2.1 tokenize them and run a reward model to filter them\n# Here is an example created dataset: https://huggingface.co/datasets/vwxyzjn/rejection_sampling_1727887719\n# Here is an example created dataset for raw scores: https://huggingface.co/datasets/vwxyzjn/rejection_sampling_scores_1727887719/\npython open_instruct/rejection_sampling/rejection_sampling.py \\\n    --input_filename output/completions.jsonl \\\n    --model_names_or_paths allenai/llama-3-tulu-2-8b-uf-mean-rm \\\n    --save_filename_scores output/completions_scores.jsonl \\\n    --save_filename output/rejection_sampled_completions.jsonl \\\n    --num_completions 3 \\\n    --push_to_hub \\\n    --num_gpus 1 \\\n\n# 2.1.2 without reference completion in rejection sampling\n# Here is an example created dataset: https://huggingface.co/datasets/vwxyzjn/rejection_sampling_1727887719\n# Here is an example created dataset for raw scores: https://huggingface.co/datasets/vwxyzjn/rejection_sampling_scores_1727887719/\npython open_instruct/rejection_sampling/rejection_sampling.py \\\n    --input_filename output/completions.jsonl \\\n    --model_names_or_paths allenai/llama-3-tulu-2-8b-uf-mean-rm \\\n    --save_filename_scores output/completions_scores.jsonl \\\n    --save_filename output/rejection_sampled_completions.jsonl \\\n    --no_include_reference_completion_for_rejection_sampling \\\n    --num_completions 3 \\\n    --push_to_hub \\\n    --num_gpus 1 \\\n\n# 2.2 tokenize them and run llm as a judge\n# Note then when using LLM as a judge, it's possible that llm api failed to produce a score in our expected\n# format, so score extraction failed and we simply mark the score -1.\n# Here is an example created dataset: https://huggingface.co/datasets/vwxyzjn/rejection_sampling_1727889563\n# Here is an example created dataset for raw scores: https://huggingface.co/datasets/vwxyzjn/rejection_sampling_scores_1727889563\npython open_instruct/rejection_sampling/rejection_sampling.py \\\n    --input_filename output/completions.jsonl \\\n    --model_names_or_paths gpt-4o-mini  \\\n    --save_filename_scores output/completions_scores.jsonl \\\n    --save_filename output/rejection_sampled_completions.jsonl \\\n    --num_completions 3 \\\n    --push_to_hub \\\n    --num_gpus 1 \\\n\n# 2.3 tokenize them and run a combination of reward models / llm as a judge\n# Here is an example created dataset: https://huggingface.co/datasets/vwxyzjn/rejection_sampling_1724273702\n# Here is an example created dataset for raw scores: https://huggingface.co/datasets/vwxyzjn/rejection_sampling_scores_1724273702\npython open_instruct/rejection_sampling/rejection_sampling.py \\\n    --input_filename output/completions.jsonl \\\n    --model_names_or_paths allenai/llama-3-tulu-2-8b-uf-mean-rm gpt-4o-mini gpt-4-turbo \\\n    --save_filename_scores output/completions_scores.jsonl \\\n    --save_filename output/rejection_sampled_completions.jsonl \\\n    --num_completions 3 \\\n    --push_to_hub \\\n    --num_gpus 1 \\\n ```\n\n\n\n# Run through the entire dataset run\n\nTo run through the entire dataset you would need a lot more GPUs to finish the generation more quickly. \n\n\n```bash\n# NOTE: the scripts below only generate 400 prompts, so it's for demonstration purposes only. The scripts are highly scalable, and you could modify its `num_prompts=400` to something else like 300000 for the tulu dataset.\n\n# you need to make sure your default beaker workspace has WANDB_API_KEY and HF_TOKEN secrets in them\nbeaker secret write HF_TOKEN xxxxxxxxxxxx\nbeaker secret write WANDB_API_KEY xxxxxxxxxxx\n\n# You can use docker to do the job submission\nbash scripts/rejection_sampling_tulu_docker.bash\n\n# if you are using mason you can debug with the following command(s), the\n# rejection sampling shards should appear in your local foldeer\nbash scripts/rejection_sampling_tulu.bash\n</code></pre> <p>You can see a demo here</p> <p></p>"},{"location":"algorithms/rejection_sampling/#implementation-details","title":"Implementation details","text":"<p>Note that it is possible to generate identical completions per prompt, which is not going to be that useful, so we filter them out via</p> <pre><code>if len(set([item.text for item in output.outputs])) == 1:\n    continue\n</code></pre>"},{"location":"algorithms/rejection_sampling/#debug-commands","title":"Debug commands","text":"<pre><code># debug job submission; you should install your python on NFS and\n# make sure `which python` returns the python environment you are using\npython mason.py \\\n    --cluster ai2/jupiter \\\n    --priority low \\\n    --budget ai2/jupiter \\\n    --gpus 1 -- which python\n# sometimes we run into permission issues; need to run the following\npython mason.py \\\n    --cluster ai2/jupiter \\\n    --priority low \\\n    --budget ai2/jupiter \\\n    --gpus 1 -- chmod -R 777 /net/nfs.cirrascale/allennlp/.cache/\n</code></pre>"},{"location":"algorithms/reward_modeling/","title":"Reward Modeling (RM)","text":"<p>We support training reward models, mostly based on Learning to summarize from human feedback.</p>"},{"location":"algorithms/reward_modeling/#implemented-variants","title":"Implemented Variants","text":"<ul> <li><code>reward_modeling.py</code> contains the script for training reward models.</li> </ul>"},{"location":"algorithms/reward_modeling/#reward_modelingpy","title":"<code>reward_modeling.py</code>","text":"<p>This implementation has the following key features:</p> <ul> <li>Auto save the trained checkpoint to HuggingFace Hub</li> <li>Supports LigerKernel for optimized training with fused operations</li> </ul> <p>There are several relevant implementation details:</p> <ol> <li>The tokenizer pads from the right: when the length of the data points differ, the tokenizer pads from the right</li> <li>Disable dropout in the model: this is actually an implementation detail in PPO training, but for consistency we also disable dropout in the reward model training (see p.3. in https://arxiv.org/pdf/1909.08593)</li> <li>Layer initialization: we initialize the score's weight according to <code>std=1 / np.sqrt(model.config.hidden_size + 1)</code> (see p. 11 in https://arxiv.org/abs/2009.01325)</li> </ol>"},{"location":"algorithms/reward_modeling/#debug-single-gpu","title":"Debug (Single GPU)","text":"<p>You can run the script in a single GPU mode to debug the training process.</p> <pre><code>bash scripts/train/debug/reward_modeling.sh\n</code></pre>"},{"location":"algorithms/reward_modeling/#reproduce-allenaillama-31-tulu-3-8b-rm-8-nodes","title":"Reproduce <code>allenai/Llama-3.1-Tulu-3-8B-RM</code> (8 Nodes)","text":"<p>You can reproduce our <code>allenai/Llama-3.1-Tulu-3-8B-RM</code> model by running the following command:</p> <pre><code>bash scripts/train/tulu3/reward_modeling_8b.sh\n</code></pre> <p> </p>"},{"location":"algorithms/reward_modeling/#training-metrics","title":"Training Metrics","text":"<p>During training, the following metrics are logged:</p> <ul> <li><code>episode</code>: the global episode number training has gone through (e.g., <code>3000</code> means we have trained on 3000 data points already)</li> <li><code>epoch</code>: the fraction or multiple of the epoch (e.g., <code>2.7</code> means we have trained on the dataset for 2 epochs and 70% of the third epoch)</li> <li><code>train/rm/accuracy</code>: the training accuracy of the training batch</li> <li><code>train/rm/loss</code>: the logsigmoid loss of the reward modeling of the training batch</li> <li><code>train/rm/chosen_rewards</code>: the reward of the chosen responses of the training batch</li> <li><code>train/rm/rejected_rewards</code>: the reward of the rejected responses of the training batch</li> <li><code>train/rm/reward_margin</code>: the reward margin (chosen_reward - rejected_reward) of the training batch</li> <li><code>train/rm/lr</code>: the training learning rate</li> </ul> <p>We also have <code>eval/rm/accuracy</code>, <code>eval/rm/loss</code>, <code>eval/rm/chosen_rewards</code>, <code>eval/rm/rejected_rewards</code>, <code>eval/rm/reward_margin</code> for the evalation dataset.</p>"},{"location":"algorithms/reward_modeling/#acknowledgements","title":"Acknowledgements","text":"<p>We would like to thank the following projects for general infrastructure:</p> <ul> <li>DeepSpeedAI/DeepSpeed</li> <li>HuggingFace/Transformers</li> </ul>"},{"location":"algorithms/synthetic_preference_dataset/","title":"Synthetic preference dataset","text":"<p>This section focuses explicitly on creating synthetic preference datasets.</p>"},{"location":"algorithms/synthetic_preference_dataset/#debug-run-use-an-interactive-session","title":"Debug run (use an interactive session)","text":"<p>This code supports HF models, local models and also API-based models (e.g., <code>gpt-4</code>). For generating completions, the code now accepts one model at a time, but we're working on adding an ensemble of models. Stay tuned. </p> <pre><code># 1. first sample a bunch of completions given prompts\n# Here is an example created dataset: https://huggingface.co/datasets/vwxyzjn/generation_1725567768\npython open_instruct/rejection_sampling/generation.py \\\n    --dataset_mixer_list HuggingFaceH4/no_robots 100 \\\n    --dataset_splits train \\\n    --model_name_or_path allenai/llama-3-tulu-2-8b \\\n    --num_completions 3 \\\n    --save_filename output/completions.jsonl \\\n    --sanity_check \\\n    --push_to_hub\n</code></pre>"},{"location":"algorithms/synthetic_preference_dataset/#create-preference-pairs","title":"Create preference pairs","text":"<pre><code># 2.1 do LLM as a judge to create synthetic preference dataset\n# Here is an example created dataset: https://huggingface.co/datasets/vwxyzjn/synthetic_preference_dataset_1725567862\npython open_instruct/rejection_sampling/synthetic_preference_dataset.py \\\n    --input_filename output/completions.jsonl \\\n    --model gpt-4o-2024-08-06 \\\n    --save_filename output/synthetic_preferences.jsonl \\\n    --num_completions 3 \\\n    --push_to_hub \\\n</code></pre> <p>You can visualize the dataset via </p> <pre><code>python -m costa_utils.hf_viz \\\n    --sft vwxyzjn/synthetic_preference_dataset_1725567862 \\\n    --split train \\\n    --sft_messages_column_name whole_conversation\n\npython -m costa_utils.hf_viz \\\n    --preference vwxyzjn/synthetic_preference_dataset_1725567862 \\\n    --split train \\\n    --preference_chosen_column_name chosen \\\n    --preference_rejected_column_name rejected\n</code></pre> <p></p>"},{"location":"algorithms/synthetic_preference_dataset/#run-through-the-entire-dataset-run","title":"Run through the entire dataset run","text":"<p>To run through the entire dataset you would need a lot more GPUs to finish the generation more quickly. </p> <pre><code># NOTE: the scripts below only generate 400 prompts, so it's for demonstration purposes only. The scripts are highly scalable, and you could modify its `num_prompts=400` to something else like 300000 for the tulu dataset.\n\n# you need to make sure your default beaker workspace has WANDB_API_KEY and HF_TOKEN secrets in them\nbeaker secret write HF_TOKEN xxxxxxxxxxxx\nbeaker secret write WANDB_API_KEY xxxxxxxxxxx\n\n# Docker mode: using caches from WEKA\ndeploy_mode=\"docker_weka\" bash scripts/synthetic_preference_dataset.bash\n\n# Docker mode: using caches from NFS\ndeploy_mode=\"docker_nfs\" bash scripts/synthetic_preference_dataset.bash\n\n# Docker mode: do not use caches\ndeploy_mode=\"docker\" bash scripts/synthetic_preference_dataset.bash\n\n# If you have environment setup with NFS and want to launch debug mode:\ndeploy_mode=\"nfs\" bash scripts/synthetic_preference_dataset.bash\n</code></pre> <p>You can see a demo here</p> <p></p>"},{"location":"algorithms/trained_model_location/","title":"Trained Model Location","text":"<p>When running our training scripts, the model will get uploaded to several places when applicable for redundancy, depending on the cluster environment:</p> <ul> <li>Hugging Face</li> <li>Google Cloud Storage</li> <li>Ai2's internal beaker dataset</li> <li>Local storage</li> </ul>"},{"location":"algorithms/trained_model_location/#hugging-face","title":"Hugging Face","text":"<p>Let's use https://wandb.ai/ai2-llm/open_instruct_public/runs/tyfe1095 as an example. If you go to its wandb's Overview page -&gt; config -&gt; search for <code>hf</code>, then you can find this <code>hf_repo_url</code>. </p> <p> </p> <p>To download, notice the <code>run_name</code> for this run is <code>tulu3_8b_dpo__1__1742613782</code>. So you can use the following command to download the model:</p> <p><pre><code>exp_name=tulu3_8b_dpo__1__1742613782\n# first download the model\nhuggingface-cli download --revision $exp_name allenai/open_instruct_dev\n# get the cache directory\nexp_cache_dir=$(huggingface-cli download --revision $exp_name allenai/open_instruct_dev)\nls $exp_cache_dir\n</code></pre> <pre><code>Downloading 'config.json' to '/weka/oe-adapt-default/allennlp/.cache/hub/models--allenai--open_instruct_dev/blobs/c0ed34722856586c3fa9ccb27bd52fb8e1d759a1.incomplete'\nconfig.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 984/984 [00:00&lt;00:00, 5.84MB/s]\nDownload complete. Moving file to /weka/oe-adapt-default/allennlp/.cache/hub/models--allenai--open_instruct_dev/blobs/c0ed34722856586c3fa9ccb27bd52fb8e1d759a1\nDownloading 'pytorch_model-00001-of-00004.bin' to '/weka/oe-adapt-default/allennlp/.cache/hub/models--allenai--open_instruct_dev/blobs/9da6b1637575b207617b84e84a5a974e8ee2a9fab55bd7e0343d6edf2a9f9f28.incomplete'\npytorch_model-00001-of-00004.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 4.98G/4.98G [00:07&lt;00:00, 662MB/s]\nDownload complete. Moving file to /weka/oe-adapt-default/allennlp/.cache/hub/models--allenai--open_instruct_dev/blobs/9da6b1637575b207617b84e84a5a974e8ee2a9fab55bd7e0343d6edf2a9f9f28\nDownloading 'pytorch_model-00002-of-00004.bin' to '/weka/oe-adapt-default/allennlp/.cache/hub/models--allenai--open_instruct_dev/blobs/667937dac38f3df4ffe7f5be637b54bed58c40b78c39550b639d12f6d57461b7.incomplete'\npytorch_model-00002-of-00004.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 5.00G/5.00G [00:07&lt;00:00, 657MB/s]\nDownload complete. Moving file to /weka/oe-adapt-default/allennlp/.cache/hub/models--allenai--open_instruct_dev/blobs/667937dac38f3df4ffe7f5be637b54bed58c40b78c39550b639d12f6d57461b7\nDownloading 'pytorch_model-00003-of-00004.bin' to '/weka/oe-adapt-default/allennlp/.cache/hub/models--allenai--open_instruct_dev/blobs/7d0471f489239e21a2063568974d4b118f294b5d1a381f306fe165729b6e88d3.incomplete'\npytorch_model-00003-of-00004.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 4.92G/4.92G [00:07&lt;00:00, 678MB/s]\nDownload complete. Moving file to /weka/oe-adapt-default/allennlp/.cache/hub/models--allenai--open_instruct_dev/blobs/7d0471f489239e21a2063568974d4b118f294b5d1a381f306fe165729b6e88d3\nDownloading 'pytorch_model-00004-of-00004.bin' to '/weka/oe-adapt-default/allennlp/.cache/hub/models--allenai--open_instruct_dev/blobs/ff53f644b12798a5e81c6c8072169a29b6a318a251d7d939687e2af333efe51e.incomplete'\npytorch_model-00004-of-00004.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 1.17G/1.17G [00:03&lt;00:00, 337MB/s]\nDownload complete. Moving file to /weka/oe-adapt-default/allennlp/.cache/hub/models--allenai--open_instruct_dev/blobs/ff53f644b12798a5e81c6c8072169a29b6a318a251d7d939687e2af333efe51e\n/weka/oe-adapt-default/allennlp/.cache/hub/models--allenai--open_instruct_dev/snapshots/40227c36fb1b5b714a71f9d635ead5a79c23507f\nroot@phobos-cs-aus-452:/weka/oe-adapt-default/costah/oi2/docs/algorithms/trained_model_location# # get the cache directory\nroot@phobos-cs-aus-452:/weka/oe-adapt-default/costah/oi2/docs/algorithms/trained_model_location# exp_cache_dir=$(huggingface-cli download --revision $exp_name allenai/open_instruct_dev)\nroot@phobos-cs-aus-452:/weka/oe-adapt-default/costah/oi2/docs/algorithms/trained_model_location# ls $exp_cache_dir\nconfig.json                       pytorch_model-00003-of-00004.bin  special_tokens_map.json\ngeneration_config.json            pytorch_model-00004-of-00004.bin  tokenizer_config.json\npytorch_model-00001-of-00004.bin  pytorch_model.bin.index.json      tokenizer.json\npytorch_model-00002-of-00004.bin  README.md\n</code></pre></p>"},{"location":"algorithms/trained_model_location/#google-cloud-storage","title":"Google Cloud Storage","text":"<p>Let's use https://wandb.ai/ai2-llm/open_instruct_public/runs/tyfe1095 as an example. Because this run was conducted on the <code>ai2/augusta</code>, <code>mason.py</code> automatically appends <code>--gs_bucket_path gs://ai2-llm/post-training/</code> to the training args (for external users you can try doing the same append). Then, the model was automatically uploaded to </p> <pre><code>gs://ai2-llm/post-training//costah/output/tulu3_8b_dpo__1__1742613782\n</code></pre> <p></p> <p>To download, you can run the following command:</p> <p><pre><code>gsutil -o \"GSUtil:parallel_composite_upload_threshold=150M\" \\\n    -o \"GSUtil:parallel_thread_count=1\" \\\n    -m \\\n    cp -r gs://ai2-llm/post-training//costah/output/tulu3_8b_dpo__1__1742613782 .\nls tulu3_8b_dpo__1__1742613782\n</code></pre> <pre><code>root@phobos-cs-aus-452:/weka/oe-adapt-default/costah/oi2/tulu3_8b_dpo__1__1742613782# gsutil -o \"GSUtil:parallel_composite_upload_threshold=150M\" \\\n&gt;     -o \"GSUtil:parallel_thread_count=1\" \\\n&gt;     -m \\\n&gt;     cp -r gs://ai2-llm/post-training//costah/output/tulu3_8b_dpo__1__1742613782 .\nCopying gs://ai2-llm/post-training//costah/output/tulu3_8b_dpo__1__1742613782/config.json...\nCopying gs://ai2-llm/post-training//costah/output/tulu3_8b_dpo__1__1742613782/generation_config.json...\nCopying gs://ai2-llm/post-training//costah/output/tulu3_8b_dpo__1__1742613782/pytorch_model-00001-of-00004.bin...\nCopying gs://ai2-llm/post-training//costah/output/tulu3_8b_dpo__1__1742613782/pytorch_model-00002-of-00004.bin...\nCopying gs://ai2-llm/post-training//costah/output/tulu3_8b_dpo__1__1742613782/pytorch_model-00003-of-00004.bin...\nCopying gs://ai2-llm/post-training//costah/output/tulu3_8b_dpo__1__1742613782/pytorch_model-00004-of-00004.bin...\nCopying gs://ai2-llm/post-training//costah/output/tulu3_8b_dpo__1__1742613782/pytorch_model.bin.index.json...\nCopying gs://ai2-llm/post-training//costah/output/tulu3_8b_dpo__1__1742613782/special_tokens_map.json...\nCopying gs://ai2-llm/post-training//costah/output/tulu3_8b_dpo__1__1742613782/tokenizer.json...\nCopying gs://ai2-llm/post-training//costah/output/tulu3_8b_dpo__1__1742613782/tokenizer_config.json...\nResuming download for ./tulu3_8b_dpo__1__1742613782/pytorch_model-00003-of-00004.bin component 0\nResuming download for ./tulu3_8b_dpo__1__1742613782/pytorch_model-00003-of-00004.bin component 1\nResuming download for ./tulu3_8b_dpo__1__1742613782/pytorch_model-00003-of-00004.bin component 2\nResuming download for ./tulu3_8b_dpo__1__1742613782/pytorch_model-00003-of-00004.bin component 3\n| [10/10 files][ 15.0 GiB/ 15.0 GiB] 100% Done  52.1 MiB/s ETA 00:00:00         \nOperation completed over 10 objects/15.0 GiB.                                    \nroot@phobos-cs-aus-452:/weka/oe-adapt-default/costah/oi2/tulu3_8b_dpo__1__1742613782# ls\ntulu3_8b_dpo__1__1742613782\nroot@phobos-cs-aus-452:/weka/oe-adapt-default/costah/oi2/tulu3_8b_dpo__1__1742613782# ls tulu3_8b_dpo__1__1742613782\nconfig.json                       pytorch_model-00003-of-00004.bin  tokenizer_config.json\ngeneration_config.json            pytorch_model-00004-of-00004.bin  tokenizer.json\npytorch_model-00001-of-00004.bin  pytorch_model.bin.index.json\npytorch_model-00002-of-00004.bin  special_tokens_map.json\n</code></pre></p>"},{"location":"algorithms/trained_model_location/#local-storage-nfs","title":"Local storage / NFS","text":"<p>The local storage is quite ephemeral. Sometimes we try to assign an <code>output_dir</code> to a particular directory. For example, when launching with <code>mason.py</code>, we automatically overwrite the <code>output_dir</code> to be <code>/weka/oe-adapt-default/allennlp/deletable_checkpoint/$beaker_user/</code>. You can find the model in the following directory:</p> <p></p> <pre><code>ls \"/weka/oe-adapt-default/allennlp/deletable_checkpoint/valpy/tulu3_8b_sft_no_IF__8__1745534652\"\nconfig.json                       pytorch_model-00003-of-00004.bin  tokenizer_config.json\ngeneration_config.json            pytorch_model-00004-of-00004.bin  tokenizer.json\npytorch_model-00001-of-00004.bin  pytorch_model.bin.index.json\npytorch_model-00002-of-00004.bin  special_tokens_map.json\n</code></pre>"},{"location":"algorithms/trained_model_location/#beaker-dataset-ai2s-internal-storage","title":"Beaker Dataset (Ai2's internal storage)","text":"<p>When possible, we try to upload the model to beaker dataset. You can find the model in corresponding beaker experiment by looking up <code>beaker_experiment_url</code> in the tracked wandb run.</p> <p></p> <p>You can download the model by running the following command:</p> <p><pre><code>exp_name=tulu3_8b_dpo__1__1742613782\ndataset_id=01JPXXXKZPACGK5AZ1XSD5V54F\nmkdir $exp_name\nbeaker dataset fetch \"$dataset_id\" -o $exp_name --concurrency 64\n</code></pre> <pre><code>Downloading dataset 01JPXXXKZPACGK5AZ1XSD5V54F to tulu3_8b_dpo__1__1742613782\nFiles: 6          4 in progress \u2838  \nBytes: 16.49 MiB  14.96 GiB in progress \u2838 \n</code></pre></p>"},{"location":"data/preference-data/","title":"Current preference datasets","text":"<p>To build all the datasets at once (use this carefully), run: <pre><code>sh scripts/data/preferences/prepare_all.sh\n</code></pre></p>"},{"location":"data/preference-data/#chat","title":"Chat","text":""},{"location":"data/preference-data/#maintained-here","title":"Maintained here","text":"<p>First, older popular datasets. Build these datasets (a subset only) with: <pre><code>python scripts/data/preferences/webgpt.py --push_to_hub --hf_entity=ai2-adapt-dev\npython scripts/data/preferences/hh-harmless.py --push_to_hub --hf_entity=ai2-adapt-dev\npython scripts/data/preferences/hh-helpful.py --push_to_hub --hf_entity=ai2-adapt-dev\n</code></pre> * ai2-adapt-dev/webgpt-binarized * ai2-adapt-dev/hh-rlhf-harmless * ai2-adapt-dev/hh-rlhf-helpful</p> <p>Next, Nvidia's recent HelpSteer 2.  They are created with: <pre><code>python scripts/data/preferences/helpsteer2.py --push_to_hub --min_score 2.5 --hf_entity=ai2-adapt-dev\npython scripts/data/preferences/helpsteer2.py --push_to_hub --min_score 2 --hf_entity=ai2-adapt-dev\npython scripts/data/preferences/helpsteer2.py --push_to_hub --min_score 2 --hf_entity=ai2-adapt-dev --aspects_to_ignore verbosity\npython scripts/data/preferences/helpsteer2.py --push_to_hub --hf_entity=ai2-adapt-dev --aspects_to_ignore verbosity\n</code></pre> The binarization weighting that Nvidia recommends can be used with: <pre><code>python scripts/data/preferences/helpsteer2_nvidia.py --push_to_hub --hf_entity ai2-adapt-dev\n</code></pre> Some examples include: * ai2-adapt-dev/helpsteer-2-binarized-above-2.0-margin-0.5-ignore-verbosity * ai2-adapt-dev/helpsteer-2-binarized-ignore-verbosity: This ignores verbosity aspect, which is unclear in the paper. * ai2-adapt-dev/helpsteer2-binarized-nvidia-spec: This uses the specific weighting that Nvidia converged on in their HelpSteer2 paper training multiple types of reward models. </p> <p>Also, specific splits of Nectar (randomly binarized from top 3 completions and a bottom completion) are included with: <pre><code>python scripts/data/preferences/nectar.py --push_to_hub --hf_entity ai2-adapt-dev\npython scripts/data/preferences/nectar.py --push_to_hub --subset anthropic-hh --hf_entity ai2-adapt-dev\npython scripts/data/preferences/nectar.py --push_to_hub --deduplication --hf_entity ai2-adapt-dev\n</code></pre> The default split is <code>lmsys-chat-1m</code>. The last example is called \"deduplication\" due to potential overlap with UltraFeedback, given they source from the same underlying dataset. Basic tests showed they did not use the same prompts, but slight modifications could've occured. * ai2-adapt-dev/nectar_binarized-anthropic-hh * ai2-adapt-dev/nectar_binarized-lmsys-chat-1m * ai2-adapt-dev/nectar_binarized-dedup-ultrafeedbackack</p>"},{"location":"data/preference-data/#stored-on-hf","title":"Stored on HF","text":"<ul> <li>allenai/ultrafeedback_binarized_cleaned_train</li> <li>ai2-adapt-dev/summarize_from_feedback</li> <li>ai2-adapt-dev/DaringAnteater-prefs</li> <li>ai2-adapt-dev/DaringAnteater-prefs-RM-filter</li> <li>ai2-adapt-dev/WildChat-prefs-280824</li> <li>ai2-adapt-dev/helpsteer2-binarized-mean-aspects: Similar to our other HelpSteer splits, less processing.</li> <li>ai2-adapt-dev/Skywork-Magpie: Subset of the Skywork Preference Dataset for only the Magpie splits.</li> </ul>"},{"location":"data/preference-data/#ultrafeedback-replication","title":"UltraFeedback Replication","text":"<p>The current replications have fewer prompts than the original. These are built by splitting the original and recreating completions. We are working on merging them.</p> <p>Build these datasets with: <pre><code>python scripts/data/preferences/ultrafeedback.py --push_to_hub --hf_entity=ai2-adapt-dev\n</code></pre> The master version of the UltraFeedback pipeline replication can be found here: ai2-adapt-dev/ultrafeedback-pipeline-replication</p> <p>UltraFeedback variants explore different combinations of prompt sources, model diversity, sampling methods, and prompt templates:</p> <ul> <li>Setup 0: Replication of original UltraFeedback</li> <li>Setup 1-2: Custom prompts with UltraFeedback methodology</li> <li>Setup 3-4: Custom prompts with varied model diversity and principle sampling</li> <li>Setup 5: Custom prompts with UltraFeedback template</li> <li> <p>Setup 6: Increased model diversity</p> </li> <li> <p>ai2-adapt-dev/ultrafeedback-replication-p0</p> </li> <li>ai2-adapt-dev/ultrafeedback-replication-p1</li> <li>ai2-adapt-dev/ultrafeedback-replication-p2</li> <li>ai2-adapt-dev/ultrafeedback-replication-p3</li> <li>ai2-adapt-dev/ultrafeedback-replication-p4</li> <li>ai2-adapt-dev/ultrafeedback-replication-p5</li> <li>ai2-adapt-dev/ultrafeedback-replication-p6</li> </ul>"},{"location":"data/preference-data/#ultrainteract-variants","title":"UltraInteract Variants","text":"<p>Build these datasets with: <pre><code>python scripts/data/preferences/ultrainteract.py --push_to_hub --hf_entity=ai2-adapt-dev\n</code></pre> Split by category and by selecting the longest conversations per prompt or a random length per prompt. From UltraInteract_pair.</p> <ul> <li>ai2-adapt-dev/UltraInteract_pair_maxlen_Coding</li> <li>ai2-adapt-dev/UltraInteract_pair_randomlen_Coding</li> <li>ai2-adapt-dev/UltraInteract_pair_maxlen_Math_CoT</li> <li>ai2-adapt-dev/UltraInteract_pair_randomlen_Math_CoT</li> <li>ai2-adapt-dev/UltraInteract_pair_maxlen_Math_PoT</li> <li>ai2-adapt-dev/UltraInteract_pair_randomlen_Math_PoT</li> <li>ai2-adapt-dev/UltraInteract_pair_maxlen_Logic</li> <li>ai2-adapt-dev/UltraInteract_pair_randomlen_Logic</li> </ul>"},{"location":"data/preference-data/#tulu-25-data","title":"Tulu 2.5 Data","text":"<p>Build these datasets with: <pre><code>python scripts/data/preferences/split_tulu2.5_prefs.py --push_to_hub --hf_entity=ai2-adapt-dev\n</code></pre> Split from this dataset for easier mixing: * ai2-adapt-dev/tulu-2.5-prefs-alpaca_farm_gpt4_pref * ai2-adapt-dev/tulu-2.5-prefs-alpaca_farm_human_pref * ai2-adapt-dev/tulu-2.5-prefs-argilla_dpo_mix * ai2-adapt-dev/tulu-2.5-prefs-capybara * ai2-adapt-dev/tulu-2.5-prefs-chatbot_arena_2023 * ai2-adapt-dev/tulu-2.5-prefs-chatbot_arena_2024 * ai2-adapt-dev/tulu-2.5-prefs-helpsteer * ai2-adapt-dev/tulu-2.5-prefs-hh_rlhf * ai2-adapt-dev/tulu-2.5-prefs-hh_rlhf_60k * ai2-adapt-dev/tulu-2.5-prefs-nectar * ai2-adapt-dev/tulu-2.5-prefs-nectar_60k * ai2-adapt-dev/tulu-2.5-prefs-orca_dpo_pairs * ai2-adapt-dev/tulu-2.5-prefs-preference_big_mixture * ai2-adapt-dev/tulu-2.5-prefs-prm800k_pairs_phase2 * ai2-adapt-dev/tulu-2.5-prefs-shp_2 * ai2-adapt-dev/tulu-2.5-prefs-stack_exchange_60k * ai2-adapt-dev/tulu-2.5-prefs-stack_exchange_paired * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_evol_instruct * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_false_qa * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_flan_v2 * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_lower_10k * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_mean_aspects * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_middle_10k * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_overall * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_sharegpt * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_top_10k * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_truthful_qa * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_ultrachat</p>"},{"location":"get_started/ai2_internal_setup/","title":"Ai2 Internal Setup","text":"<p>This document details some best practices when working with our cluster.</p>"},{"location":"get_started/ai2_internal_setup/#one-time-setup-vscode-weka-setup","title":"(One-time setup) VScode + Weka setup","text":"<p>You should join the <code>#vscode-weka-dev-workflow</code> slack channel to setup your VScode to work with weka.</p> <p>After following the instructions there, you should end up with a VScode / Cursor setup that looks like this:</p> <ul> <li>Your terminal has direct access to the weka filesystem.</li> <li>You can run <code>beaker</code> commands from the terminal.</li> <li>You can edit files in the weka filesystem.</li> <li>You can run python scripts with the pyenv / uv environment.</li> </ul> <p></p>"},{"location":"get_started/ai2_internal_setup/#one-time-setup-setup-api-keys","title":"(One-time setup) Setup API keys","text":"<p>You need to first obtain API key or tokens from the following website:</p> <ul> <li><code>BEAKER_TOKEN</code>: https://beaker.org/user</li> <li><code>WANDB_API_KEY</code>: https://wandb.ai/authorize</li> <li><code>HF_TOKEN</code>: https://huggingface.co/settings/tokens</li> </ul> <p>Then you need to write them in beaker secret as follows (replace the <code>xxxx</code> with your own API key or token) <pre><code>beaker_whoami=$(beaker account whoami --format json | jq -r '.[0].name')\nbeaker secret write -w ai2/tulu-2-improvements \"${beaker_whoami}_BEAKER_TOKEN\" xxxx\nbeaker secret write -w ai2/tulu-2-improvements \"${beaker_whoami}_WANDB_API_KEY\" xxxx\nbeaker secret write -w ai2/tulu-2-improvements \"${beaker_whoami}_HF_TOKEN\" xxxx\nbeaker secret write -w ai2/tulu-3-dev \"${beaker_whoami}_BEAKER_TOKEN\" xxxx\nbeaker secret write -w ai2/tulu-3-dev \"${beaker_whoami}_WANDB_API_KEY\" xxxx\nbeaker secret write -w ai2/tulu-3-dev \"${beaker_whoami}_HF_TOKEN\" xxxx\n</code></pre></p>"},{"location":"get_started/ai2_internal_setup/#masonpy-for-job-submission","title":"mason.py (for job submission)","text":"<p><code>mason.py</code> is our job submission script. It basically takes your command and runs it in the specified clusters.</p> <p>For example, let's say you have a training job like this:</p> <pre><code>python open_instruct/finetune.py \\\n    --model_name_or_path EleutherAI/pythia-14m \\\n    --tokenizer_name EleutherAI/pythia-14m \\\n    --dataset_mixer_list allenai/tulu-3-sft-personas-algebra 100 \\\n    --use_flash_attn False \\\n    --with_tracking --report_to wandb\n</code></pre> <p>You can take your command above and run it on the weka cluster with the following command (use <code>--</code> to separate the mason command from the python command):</p> <pre><code>python mason.py \\\n    --cluster ai2/jupiter ai2/saturn ai2/neptune \\\n    --workspace ai2/tulu-3-dev \\\n    --image nathanl/open_instruct_auto --pure_docker_mode \\\n    --priority normal \\\n    --budget ai2/oe-adapt \\\n    --gpus 0 -- python open_instruct/finetune.py \\\n    --model_name_or_path EleutherAI/pythia-14m \\\n    --tokenizer_name EleutherAI/pythia-14m \\\n    --dataset_mixer_list allenai/tulu-3-sft-personas-algebra 100 \\\n    --use_flash_attn False \\\n    --with_tracking --report_to wandb\n</code></pre> <p></p> <p></p> <p><code>mason.py</code> does a few things:</p> <p>Auto set HF cache environment variables:</p> <p>During the job submission, it automatically tries to setup a shared Hugging Face cache with environment variables. For example, it sets</p> <ul> <li><code>HF_HOME=/weka/oe-adapt-default/allennlp/.cache/huggingface</code>. </li> <li><code>HF_DATASETS_CACHE=/weka/oe-adapt-default/allennlp/.cache/huggingface</code></li> <li><code>HF_HUB_CACHE=/weka/oe-adapt-default/allennlp/.cache/hub</code></li> </ul> <p>Auto set <code>--hf_entity</code> and <code>--wandb_entity</code> arguments:</p> <p>so during runtime we issue fewer HF API calls, which sometimes could fail due to rate limiting.</p> <p>Auto caching datasets:</p> <p>mason.py will auto call <code>--cache_dataset_only</code> for you, so you do the tokenization locally instead of in the jobs, which saves idle GPU time in the actual jobs.</p> <p>Auto upload to Google Cloud Storage:</p> <p>When submitting to the <code>ai2/augusta</code> cluster, mason will try to read your model and upload it to Google Cloud Storage and download it to the job (since the cluster does not have a reliable shared filesystem).</p>"},{"location":"get_started/ai2_internal_setup/#update_command_argspy-for-sweep-benchmark-etc","title":"update_command_args.py (for sweep, benchmark, etc.)","text":"<p>The /scripts/train directory contains many examples on how to launch jobs with mason.py. Sometimes the commands can get long and hard to manage, so we wrote a script called update_command_args.py that can be used to add or update arguments in a shell script. For example,</p> <pre><code>python update_command_args.py scripts/train/tulu3/grpo_fast_8b.sh \\\n    --cluster ai2/augusta \\\n    --priority normal \\\n    --image costah/open_instruct_dev0320_11  --non_stop_penalty False | uv run bash\n</code></pre> <p>This will update the <code>--cluster</code>, <code>--priority</code>, <code>--image</code>, and <code>--non_stop_penalty</code> arguments in the script with the ones specified, making it easier to launch jobs with different configurations.</p> <p>As another example, you can run something like this for a learning rate search:</p> <pre><code>for lr in 1e-6 1e-5 1e-4; do\n    python update_command_args.py scripts/train/tulu3/grpo_fast_8b.sh \\\n        --exp_name grpo_fast_8b_lr_${lr} \\\n        --learning_rate $lr \\\n        --image costah/open_instruct_dev0320_11 --non_stop_penalty False | uv run bash\ndone\n</code></pre> <p>We also have a script called scripts/train/benchmark.sh that keeps track of all the commands used to launch jobs in our public wandb project <code>ai2-llm/open_instruct_public</code>.</p>"},{"location":"get_started/ai2_internal_setup/#ai2-internal-evaluation","title":"Ai2 Internal Evaluation","text":"<p>We provide a script integrated with beaker for use internally at Ai2. There are couple of use cases.</p> <p>1. Run evals against a public Hugging Face model. Basically you need to prefix the model name with <code>hf-</code> and provide the location as the HF path (e.g. <code>meta-llama/Meta-Llama-3-8B-Instruct</code>).</p> <pre><code>for model in allenai/OLMoE-1B-7B-0125-Instruct allenai/OLMoE-1B-7B-0125-DPO allenai/OLMoE-1B-7B-0125-SFT allenai/OLMoE-1B-7B-0924-SFT allenai/OLMoE-1B-7B-0924-Instruct; do\npython scripts/submit_eval_jobs.py \\\n    --model_name hf-$model \\\n    --cluster ai2/jupiter ai2/neptune ai2/saturn ai2/ceres  \\\n    --priority high \\\n    --location $model \\\n    --is_tuned \\\n    --workspace \"tulu-3-results\" \\\n    --priority high \\\n    --preemptible \\\n    --use_hf_tokenizer_template \\\n    --run_oe_eval_experiments \\\n    --evaluate_on_weka \\\n    --skip_oi_evals\ndone\n</code></pre> <p>2. Run evals against a model hosted on Beaker dataset. If it's a training run, you should try matching the <code>exp_name</code> and <code>run_id</code> with the training run.</p> <pre><code>model_name=0222_32B_dpo_lr_8.5e-7__allenai_open_instruct_dev__42__1741225304\nurl=https://wandb.ai/ai2-llm/open_instruct_internal/runs/7afq8x28\nlocation=01JNMHSM8DDSFB3GJDBM5MP6J8\npython scripts/submit_eval_jobs.py \\\n    --model_name $model_name \\\n    --cluster ai2/jupiter ai2/neptune ai2/saturn ai2/ceres  \\\n    --priority high \\\n    --location $location \\\n    --is_tuned \\\n    --workspace \"tulu-3-results\" \\\n    --preemptible \\\n    --use_hf_tokenizer_template \\\n    --run_oe_eval_experiments \\\n    --skip_oi_evals \\\n    --run_id $url\n</code></pre> <p>This will later show up in the internal leaderboard.</p> <p></p> <p>3. Run evals against a model hosted on weka.</p> <pre><code>python scripts/submit_eval_jobs.py \\\n    --model_name test_no_hf_upload \\\n    --location /weka/oe-adapt-default/costah/models/0129_grpo_math_kl_fix_zs_0.0_16_half-m_461_checkpoints/step_640 \\\n    --cluster ai2/saturn ai2/neptune \\\n    --is_tuned \\\n    --workspace \"tulu-3-results\" \\\n    --priority high \\\n    --preemptible \\\n    --use_hf_tokenizer_template \\\n    --beaker_image \"nathanl/open_instruct_auto\" \\\n    --run_oe_eval_experiments \\\n    --evaluate_on_weka \\\n    --oe_eval_tasks gsm8k::tulu,minerva_math::tulu \\\n    --run_id https://wandb.ai/ai2-llm/open_instruct_internal/runs/swf79vby \\\n    --skip_oi_evals \\\n    --oe_eval_max_length 8096\n</code></pre> <p>4. Run evals against a model on Google Cloud Storage.</p> <pre><code>python scripts/submit_eval_jobs.py \\\n    --model_name test_gs_location \\\n    --location gs://ai2-llm/post-training/allenai/Llama-3.1-Tulu-3.1-8B \\\n    --cluster ai2/augusta \\\n    --is_tuned \\\n    --workspace tulu-3-results \\\n    --preemptible \\\n    --use_hf_tokenizer_template \\\n    --beaker_image nathanl/open_instruct_auto \\\n    --oe_eval_tasks gsm8k::tulu \\\n    --skip_oi_evals \\\n    --gpu_multiplier 2 \\\n    --run_oe_eval_experiments\n</code></pre>"},{"location":"get_started/ai2_internal_setup/#running-with-gantry","title":"Running with gantry","text":"<p>You can also run with gantry, if you want to test changes. Important: Before you run any command with gantry, make sure you commit and push, since gantry will attempt to clone the repo with your local latest commit hash.</p> <p>See the \"One-Time Setup\" section below before running commands. To test your setup, run the following command -- if this job succeeds, then you're ready to run evaluations with gantry.</p> <pre><code>gantry run --workspace {workspace} --budget ai2/oe-adapt --beaker-image kavelr/oe-safety --venv base --cluster ai2/jupiter --env-secret OPENAI_API_KEY=openai_api_key --env-secret HF_TOKEN=hf_token -- python -c 'print(\"Hello world\")'\n</code></pre> <p>You can freely add any additional arguments to give to Beaker, such as a <code>--priority</code> tag which can be set to preemptible, normal, high, or urgent. AI2 policies may restrict the priorities that are available to users on certain clusters.</p> <p>In the examples below, text within {} tags should be replaced with your own values. </p> <p>As a convenience, you can use the <code>evaluation/gantry_run.sh</code> script which includes some necessary arguments. You can use it the same way as <code>gantry run</code>, but excluding these boilerplate arguments (take a look at the script to see what it includes). Example usage:</p> <pre><code>PYTHONPATH=safety-eval ./evaluation/gantry_run.sh --workspace {workspace} --cluster {cluster} --gpus {n_gpus} \\\n    --priority {priority} -- python evaluation/run_all_generation_benchmarks.py \\\n    --model_name_or_path allenai/tulu-2-dpo-7b \\\n    --model_input_template_path_or_name tulu2 \\\n    --report_output_path /results/metrics.json\n</code></pre>"},{"location":"get_started/ai2_internal_setup/#extra-beaker-commands","title":"Extra Beaker Commands","text":"<p>Here is an example using the full <code>gantry run</code> command. Use the beaker image <code>seungjuh/oe-safety-support-olmo17</code></p> <p>Important: Please include all the beaker arguments exactly as in the examples unless intentionally modifying some configuration. Many of them are necessary to avoid job failures, such as <code>--beaker-image</code>, <code>--venv</code>, and <code>--env-secret</code>. Note that <code>openai_api_key</code> and <code>hf_token</code> are Beaker workspace secret names, so should not be replaced with actual values (see One-Time Setup).</p> <p>Note that the <code>--</code> divides the gantry command from the evaluation command - you can edit the second part to run whatever eval suite you want from the <code>eval.py</code> script. Any additional Beaker arguments such as a dataset mount to use a model from a Beaker dataset or adding a priority tag can be added before the <code>--</code>.</p> <p>You can also run all generator evaluations parallelized across the GPUs allocated to your batch job, like so: <pre><code>gantry run --workspace {your_workspace} --cluster {cluster} --gpus {n_gpus} \\\n    --name {beaker_experiment_name} --task-name {beaker_task_name} --beaker-image seungjuh/oe-safety-support-olmo17 --venv base \\\n    --env-secret OPENAI_API_KEY=openai_api_key \\\n    --env-secret HF_TOKEN=hf_token \\\n    --budget {budget} -- python evaluation/run_all_generation_benchmarks.py \\\n    --model_name_or_path allenai/tulu-2-dpo-7b \\\n    --model_input_template_path_or_name tulu2 \\\n    --report_output_path /results/metrics.json --save_individual_results_path /results/all.json\n</code></pre></p> <p>Because the <code>--report_output_path</code> argument is set to <code>/results/metrics.json</code>, the output will automatically get logged to Beaker metrics in the experiment page (example).</p>"},{"location":"get_started/ai2_internal_setup/#common-gotchas","title":"Common Gotchas","text":"<p>If you're experiencing job failures, here are some things to check:</p> <ul> <li>Make sure your local changes are committed,  pushed, and up to date with the remote</li> <li>Make sure you have <code>--beaker-image seungjuh/oe-safety-support-olmo17</code> and <code>--venv base</code> in your <code>gantry run</code> command</li> <li>Check your GitHub personal access token is authorized to access the allenai organization</li> <li>Make sure the openai_api_key and hf_token secrets exist in your Beaker workspace</li> </ul>"},{"location":"get_started/installation/","title":"Installation","text":"<p>Our setup mostly follows our Dockerfile, which uses Python 3.10. Note that Open Instruct is a research codebase and does not guarantee backward compatibility. We offer two installation strategies:</p> <ul> <li> <p>Local installation: This is the recommended way to install Open Instruct. You can install the dependencies by running the following commands: <pre><code>pip install --upgrade pip \"setuptools&lt;70.0.0\" wheel \n# TODO, unpin setuptools when this issue in flash attention is resolved\npip install torch==2.5.1 torchvision==0.20.1 --index-url https://download.pytorch.org/whl/cu121\npip install packaging\npip install flash-attn==2.7.2.post1 --no-build-isolation\npip install -r requirements.txt\npip install -e .\npython -m nltk.downloader punkt\n</code></pre></p> </li> <li> <p>Local installation with uv (preview): We are experimenting with using uv. You can install via <pre><code>uv sync\nuv sync --extra compile # to install flash attention\n</code></pre></p> </li> <li> <p>Docker installation: You can also use the Dockerfile to build a Docker image. You can build the image with the following command:</p> </li> </ul> <pre><code>docker build . -t open_instruct_dev\n# if you are interally at AI2, you can create an image like this:\nbeaker_user=$(beaker account whoami --format json | jq -r '.[0].name')\nbeaker image delete $beaker_user/open_instruct_dev \nbeaker image create open_instruct_dev -n open_instruct_dev -w ai2/$beaker_user\n</code></pre> <p>Optionally you can build the base image with the following command:</p> <pre><code>docker build --build-arg CUDA=12.1.0 --build-arg TARGET=cudnn8-devel --build-arg DIST=ubuntu20.04 -f  Dockerfile.base . -t cuda-no-conda:12.1-cudnn8-dev-ubuntu20.04\n</code></pre> <ul> <li>Docker with uv: You can also use the Dockerfile to build a Docker image with uv. You can build the image with the following command:</li> </ul> <pre><code>docker build -f Dockerfile.uv --build-arg UV_CACHE_DIR=$UV_CACHE_DIR -t open_instruct_dev_uv .\n# if you are interally at AI2, you can create an image like this:\nbeaker_user=$(beaker account whoami --format json | jq -r '.[0].name')\nbeaker image delete $beaker_user/open_instruct_dev_uv \nbeaker image create open_instruct_dev_uv -n open_instruct_dev_uv -w ai2/$beaker_user\n</code></pre> <p>If you are internally at AI2, you may launch experiments using our always-up-to-date auto-built image <code>nathanl/open_instruct_auto</code>.</p>"},{"location":"safety-eval/safety/","title":"Safety Evaluations","text":"<p>We are using the Ai2 Safety Evaluation suite for safety evals. This contains a bunch of sub-evals, and you can learn more by looking at the eval-safety fork.</p>"},{"location":"safety-eval/safety/#running-at-ai2","title":"Running at Ai2","text":"<p>This should be the most relevant thing for internal Ai2 users of open-instruct. To run evals, use the task suite <code>SAFETY_EVAL</code> or <code>SAFETY_EVAL_REASONING</code> when calling <code>submit_eval_jobs.py</code>. This will create a job that uploads and runs the safety evaluations (and uploads to the leaderboard if the appropriate flag is set). </p> <p>An example command on a reasoning model would be: <pre><code>python scripts/submit_eval_jobs.py \\\n    --model_name &lt;model name&gt; \\\n      --location &lt;beaker id&gt; \\\n      --is_tuned --workspace tulu-3-results \\\n      --preemptible \\\n      --use_hf_tokenizer_template \\\n      --beaker_image nathanl/open_instruct_auto \\\n      --upload_to_hf allenai/tulu-3-evals \\\n      --run_oe_eval_experiments \\\n      --oe_eval_task_suite \"SAFETY_EVAL_REASONING\"\n</code></pre></p> <p>An example command on a non-reasoning model would be: <pre><code>python scripts/submit_eval_jobs.py \\\n    --model_name &lt;model name&gt; \\\n      --location &lt;beaker id&gt; \\\n      --is_tuned --workspace tulu-3-results \\\n      --preemptible \\\n      --use_hf_tokenizer_template \\\n      --beaker_image nathanl/open_instruct_auto \\\n      --upload_to_hf allenai/tulu-3-evals \\\n      --run_oe_eval_experiments \\\n      --oe_eval_task_suite \"SAFETY_EVAL\"\n</code></pre></p>"},{"location":"safety-eval/safety/#running-on-an-interactive-session","title":"Running on an interactive session","text":"<p>Clone the fork and run from that location.</p>"},{"location":"safety-eval/safety/#safety-benchmarks","title":"Safety benchmarks","text":"<p>For all benchmarks requiring safety evaluation unless noted otherwise, as a default, we use the WildGuard classifier to evaluate the safety of model outputs.</p> <ul> <li>WildGuardTest</li> <li>Harmbench</li> <li>ToxiGen</li> <li>XSTest</li> <li>JailbreakTrigger (in TrustLLM)</li> <li>Do-anything-now</li> <li>WildJailbreak (both harmful and benign contrast sets)</li> </ul> <pre><code>PYTHONPATH=safety-eval python evaluation/run_all_generation_benchmarks.py    \\\n --model_name_or_path allenai/tulu-2-dpo-7b     --model_input_template_path_or_name tulu2    \\\n  --report_output_path ./generation_results/metrics.json     --save_individual_results_path ./generation_results/all.json \\\n  --hf_upload_name {HF upload name} --upload_to_hf {HF repo ID} --min_gpus_per_task {num. GPUs available}\n</code></pre> <p>Changing classifiers for safety benchmarks:</p> <p>You can change the safety classifier used for evaluation by specifying the <code>classifier_model_name</code> in the yaml file. For example, when you want to use the HarmBench's classifiers for evaluation on HarmBench, you can use <code>HarmbenchClassifier</code> as the <code>classifier_model_name</code>. Please check out the <code>evaluation/tasks/generation/harmbench/default.yaml</code> and <code>evaluation/tasks/classification/harmbench/harmbench_classsifier.yaml</code> to see the classifier's specification.</p>"},{"location":"safety-eval/safety/#running-with-gantry","title":"Running with gantry","text":"<p>You can also run with gantry, if you want to test changes. Important: Before you run any command with gantry, make sure you commit and push, since gantry will attempt to clone the repo with your local latest commit hash.</p> <p>See the \"One-Time Setup\" section below before running commands. To test your setup, run the following command -- if this job succeeds, then you're ready to run evaluations with gantry.</p> <pre><code>gantry run --workspace {workspace} --budget ai2/oe-adapt --beaker-image kavelr/oe-safety --venv base --cluster ai2/jupiter --env-secret OPENAI_API_KEY=openai_api_key --env-secret HF_TOKEN=hf_token -- python -c 'print(\"Hello world\")'\n</code></pre> <p>You can freely add any additional arguments to give to Beaker, such as a <code>--priority</code> tag which can be set to preemptible, normal, high, or urgent. AI2 policies may restrict the priorities that are available to users on certain clusters.</p> <p>In the examples below, text within {} tags should be replaced with your own values. </p> <p>As a convenience, you can use the <code>evaluation/gantry_run.sh</code> script which includes some necessary arguments. You can use it the same way as <code>gantry run</code>, but excluding these boilerplate arguments (take a look at the script to see what it includes). Example usage:</p> <pre><code>PYTHONPATH=safety-eval ./evaluation/gantry_run.sh --workspace {workspace} --cluster {cluster} --gpus {n_gpus} \\\n    --priority {priority} -- python evaluation/run_all_generation_benchmarks.py \\\n    --model_name_or_path allenai/tulu-2-dpo-7b \\\n    --model_input_template_path_or_name tulu2 \\\n    --report_output_path /results/metrics.json\n</code></pre>"},{"location":"safety-eval/safety/#extra-beaker-commands","title":"Extra Beaker Commands","text":"<p>Here is an example using the full <code>gantry run</code> command. Use the beaker image <code>seungjuh/oe-safety-support-olmo17</code></p> <p>Important: Please include all the beaker arguments exactly as in the examples unless intentionally modifying some configuration. Many of them are necessary to avoid job failures, such as <code>--beaker-image</code>, <code>--venv</code>, and <code>--env-secret</code>. Note that <code>openai_api_key</code> and <code>hf_token</code> are Beaker workspace secret names, so should not be replaced with actual values (see One-Time Setup).</p> <p>Note that the <code>--</code> divides the gantry command from the evaluation command - you can edit the second part to run whatever eval suite you want from the <code>eval.py</code> script. Any additional Beaker arguments such as a dataset mount to use a model from a Beaker dataset or adding a priority tag can be added before the <code>--</code>.</p> <p>You can also run all generator evaluations parallelized across the GPUs allocated to your batch job, like so: <pre><code>gantry run --workspace {your_workspace} --cluster {cluster} --gpus {n_gpus} \\\n    --name {beaker_experiment_name} --task-name {beaker_task_name} --beaker-image seungjuh/oe-safety-support-olmo17 --venv base \\\n    --env-secret OPENAI_API_KEY=openai_api_key \\\n    --env-secret HF_TOKEN=hf_token \\\n    --budget {budget} -- python evaluation/run_all_generation_benchmarks.py \\\n    --model_name_or_path allenai/tulu-2-dpo-7b \\\n    --model_input_template_path_or_name tulu2 \\\n    --report_output_path /results/metrics.json --save_individual_results_path /results/all.json\n</code></pre></p> <p>Because the <code>--report_output_path</code> argument is set to <code>/results/metrics.json</code>, the output will automatically get logged to Beaker metrics in the experiment page (example).</p>"},{"location":"safety-eval/safety/#gantry-one-time-setup","title":"Gantry One-Time Setup","text":"<p>Before you can use gantry, there are a couple of things to set up. For the workspace you use, ensure it is owned by the <code>ai2</code> organization, or gantry won't be able to create the experiments.</p> <ol> <li>Run <code>pip install beaker-gantry beaker-py</code></li> <li>Create a GitHub personal access token with \"repo\" scope</li> <li>Go to https://github.com/settings/tokens and authorize your token to configure SSO access to the allenai organization</li> <li>Run <code>gantry config set-gh-token</code> and paste the token created above when prompted</li> <li>Create a HuggingFace access token with \"read\" scope (this is used to authenticate for using restricted models like Llama series)</li> <li>Run <code>beaker secret write --workspace {your_workspace} hf_token {your_token}</code></li> <li>Obtain an OpenAI API key and run `beaker secret write --workspace {your_workspace} openai_api_key {your_api_key}</li> </ol> <p>Doing these steps once will set up your workspace to use gantry.</p>"},{"location":"safety-eval/safety/#common-gotchas","title":"Common Gotchas","text":"<p>If you're experiencing job failures, here are some things to check:</p> <ul> <li>Make sure your local changes are committed,  pushed, and up to date with the remote</li> <li>Make sure you have <code>--beaker-image seungjuh/oe-safety-support-olmo17</code> and <code>--venv base</code> in your <code>gantry run</code> command</li> <li>Check your GitHub personal access token is authorized to access the allenai organization</li> <li>Make sure the openai_api_key and hf_token secrets exist in your Beaker workspace</li> </ul>"}]}