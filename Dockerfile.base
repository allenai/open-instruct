# Run the following command to build `ghcr.io/allenai/cuda:12.8-dev-ubuntu22.04-torch2.6.0-v1.2.170
# docker build -f Dockerfile.base --build-arg CUDA="12.8.0" --build-arg VARIANT=devel --build-arg DIST=ubuntu22.04 --build-arg TORCH_VER="2.6.0" --build-arg BEAKER_VERSION=v1.5.208 --build-arg VULKAN_SDK_VERSION="1.3.275" -t beaker/cuda:12.8-ubuntu22.04-torch2.6.0-v1.2.170 .

ARG CUDA
ARG DIST
ARG VARIANT
FROM --platform=linux/amd64 nvidia/cuda:${CUDA}-${VARIANT}-${DIST} AS basic-cuda

ARG DEBIAN_FRONTEND="noninteractive"
ENV TZ="America/Los_Angeles"

# Install base tools.
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    git \
    jq \
    language-pack-en \
    make \
    man-db \
    manpages \
    manpages-dev \
    manpages-posix \
    manpages-posix-dev \
    sudo \
    unzip \
    vim \
    wget \
    fish \
    parallel \
    iputils-ping \
    htop \
    emacs \
    zsh \
    rsync \
    tmux \
    && rm -rf /var/lib/apt/lists/*

# This ensures the dynamic linker (or NVIDIA's container runtime, I'm not sure)
# puts the right NVIDIA things in the right place (that THOR requires).
ENV NVIDIA_DRIVER_CAPABILITIES=graphics,utility,compute

# Install conda. We give anyone in the users group the ability to run
# conda commands and install packages in the base (default) environment.
# Things installed into the default environment won't persist, but we prefer
# convenience in this case and try to make sure the user is aware of this
# with a message that's printed when the session starts.
RUN wget https://repo.anaconda.com/miniconda/Miniconda3-py312_25.1.1-0-Linux-x86_64.sh \
    && echo "832de27a5a35b7963f0d83466abada3eb138e51985255f190e0dc350427a9dd1 Miniconda3-py312_25.1.1-0-Linux-x86_64.sh" \
        | sha256sum --check \
    && bash Miniconda3-py312_25.1.1-0-Linux-x86_64.sh -b -p /opt/miniconda3 \
    && rm Miniconda3-py312_25.1.1-0-Linux-x86_64.sh

ENV PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:$PATH
ENV LD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/lib64:$LD_LIBRARY_PATH

# Install uv and uvx
# uv will default to using conda's python executable at /opt/miniconda3/bin/python
# https://docs.astral.sh/uv/concepts/python-versions/#discovery-of-python-versions
COPY --from=ghcr.io/astral-sh/uv:0.7.17 /uv /uvx /bin/

# Install torch. See https://pytorch.org/get-started/locally/
# Current version 2.6.0 supports CUDA runtime version up to 12.6
# Previous version 2.5.1 supports CUDA runtime version up to 12.4
# The `nightly` build currently installs version 2.8.0dev which supports Blackwell GPUs.
# Any other values supplied for TORCH_VER will skip the torch installation.
# A lower CUDA runtime version is compatible with a higher CUDA driver version.
# TODO: explore using https://docs.astral.sh/uv/guides/integration/pytorch/#automatic-backend-selection
ARG TORCH_VER
RUN echo "To install torch, TORCH_VER must be `2.7.0`, `2.6.0`, or `2.5.1`"
RUN if [ "$TORCH_VER" = "2.6.0" ]; then \
        uv pip install --no-cache-dir --system \
        torch==2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124; \
    elif [ "$TORCH_VER" = "2.5.1" ]; then \
        uv pip install --no-cache-dir --system \
        torch==2.5.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121; \
    elif [ "$TORCH_VER" = "2.7.0" ]; then \
        uv pip install --no-cache-dir --system \
        torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128; \
        uv pip install --no-cache-dir --system nvidia-nccl-cu12==2.26.5; \
    else \
        echo "TORCH not installed"; \
    fi

# Install a few additional utilities user-wide in isolated venvs
# notebook is the correct way to install jupyter notebook (https://jupyter.org/install)
RUN uv tool install --no-cache-dir gpustat
RUN uv tool install --no-cache-dir notebook

# Ensure users can modify their container environment.
RUN echo '%users ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers

# Install DOCA OFED user-space drivers
# See https://docs.nvidia.com/doca/sdk/doca-host+installation+and+upgrade/index.html
# doca-ofed-userspace ver 2.10.0 depends on mft=4.31.0-149
ENV MFT_VER 4.31.0-149
RUN wget https://www.mellanox.com/downloads/MFT/mft-${MFT_VER}-x86_64-deb.tgz && \
    tar -xzf mft-${MFT_VER}-x86_64-deb.tgz && \
    mft-${MFT_VER}-x86_64-deb/install.sh --without-kernel && \
    rm mft-${MFT_VER}-x86_64-deb.tgz

ENV DOFED_VER 2.10.0
ENV OS_VER ubuntu2204
RUN wget https://www.mellanox.com/downloads/DOCA/DOCA_v${DOFED_VER}/host/doca-host_${DOFED_VER}-093000-25.01-${OS_VER}_amd64.deb && \
    dpkg -i doca-host_${DOFED_VER}-093000-25.01-${OS_VER}_amd64.deb && \
    apt-get update && apt-get -y install doca-ofed-userspace && \
    rm doca-host_${DOFED_VER}-093000-25.01-${OS_VER}_amd64.deb

ENTRYPOINT ["bash", "-l"]

################################################################################
# Build default cuda image with additional installs.
################################################################################
FROM basic-cuda AS default-cuda

# Install AWS CLI
RUN curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip" \
    && unzip awscliv2.zip \
    && ./aws/install \
    && rm awscliv2.zip

# Install Google Cloud CLI
RUN echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main" \
        | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list \
    && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg \
        | apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - \
    && apt-get update -y && apt-get install google-cloud-sdk -y \
    && rm -rf /var/lib/apt/lists/*

# Install Docker CLI. Version matches Beaker on-premise servers.
RUN curl -fsSL https://download.docker.com/linux/static/stable/x86_64/docker-27.5.1.tgz -o docker.tgz \
    && sudo tar xzvf docker.tgz --strip 1 -C /usr/local/bin docker/docker \
    && rm docker.tgz

# Install Beaker
ARG BEAKER_VERSION
RUN curl --silent \
    --connect-timeout 5 \
    --max-time 10 \
    --retry 5 \
    --retry-delay 0 \
    --retry-max-time 40 \
    --output beaker.tar.gz \
    "https://beaker.org/api/v3/release/cli?os=linux&arch=amd64&version=${BEAKER_VERSION}" \
    && tar -zxf beaker.tar.gz -C /usr/local/bin/ ./beaker \
    && rm beaker.tar.gz

# Install Beaker Gantry user-wide in an isolated venv
RUN uv tool install --no-cache-dir beaker-gantry

# Add Vulkan SDK to the base image
ARG VULKAN_SDK_VERSION
# Download the version LunarG VulkanSDK Packages
RUN    wget -qO- https://packages.lunarg.com/lunarg-signing-key-pub.asc | sudo tee /etc/apt/trusted.gpg.d/lunarg.asc \
    && wget -qO /etc/apt/sources.list.d/lunarg-vulkan-${VULKAN_SDK_VERSION}-jammy.list \
            https://packages.lunarg.com/vulkan/${VULKAN_SDK_VERSION}/lunarg-vulkan-${VULKAN_SDK_VERSION}-jammy.list \
    && apt-get update && apt-get install -y --no-install-recommends vulkan-sdk

# Shell customization including prompt and colors.
COPY docker/profile.d/ /etc/profile.d/

# The -l flag makes bash act as a login shell and load /etc/profile, etc.
ENTRYPOINT ["bash", "-l"]
