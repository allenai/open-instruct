[
  {
    "initial_prompt": "What publicly available datasets are typically used for evaluating type inference systems in python?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What publicly available datasets are typically used for evaluating type inference systems in python?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define what is the goal of using a type inference system for programming languages in general.",
            "weight": 0.13333333333333333,
            "evidence": [
              "Goal of type inference: Automatically deduce the most general type for each expression. Two key points: 1. Automatically inferring types: This means the programmer has to write no types, but still gets all the benefit from static typing 2. Inferring the most general type: This means we want to infer polymorphic types whenever possible"
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should emphasize on the importance of an automatic type inference system for Python.",
            "weight": 0.13333333333333333,
            "evidence": [
              " its dynamic type system can lead to potential type errors, leading researchers to explore automatic type inference approaches for Python programs.",
              "In Python, which is dynamically typed, this determination takes place at runtime. To address potential ambiguities, developers can utilize type annotations, which explicitly specifies the expected data types of variables or function returns. As the complexity of software projects increases, programmers find it increasingly challenging to maintain consistent data types. In response to this challenge, both industry and academia have developed type inference tools and static type checkers."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss the need for a unified approach for evaluating different type inference systems and mention several evaluation metrics, including exact matches, report of missing types, accuracy, etc.",
            "weight": 0.13333333333333333,
            "evidence": [
              "In light of the growing interest in type inference research for Python, both researchers and practitioners require a standardized process to assess the performance of various type inference techniques.",
              "Exact matches: The number of inferred types that exactly match the ground truth. This metric is used widely used in the literature to evaluate type inference tools (Allamanis et al., 2020; Peng et al., 2022; Mir et al., 2022).",
              "Report of missing types: List of types that are present in the ground truth but are unreported by the tools.",
              "### **Accuracy** The correctness of the inferred types compared to the ground truth. ### **Performance** The computation resources and time required to run the type inference. ### **Coverage** The range and variety of code constructs and libraries handled by the inference system."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should enumerate publicly available datasets used for evaluating type inference systems in Python and provide a brief description for each of them.",
            "weight": 0.13333333333333333,
            "evidence": [
              "1. **ManyTypes4Py**: - **Description**: ManyTypes4Py is a large Python dataset for machine learning-based type inference. It contains 5,382 Python projects with over 869,000 type annotations. The dataset is split into training, validation, and test sets by files to facilitate the training and evaluation of machine learning models. - **Features**: The dataset includes a lightweight static analyzer pipeline to extract type information from abstract syntax trees (ASTs) and store the results in JSON-formatted files.",
              "The Typilus model [8] is accompanied by a dataset that contains 600 Python projects. Moreover, the source code files of Typilus' dataset are converted to graph representations that are only suitable for training the Typilus model.",
              "2. **TypeEvalPy**: - **Description**: TypeEvalPy is a micro-benchmarking framework for evaluating type inference tools. It contains 154 code snippets with 845 type annotations across 18 categories targeting various Python features. - **Features**: The framework manages the execution of containerized tools, transforms inferred types into a standardized format, and produces meaningful metrics for assessment.",
              "3. **BigQuery Public Datasets**: - **Description**: BigQuery provides a range of public datasets that can be used for various purposes, including type inference. These datasets are accessible through the Google Cloud Public Dataset Program and can be queried using SQL or GoogleSQL. - **Features**: The datasets include a variety of data sources, such as weather information, GitHub repository data, and Wikipedia revision history.",
              "Raychev et al. [16] published the Python-150K dataset in 2016, which contains 8,422 Python projects.",
              "Python-150K dataset [16] is not collected solely for the ML-based type inference task, meaning that a large number of projects in the dataset may not have type annotations at all, especially given the time that the dataset was created.",
              "Our main dataset, BetterTypes4Py, is constructed by selecting a high-quality subset from the ManyTypes4Py dataset (Mir et al., 2021), which was used to train Type4Py.",
              "InferTypes4Py, a test set derived from the source code of Typilus, Type4Py, and our own tool, none of which were used as CodeT5's (pre-)training data"
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer could explain different categories of methods for type inference in Python such as rule-based and ML-based approaches.",
            "weight": 0.06666666666666667,
            "evidence": [
              "Existing type inference approaches can be generally grouped into three categories, i.e., rule-based, supervised, and cloze-style approaches. The rule-based type inference approaches can ensure the accuracy of predicted variable types, but they suffer from low coverage problems caused by dynamic features and external calls. Supervised type inference approaches, while feature-agnostic and able to mitigate the low coverage problem, require large, high quality annotated datasets and are limited to pre-defined types. As zero-shot approaches, the cloze-style approaches reformulate the type inference problem into a fill-in-the-blank problem by leveraging the general knowledge in powerful pre-trained code models. However, their performance is limited since they ignore the domain knowledge from static typing rules which reflect the inference logic."
            ]
          }
        ]
      }
    },
    "case_id": "d44280651a6fb71d56ee96834e180fa6",
    "annotator": "Annotator 1 Assignments",
    "agreement": true
  },
  {
    "initial_prompt": "What are leading methods for generating hard examples of the boolean satisfiability problem, and what are their strengths and weaknesses?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are leading methods for generating hard examples of the boolean satisfiability problem, and what are their strengths and weaknesses?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define the boolean satisfiability problem.",
            "weight": 0.17142857142857143,
            "evidence": [
              "The Boolean satisfiability problem (SAT) is a fundamental NP-complete decision problem in automated reasoning and mathematical logic. As evidenced by the results of SAT competitions, the performance of SAT solvers varies substantially between different SAT categories (random, crafted, and industrial).",
              "What is SAT? Given a propositional logic (Boolean) formula, find a variable assignment such that the formula evaluates to true, or prove that no such assignment exists"
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should point that why generating examples of the boolean satisfiability problem is hard",
            "weight": 0.17142857142857143,
            "evidence": [
              "A key limitation of current problem generators concerns their use in the evaluation of incomplete local search methods. This is because the generators generally produce a mixture of solvable (satisfiable) and unsolvable (unsatisfiable) instances. When a local search style method does not find a solution, it can be difficult to determine whether this is because the algorithm fails to find a solution or because the instance itself is unsolvable."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should include a list of methods for generating hard examples of the boolean satisfiability problem with a brief description",
            "weight": 0.17142857142857143,
            "evidence": [
              "## 1. Random k-SAT Generators### DescriptionRandom k-SAT involves generating formulas by randomly selecting clauses, each consisting of k literals, from a given set of variables. The famous 3-SAT variant is particularly well-studied.### Strengths**Parameter Control**: Allows fine-tuning through parameters like the ratio of clauses to variables (r = m/n), which influences the problem's hardness.**Study of Phase Transitions**: Near the critical threshold (~4.26 for 3-SAT), instances of SAT exhibit a sharp jump in difficulty, providing a natural source of hard instances.### Weaknesses**Unpredictability**: While near the critical threshold can yield hard instances, the problem's hardness isn't guaranteed.**Scalability**: Beyond certain sizes and clause lengths, the generation process may become impractically slow.",
              "## 2. Structured SAT Generators### DescriptionStructured SAT generators create instances based on specific combinatorial structures or particularly crafted mathematical properties. Examples include graph-based instances and Grid-based SAT.### Strengths**Controlled Hardness**: More predictable instance difficulty due to embedded structures.**Domain-specific**: Useful for SAT applications in specific fields where structured instances naturally occur, such as circuit design.### Weaknesses**Limited Generalizability**: Hardness might not translate to other problem settings.**Design Complexity**: Creating structured configurations that ensure hardness can be more complex compared to random generation.### Key References",
              "Graph-based methods:- High-girth bipartite incidence graphs: This approach incrementally constructs graphs with high expansion properties, which implies high resolution width and results in hard k-SAT instances (29, Ansotegui et al., 2008).- Eulerian graph transformations: SAT instances based on Eulerian graphs are designed to be challenging for resolution-based SAT solvers  (26, Markstrom, 2006).- Digraph-based generators: Weak models generated from strong digraphs can produce minimal unsatisfiable SAT instances, which are particularly hard to solve (57, Biro et al., 2020).",
              "### 5. Evolutionary AlgorithmsEvolutionary algorithms, such as genetic algorithms, can be used to evolve hard SAT instances. These algorithms iteratively generate and mutate instances based on their hardness. The strength of this method lies in its ability to generate instances that are tailored to the specific strengths and weaknesses of a solver. The weakness is that the generation process can be computationally expensive and may not converge to the hardest instances.",
              "7. Circuit-Based Instance GenerationThis method generates SAT instances by encoding complex Boolean circuits.Method: Design circuits with properties known to be challenging for SAT solvers (e.g., multipliers) and convert them to CNF form.Strengths:- Generates instances with structure similar to real-world verification problems- Difficulty can be controlled by adjusting circuit complexity- Instances have known solutions (the circuit's truth table)Weaknesses:- May require expertise in circuit design- The translation to CNF can impact instance difficulty in unpredictable ways",
              "Markov Chain Monte Carlo (MCMC)-based Methods: A wide variety of MCMC-based algorithms have been proposed in the literature to sample from complex distributions. These include Metropolis algorithm, simulated annealing and the [37, 38, 39]. The core idea of these algorithms is to sample using carefully chosen Markov chains in which the steady state distribution matches the desired distribution. MCMC methods guarantee convergence to uniform distribution only when run for sufficiently long time. Most practical algorithms based on MCMC methods, however, use heuristic adaptations to ensure better performance.",
              "Weighted Binary Decision Diagram (BDD) based Methods: A new approach based on sampling from a set of constraints based on weighted binary decision diagrams [41] was proposed in [42, 43]. The core idea of the algorithm is to construct a BDD-based on the input constraints and then generate uniform samples in a single pass over the BDD. The approach works well for small to medium-sized examples but does not scale to larger problems. Hence it is not scalable to large problems in practice. A detailed analysis of the scalability limitations of BDD-based methods is presented in [35]. An alternative approach to uniform generation based on BDDs was proposed in [44]. This approach relies on constructing an equivalent circuit for BDD constraints [44]. Unfortunately, this approach fails to provide guarantees of uniformity.",
              "Interval-propagation-based sampling: Interval-propagation-based sampling techniques have been used by some researchers to address the scalability challenges posed by uniform generation in practice [45]. The central idea underlying these techniques is to maintain intervals of values that a variable can take and generate samples by performing random sampling over these intervals. The simplicity of such approaches provides good performance in practice but the distributions generated can deviate significantly from the uniform distribution [35].",
              "Belief networks: Another class of methods based on Constraint satisfaction problems (CSP), particularly belief propagation, have been proposed in [46, 47]. The proposed techniques improve on the traditional MCMC based methods by integrating sampling with back-jumping search and no-good learning. Experimental comparisons, however, have shown that these techniques perform poorly compared to MCMC based techniques with random walk and simulated annealing heuristics, as in SampleSAT [47].",
              "Hashing-based techniques Sipser pioneered hashing-based approach in [34] building upon the universal hashing introduced by Carter and Wegman [48]. This has subsequently been used in theoretical [18] and practical [33] treatments of uniform sampling. The key idea in these works is to randomly partition the solution space into \"small cells\" of roughly equal size. The act of picking a solution randomly chosen cell provides the required guarantees.",
              "In this paper, we present empirical results showing that random instances of satisfiability can be generated in such a way that easy and hard sets of instincts for a particular sat procedure, anyway) are predictable in advance",
              "In this paper, we have shown, both theoretically and practically, that the models RB (and RD) can be used to produce, very easily, hard random instances. More importantly, the same result holds for instances that are forced to be satisfiable."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should explain the different between solving (checking) and generating boolean satisfiability problem",
            "weight": 0.08571428571428572,
            "evidence": [
              "Apart from the classical problem of checking Boolean satisfiability, the problems of generating satisfying assignments uniformly at random, and of counting the total number of satisfying assignments have also attracted significant theoretical and practical interest over the years"
            ]
          }
        ]
      }
    },
    "case_id": "42f2553e73b1dc84669baf830c3eb140",
    "annotator": "Annotator 1 Assignments",
    "agreement": true
  },
  {
    "initial_prompt": "What is unique k-sat and what theoretical results are known about it?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What is unique k-sat and what theoretical results are known about it?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should define the general k-sat problem and unique k-sat problems formally.",
            "weight": 0.13333333333333333,
            "evidence": [
              "Unique k-SAT is a variation of the classical k-SAT (k-Satisfiability) problem, which itself is a well-known problem in computer science and combinatorial optimization. In the k-SAT problem, you are given a boolean formula in conjunctive normal form (CNF), where each clause contains exactly k literals, and you need to determine if there exists an assignment to the variables that satisfies the entire formula. In Unique k-SAT, the additional constraint is that the boolean formula has at most one satisfying assignment.",
              "The general SAT problem is as follows. Given are n Boolean variables x1, . . . , xn, and a formula f(x1, . . . , xn) = (a1,1x1 [?] * * * [?] a1,nxn) [?] * * * [?] (am,1x1 [?] * * * [?] am,nxn), (1) where by ai,j [?] {[?]1, 0, 1} we denote that in each clause i (i.e. the each disjunction contained in parentheses as written above) each variable xi may appear negated, not negated, or not at all.",
              "The formula f is thus the conjunction of m clauses, each of which are disjunctions of at most n literals, and is therefore said to be in conjunctive normal form. The SAT problem is that of determining whether there is an assignment to the variables x1, . . . , xn such that f(x1, . . . , xn) evaluates to true.",
              "In the k-SAT problem, each clause is limited to being a disjunction of at most k literals. In this paper, we will focus on the Unique-k-SAT problem, in which we are promised that the formula in uniquely satisfiable -- that is, that f is true only for one set of values of x1, . . . , xn. The problem is this case is that of finding the satisfying assignment."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should first explain the brute force solution for solving this problem and its complexity.",
            "weight": 0.13333333333333333,
            "evidence": [
              "The naive solution to the Unique-k-SAT problem would be to simply iterate through all possible variable assignments until the satisfying assignment is found. As each variable in an assignment may be either true or false, there are 2n such assignments, and this algorithm will thus take O(2n ) time"
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should mention that this problem is NP-complete since it is harder than the K-sat problem and quote relevant proof for it.",
            "weight": 0.13333333333333333,
            "evidence": [
              "1. **NP-Completeness**: Unique \\(k\\)-SAT is NP-complete for \\(k \\ge 3\\), meaning that it is as difficult to solve as the general \\(k\\)-SAT problem. 2. **Reduction from \\(k\\)-SAT**: There exists a randomized polynomial-time reduction from \\(k\\)-SAT to Unique \\(k\\)-SAT, showing that Unique \\(k\\)-SAT is at least as hard as \\(k\\)-SAT. 3. **Isolation Lemma**: A given satisfiable \\(k\\)-CNF can be efficiently probabilistically reduced to a uniquely satisfiable \\(k\\)-CNF with non-trivial, albeit exponentially small, success probability.4. **Complexity**: If Unique 3-SAT can be solved in time \\(2^{\\epsilon n}\\) for every \\(\\epsilon > 0\\), then so can \\(k\\)-SAT for all \\(k \\ge 3\\)."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should enumerate some existing solutions with their run-times for solving this problem.",
            "weight": 0.13333333333333333,
            "evidence": [
              "3.3 Deterministic Algorithms: Several deterministic algorithms have been developed for Unique k-SAT: - For Unique 3-SAT, there exists a deterministic algorithm running in time O(1.3071^n), where n is the number of variables [4].- For Unique 4-SAT, the best known deterministic algorithm runs in time O(1.4704^n) [5]. These algorithms are significantly faster than the best known algorithms for general k-SAT, showcasing the power of the uniqueness promise.",
              "Pudlak, Saks, and Zane [6] has been improved upon recently to produce a deterministic solution to the Unique-3-SAT solution that runs in O(1.307n ) time [7], as well as a probabilistic solution to the 4-SAT that runs in O(1.46981n ) time [4].",
              "2. **Randomized Algorithms:**- Randomized reductions, as suggested by the Valiant-Vazirani theorem, play an important role in solving Unique k-SAT by converting it to a simpler problem that can then be tackled using probabilistic methods[^2].",
              "5. **Quantum Solutions**: Grover's algorithm can be applied to solve Unique \\(k\\)-SAT with a competitive asymptotic running time of \\(O(2^{n/2})\\), which is competitive with the current best runtime for deterministic and randomized solutions."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer could point to the fact that like other NP-hard problems, there may be instances of this problem that can be easily solved.",
            "weight": 0.06666666666666667,
            "evidence": [
              "Not necessarily every instances of k-SAT, for a fixed k [?] 3 show worst-case behaviors.",
              "Research has shown that the average-case complexity of Unique k-SAT demonstrates interesting properties. Specifically, it has been studied under random distributions of problems, showing that the problem remains hard on average, under certain distributions[^4]."
            ]
          }
        ]
      }
    },
    "case_id": "913bc2f74263079777db4b94a22d7f5a",
    "annotator": "Annotator 1 Assignments",
    "agreement": true
  },
  {
    "initial_prompt": "In recommendation systems, how are new methods that optimize diversity typically evaluated?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "In recommendation systems, how are new methods that optimize diversity typically evaluated?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly point to the importance of considering diversity in recommendation systems and how its impact on research in this field.",
            "weight": 0.09999999999999999,
            "evidence": [
              "Diversity in recommendation systems refers to the variety of items suggested to users, aiming to avoid monotony and provide a broader range of options.",
              "Diversification has become one of the leading topics of recommender system research not only as a way to solve the over-fitting problem but also an approach to increasing the quality of the user's experience with the recommender system."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide different diversity measures that have been considered in the literature such as similarity, coverage, novelty, etc.",
            "weight": 0.09999999999999999,
            "evidence": [
              "2.1 Intra-List Diversity (ILD): Intra-List Diversity measures the dissimilarity between items within a single recommendation list. It is commonly calculated using the average pairwise distance between items [1].",
              "2.2 Coverage: Coverage measures the proportion of items from the entire catalog that appear in recommendations across all users [2].",
              "2.3 Novelty and Serendipity: Novelty measures how unfamiliar recommended items are to users, while serendipity captures the pleasant surprise factor of recommendations [3].",
              "2.4 Gini Coefficient: The Gini coefficient measures the inequality in item popularity distribution within recommendations [4]."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss that there is no single unified metric for measuring diversity as each paper tends to define its own measure.",
            "weight": 0.09999999999999999,
            "evidence": [
              "We found that while all research groups agree that diversity is important and should be measured, few groups agree on the metric that should be used. While some metrics (such as the intra-list diversity) appear more often, the community still has not accepted one (or several) of them as the preferred diversity measurement."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answers should mention that conventional performance metrics should be used in conjunction with diversity measures for an overall comparison of different recommendation systems.",
            "weight": 0.09999999999999999,
            "evidence": [
              "Several of research groups have focused on the effect of diversification on the quality of the recommendation procedure. In order to evaluate this they used a combination of proposed diversity measure and existing RS performance measures such as F-measure, MAE and NMAE [28] to determine how diversification impacts the overall performance of the RS."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should explain that the evaluation can be done offline, online through user study or in a hybrid way.",
            "weight": 0.09999999999999999,
            "evidence": [
              "1. **Offline Evaluation**: Uses historical data to evaluate the performance of the recommendation algorithm. Metrics such as precision, recall, and F1-score are used to assess accuracy, while diversity metrics are used to assess diversity.",
              "2. **Online Evaluation**: Involves A/B testing or sample testing to evaluate the performance of the recommendation algorithm in real-world scenarios. This method provides more accurate results but is resource-intensive.",
              "3. **Hybrid Evaluation**: Combines offline and online evaluation methods to leverage the strengths of both approaches."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer could discuss some challenges of evaluating diversity in RS systems such as the tradeoff between diversity measure and accuracy.",
            "weight": 0.049999999999999996,
            "evidence": [
              "1. **Trade-off between Diversity and Accuracy**: Optimizing diversity often comes at the cost of accuracy. Finding the right balance between these two metrics is crucial.",
              "2. **Cold Start Problem**: Evaluating diversity for new users or items with limited interaction data is challenging.",
              "3. **Scalability**: Evaluating diversity for large-scale recommendation systems can be computationally expensive."
            ]
          },
          {
            "name": "nice_to_have_item_1",
            "criterion": "The answer could quote the first paper introducing diversity in the recommendation systems.",
            "weight": 0.049999999999999996,
            "evidence": [
              "Bradley and Smith [2] were one the first to mention diversity by proposing the introduction of diversification in the recommendation procedure and also evaluating a new algorithm designed to diversify recommendations."
            ]
          }
        ]
      }
    },
    "case_id": "1f2a54322f88581b47108f363b961d22",
    "annotator": "Annotator 1 Assignments",
    "agreement": true
  },
  {
    "initial_prompt": "In robotics, what are the leading methods for learning terrain traversibility costs automatically from robot experience?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "In robotics, what are the leading methods for learning terrain traversibility costs automatically from robot experience?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should define terrain traversability learning and its importance in robotics.",
            "weight": 0.0923076923076923,
            "evidence": [
              "The autonomous exploration of unstructured environments by mobile robots is witnessing increased interest, as it helps to accomplish diverse tasks such as search and rescue, surveying and data collection (e.g., [1] with unmanned ground vehicles and [2] with unmanned aerial vehicle), and surveillance. Performing these missions by autonomous mobile robots presents several advantages. It allows the avoidance of human intervention in hazardous area [3], as well as overcomes difficulties that may arise in the case of remotely controlled mobile robots such as a loss of connectivity or degraded latency. With robots being utilized in a wide spectrum of unstructured environments, the need for systems to build reliable representations of the environment is critical. Therefore, the estimation of the traversability of terrains is of great importance."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should explain supervised or self-supervised learning methods for terrain traversability",
            "weight": 0.0923076923076923,
            "evidence": [
              "Supervised (learning) method: a data-driven method that learns from provided examples, i.e., using training data that include ground truth labels, usually provided by experts, prior to training",
              "In this context, a recent study by Martinez et al. [85] introduced the supervised learning method, which uses labelled data for training quantifying part of terrains such as ground, grass (of height below 5 cm), and the side-walk portion to label it as traversable area via the use of Lidar sensing.",
              "Another supervised learning-based terrain classification approach has been studied by the authors in [161]. Here, a Microsoft Kinect V2 visual sensor able to supply infrared (IR), colour and depth stream features was used to predict terrain types categorised into five groups, i.e. gravel, sand, pavement, grass and litterfall & straw. After manual labelling and pre-processing, IR, colour and depth features were combined for the terrain recognition model with a Support Vector Machine (SVM) classifier.",
              "In this work, we proposed a self-supervised terrain classification framework that exploits the training signal from an unsupervised proprioceptive terrain classifier to learn an exteroceptive classifier for pixel-wise semantic terrain segmentation.",
              "This paper introduces a self-supervised traversability estimation method that can learn traversability in varied environments without manual annotations. Using the self-supervised labels only, the network is trained to predict traversable regions using a one-class classification method.",
              "We pose the traversability estimation as a vector regression task over vertical bands of the observed frame. The model is pre-trained through self-supervision to reduce the distribution shift between synthetic and real data and encourage shared feature learning. Then, supervised training on synthetic videos is carried out, while employing an unsupervised domain adaptation loss to improve its generalization capabilities on real scenes."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should explain unsupervised learning methods for terrain traversability",
            "weight": 0.0923076923076923,
            "evidence": [
              "Unsupervised (learning) method: an approach in which the learning model is given a dataset without specific instructions on what the output should be. The method then aims at automatically extracting patterns or structure in the data by finding meaningful features",
              "Within the review of recent traversability-related literature, [124] proposed an automatic labelling method as a part of unsupervised learning that learns samples from unlabelled data using 2D maps obtained from the Gazebo environment and converted to a 3D point cloud."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should explain reinforcement learning and inverse reinforcement learning methods for terrain traversability",
            "weight": 0.0923076923076923,
            "evidence": [
              "Reinforcement Learning (RL) and Inverse Reinforcement Learning (IRL) methods are being applied to learn terrain traversability costs from robot experiences and human demonstrations. These approaches enable robots to adapt to complex terrains and mimic specific navigation styles.Reinforcement Learning (RL) and Inverse Reinforcement Learning (IRL) have emerged as powerful techniques for learning terrain traversability costs in robotics. These methods offer the advantage of learning directly from robot experiences or human demonstrations, allowing for more adaptive and nuanced navigation strategies.IRL has gained significant attention for modeling terrain traversability, particularly in autonomous vehicles  (11, Grizzle et al., 2022). Unlike traditional approaches that use predefined metrics, IRL aims to learn traversability costs directly from a robot's traversing behaviors. This method allows for a more flexible and context-aware understanding of terrain challenges.",
              "Authors in [171] proposed a traversability map created via a fully convolutional network (FCN) based method whereby a feature map generated using Lidar has been used as input to the FCN. Also, trajectory planning has been implemented using Inverse Reinforcement Learning (IRL), and different methods under various scenes have been analysed",
              "In the study performed by the authors in [168], the depth images, the orientation of the robot and the elevation map which represent the obstacle around the robot have been used as inputs in deep reinforcement learning.",
              "Furthermore, research [167] proposed a path-planning algorithm based on deep-reinforcement learning, using a reward function relating to terrain slope.",
              "Another recent study, i.e. in [55], proposed a deep reinforcement learning-based navigation method that predicted the navigation actions: forward, tiny left, tiny right, hard left and hard right using images as inputs."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should explain online and Incremental learning methods for terrain traversability",
            "weight": 0.0923076923076923,
            "evidence": [
              "TLDR: Online and incremental learning techniques allow robots to continuously update their terrain traversability models based on real-time experiences. These methods enable robots to adapt to changing environments and improve their navigation capabilities over time.Online and incremental learning approaches have gained significant attention in robotics for learning terrain traversability costs. These methods allow robots to update their models in real-time as they encounter new terrains, making them particularly useful for long-term autonomous operation in dynamic environments.Pragr et al. developed a robotic system that incrementally learns to predict power consumption-based traversal costs experienced by robots over various terrains (7, Pragr et al., 2019). This system combines autonomous spatial exploration with simultaneous learning of the underlying traversal cost model, enabling continuous adaptation to new terrain types."
            ]
          },
          {
            "name": "most_important_item_5",
            "criterion": "The answer should explain transfer learning and multi-robot methods for terrain traversability",
            "weight": 0.0923076923076923,
            "evidence": [
              "TLDR: Transfer learning and multi-robot approaches enable the sharing of terrain traversability knowledge between different robots and scenarios. These methods improve adaptability and efficiency in learning traversability costs across diverse robotic platforms and environments.Transfer learning and multi-robot approaches have emerged as promising techniques for enhancing the learning of terrain traversability costs in robotics. These methods address the challenge of adapting learned models across different robot types and environments, potentially reducing the time and resources required for training.Zelinka et al. proposed a transfer learning approach based on convolutional neural networks to create cost assessment models for individual robots  (137, Zelinka et al., 2021). Their results indicate that terrain cost assessment can be improved through transfer learning, demonstrating the viability of sharing traversability knowledge between different robotic platforms."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should describe the main challenges in learning terrain traversability costs.",
            "weight": 0.04615384615384615,
            "evidence": [
              "There is a significant growth in autonomy level in off-road ground vehicles. However, unknown off-road environ-ments are often challenging due to their unstructured and roughnature. To find a path that the robot can move smoothly toits destination, it needs to analyse the surrounding terrain."
            ]
          }
        ]
      }
    },
    "case_id": "018bff91263ab3be5f8ad5bade76b030",
    "annotator": "Annotator 1 Assignments",
    "agreement": true
  },
  {
    "initial_prompt": "what are the most important open challenges in using neural networks in combination with PDE solvers for fluid simulation?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "what are the most important open challenges in using neural networks in combination with PDE solvers for fluid simulation?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should highlight the difficulty in numerically solving PDEs that arise in fluid simulation, and the increasing application of neural networks to approximate solutions to these PDEs.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Solving large complex partial differential equations (PDEs), such as those that arise in computational fluid dynamics (CFD), is a computationally expensive process. This has motivate the use of deep learning approaches to approximate the PDE solutions",
              "Convolutional neural networks (CNNs) have been shown to achieve a speedup of between two and four orders of magnitude over numerical flow solvers [40-42]."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should explain how DL methods are applied in this area and note the different DL models (CNNs, GNNs, PINNs) that have been used for approximating solutions to PDE problems.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Data-driven neural solvers learn from observations of fluid dynamics, usually from large datasets generated by numerical simulation, without (in general) any knowledge of the governing equations.",
              "The type of DL model employed for data-driven solvers primarily depends on the nature of the physical problem, with CNNs being a natural choice for simulations where the geometry aligns with the coordinate system, while GNNs are preferred for simulations of more complex geometries and Lagrangian systems.",
              "More recently,physics-informed neural networks(PINNs) have been developed to solve forward and inverse problems where full or partial knowledge of the governing equations is known [23,24,29,30,103,104]."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss the challenges of using neural networks for solving PDE problems such as large state space, lack of generalizability to unseen scenarios, computationally intensive training time, and the need for large training datasets.",
            "weight": 0.17142857142857143,
            "evidence": [
              "### 1. High-Dimensionality and Scale**Problem:** Fluid simulations often deal with high-dimensional spaces, particularly in three-dimensional simulations. Neural networks used for such tasks need to cater to vast input and output data scales.**Impact:** Handling high-dimensional input data while ensuring computational tractability remains a significant obstacle. The curse of dimensionality can lead to increased model complexity and training data requirements.",
              "### 2. Generalization and Extrapolation**Problem:** Neural networks often struggle to generalize beyond the training data and may fail to extrapolate well to unseen conditions, which is problematic for fluid simulations that often involve varied and complex domains.**Impact:** Ensuring reliable performance across different fluid flows, geometries, and boundary conditions remains unsolved",
              "### 3. Data Efficiency and Scarcity**Problem:** Generating the high-fidelity simulation data needed to train neural networks can be prohibitively expensive and time-consuming.**Impact:** NNs require large amounts of training data to generalize well. Data-efficient learning methods are critically needed to make the neural network approach feasible for fluid simulations.",
              "### 5. Interpretability and Physics Consistency**Problem:** Neural networks often function as black boxes, providing little insight into how they arrive at their solutions or maintaining physical constraints.**Impact:** Understanding and ensuring that neural network predictions adhere to the underlying physical laws described by the PDEs is crucial for their effective application in fluid simulations."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer could provide solutions to some of the discussed challenges such as training and using DL methods for a specific domain and not as an end-to-end solution, and innovative training methods.",
            "weight": 0.08571428571428572,
            "evidence": [
              "Addressing these generalization and adaptability challenges requires innovative approaches in model architecture, training strategies, and data generation. Researchers are exploring techniques such as physics-informed regularization, multi-fidelity training, and adaptive sampling to improve the robustness and transferability of neural network models in fluid simulations [LLM MEMORY | 2024]. However, significant work remains to be done to achieve the level of generalization and adaptability required for widespread adoption in complex fluid dynamics applications.",
              "As a consequence of these challenges, end-to-end flow solvers using DL are not yet reliable for most general-purpose simulations and long-term predictions. However, DL is proving a valuable component to help address some of the numerical challenges of conventional fluid dynamics simulators"
            ]
          }
        ]
      }
    },
    "case_id": "bb7198e650267504d67b14b6e4c67c7c",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What data preprocessing steps are most important for point cloud datasets before performing surface reconstruction?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What data preprocessing steps are most important for point cloud datasets before performing surface reconstruction?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define point cloud datasets.",
            "weight": 0.15,
            "evidence": [
              "With the invention of the iPhone 12 Pro's new Light Detection and Ranging (LiDAR) sensor, and the increased capability of the iPhone's camera array system, the generation of 3D point clouds using this sensor led to several studies on their capability and resulting accuracy for some applications in Architecture, Engineering and Construction (AEC) industries, especially in planning and decision-making purposes towards 3D reconstructed model. Generally, this 3D model is incorporated from different surface materials which have different results when scanning via laser scanning platform"
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention the importance of data preprocessing for point cloud datasets before performing surface reconstruction",
            "weight": 0.15,
            "evidence": [
              "If these raw data are directly used for surface reconstruction, the reconstructed surface will be not smooth or vulnerable, and the desired surface model will not be obtained. Moreover, the huge amount of data will put higher demands on the computer in terms of storage and display, resulting in inefficient data processing in the later stage. Therefore, under the premise of retaining the details of point cloud data and not affecting the accuracy of model reconstruction, point cloud reduction is required for massive data"
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should mentioned key data processing steps for point cloud datasets",
            "weight": 0.15,
            "evidence": [
              "Moreover, the main steps of point cloud processing are: 1. registration that combines multiple point clouds into one coordinate system; 2. segmentation that is the process of grouping similar points into homogeneous regions; 3. classification that classifies points into certain categories; and 4. object detection that recognizes objects in point clouds and localizes them [10].",
              "4.1 Outlier removalSome acquisition techniques generate points which are far away from the surface. These points, commonly referred to as \"outliers\", have no relevance for reconstruction.",
              "4.2 SimplificationSome laser scanners generate points with widely variable sampling. Typically, lines of scan are very densely sampled but the gap between two lines of scan is much larger, leading to an overly massive point cloud with large variations of sampling density. This type of input point cloud might generate imperfect output using algorithms which, in general, only handle small variations of sampling density.",
              "#### 2. **Noise Reduction**Noise in point cloud data can be caused by various factors such as sensor errors or environmental conditions. Noise reduction techniques, such as filtering, are essential to smooth out the point cloud and reduce the impact of noise on surface reconstruction. This can be achieved using techniques like least-square fitting of ellipses.",
              "#### 3. **Data Normalization**Normalizing the point cloud data is crucial to ensure that all points are on the same scale. This step helps in preventing features with large ranges from dominating the reconstruction process. Normalization can be done by scaling the point cloud to a common range, such as between 0 and 1.",
              "## 9. Hole FillingIncomplete data often results in holes in the point cloud, which need to be addressed for a seamless reconstruction.**Inpainting Techniques**: Predict and fill missing areas based on surrounding geometry.**Surface Interpolation**: Generates surfaces to bridge gaps between existing points.",
              "#### 4. **Feature Extraction**Feature extraction is a critical step in point cloud preprocessing. It involves identifying and extracting relevant features such as corners, edges, and creases from the point cloud. These features are essential for surface reconstruction algorithms to accurately reconstruct the surface. Techniques like Delaunay filtering can be used for feature extraction.",
              "#### 7. **Data Loading and Preparation**Finally, the preprocessed point cloud data needs to be loaded and prepared for surface reconstruction. This involves loading the data into the desired format and preparing it for the specific surface reconstruction algorithm being used. Tools like `dataloader.py` can be used to load visibility-augmented point clouds from `scan.npz` files."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should mention point cloud sampling can help in reducing the input size in the preprocessing step",
            "weight": 0.15,
            "evidence": [
              "In this paper, we proposed a point cloud sampling method to improve the scalability of surface reconstruction with neural fields. We introduced a seed point that serves as the origin to encourage the sampling network to sample point clouds from partial regions of the scene. Our split-and-merge approach allowed the construction of sampled point clouds while suppressing the input size to the sampling network."
            ]
          }
        ]
      }
    },
    "case_id": "11e71107ddfdc824b8b87d4f5a2ef843",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are advantages and disadvantages of top methods for picking the right number of topics in topic modeling?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are advantages and disadvantages of top methods for picking the right number of topics in topic modeling?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define topic modeling and its applications.",
            "weight": 0.13333333333333333,
            "evidence": [
              "Topic modeling is a crucial technique in natural language processing (NLP) that helps identify underlying themes or topics within a collection of documents.",
              "In addition to topic models' effective application to traditional problems like information retrieval, visualization, statistical inference, multilingual modeling, and linguistic understanding, Applications of Topic Models also reviews topic models' ability to unlock large text collections for qualitative analysis. It reviews their successful use by researchers to help understand fiction, non-fiction, scientific publications, and political texts."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should highlight the importance of picking the right number of topics in the performance of topic models.",
            "weight": 0.13333333333333333,
            "evidence": [
              "Researchers consistently face the difficulty of deciding on the optimal number of topics in advance, which can significantly impact the effectiveness of their analyses  (7, Rahman et al., 2018). The importance of this selection process is underscored by its direct relationship to the overall performance and utility of topic models  (22, Ruiz et al., 2023). Moreover, the choice of topic number is intrinsically linked to the evaluation of topic model quality, making it a longstanding challenge that requires careful consideration and robust methodologies  (23, Pan et al., 2023). Given its critical role in shaping the outcomes of topic modeling, selecting the right number of topics remains a key focus for researchers seeking to improve the accuracy and reliability of their models."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should mention the most important methods for topic modeling, such as elbow method, coherence metrics, and likelihood, together with their advantages and disadvantages.",
            "weight": 0.13333333333333333,
            "evidence": [
              "### 4. Elbow Method**Advantages:**- **Simplicity:** The elbow method is easy to visualize and interpret, using a plot to identify the \"elbow point\" where adding more topics yields diminishing returns in model performance metrics.- **Broad Applicability:** It can be used with various performance metrics like coherence or perplexity.**Disadvantages:**- **Ambiguity:** The \"elbow point\" may not always be clear-cut, leading to ambiguous decisions.- **Over-reliance on Visual Inspection:** The method relies heavily on visual interpretation, which can introduce subjectivity.",
              "### 2. Coherence Metrics**Advantages:**- **Human Interpretability:** Coherence metrics, such as UMass, UCI, and NPMI, focus on the semantic similarity of words within topics, aligning well with human judgment.- **Model Quality:** High coherence often correlates with more interpretable and meaningful topics.**Disadvantages:**- **Computational Cost:** Calculating coherence metrics, especially on large datasets, can be computationally intensive.- **Dependency on External Resources:** Some coherence metrics (e.g., UCI and NPMI) require access to external corpora or search engines.",
              "### 1. Perplexity and Likelihood**Advantages:**- **Quantitative Measure:** Provides a quantitative way to compare models, helping to identify the model that best generalizes to new data.- **Ease of Implementation:** Many computational packages for LDA and other methods have built-in functions to calculate perplexity and likelihood. **Disadvantages:**- **Overfitting:** Lower perplexity does not always imply a better model. It might indicate overfitting to the training data.- **Interpretability:** Perplexity and likelihood alone do not provide qualitative insights into topic coherence or interpretability.",
              "### 5. Cross-Validation**Advantages:**- **Robustness:** Provides a robust way to estimate the generalizability of the model by training and testing across multiple subsets of the data.- **Comprehensive:** Can be combined with other metrics like coherence to balance quantitative and qualitative performance.**Disadvantages:**- **Computationally Intensive:** Cross-validation can be very time-consuming, especially with large datasets and complex models.- **Implementation Complexity:** Requires setting up and running multiple models, which can be technically challenging."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should conclude that the choice of the method depends on different factors such as computational capability, the intended application, and maybe a mix of various methods would be required.",
            "weight": 0.13333333333333333,
            "evidence": [
              "In practice, a combination of these methods is often used to make a well-informed decision. For example, one might use perplexity or coherence to narrow down a range of potential topic numbers, then use stability analysis or manual inspection to make a final decision. It's also worth noting that the \"right\" number of topics can be subjective and may vary depending on the intended use of the model. Therefore, domain expertise and the specific goals of the analysis should always be considered alongside these quantitative methods."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer could name LDA as one of the most important methods for topic modeling.",
            "weight": 0.06666666666666667,
            "evidence": [
              "Latent Dirichlet Allocation (LDA), as it is the most used [6,12,17,20,32], state-of-the-art method [25] and simplest method [8].",
              "The topic modelling method LDA is an unsupervised, probabilistic modelling method which extracts topics from a collection of papers."
            ]
          }
        ]
      }
    },
    "case_id": "ce433b751f38f7d0173095c2faa2f75a",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What is the known evidence for pervasiveness of gaming of citation metrics like citation count and h-index?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What is the known evidence for pervasiveness of gaming of citation metrics like citation count and h-index?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define citation metrics and their significance in academic evaluation",
            "weight": 0.0923076923076923,
            "evidence": [
              "This article provides a broad overview of widely available measures of academic productivity and impact using publication data and highlights uses of these metrics for various purposes. Metrics based on publication data include measures such as number of publications, number of citations, the journal impact factor score, and the h-index, as well as emerging metrics based on document-level metrics. Publication metrics can be used for a variety of purposes for tenure and promotion, grant applications and renewal reports, benchmarking, recruiting efforts, and administrative purposes for departmental or university performance reports. The authors also highlight practical applications of measuring and reporting academic productivity and impact to emphasize and promote individual investigators, grant applications, or department output."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention evidences of manipulation in citation counts and h-indexusing self-citation",
            "weight": 0.0923076923076923,
            "evidence": [
              "The phenomenon of self-citation can present in many different forms, including direct, co-author, collaborative, and coercive induced self-citation. It can also pertain to the citation of single scientists, groups of scientists, journals, and institutions. This article presents some case studies of extreme self-citation practices. It also discusses the implications of different types of self-citation.",
              "However, misuse and gaming of metrics are rampant [2, 3]. The urge to \"publish or perish\" (or even \"get cited or perish\") creates an environment where gaming of metrics is amply incentivized. A whole generation of old and new tricks try to make CVs and their impact look good and impactful--better and more impactful than they really are. Many of these gaming tricks can reach extravagant levels, as in the case of paper mills, massive self-citations, or citation cartels [4,5,6].",
              "It has recently been suggested there may be increasing misrepresentation of research performance by individuals who self-cite inordinately to achieve scores and win rewards. In this paper we consider self-referencing and self-citing, describe the typical shape of self-citation patterns for carefully curated publication sets authored by 3517 Highly Cited Researchers and quantify the variance in the distribution of self-citation rates within and between all 21 Essential Science Indicators' fields."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should mention some evidences of citation cartels",
            "weight": 0.0923076923076923,
            "evidence": [
              "Citation cartels are groups of researchers who excessively cite each other's work to artificially inflate their citation counts and enhance their reputation. The practice of the citation cartel involves journals agreeing to cite each other's publications to boost their own impact factors. The citation cartel has been criticised for distorting the impact factors of participating journals and undermining the integrity of the scientific process. Citation cartels can take many forms, including reciprocal citing, where researchers agree to cite each other's work in exchange for citations. Citation cartels often involve a small group of researchers who are closely connected and who may be deliberately hiding their activities.",
              "This paper is an attempt to study a well known (probably little studied) phenomenon in academia: citation cartels. This is the tacit or explicit agreement among authors to cite each other more often than they would do in a more \"sincere\" approach to science. It can be intended as collusion and it can distort scientific progress in affecting a scholar's attention. The phenomenon has been around for decades and it does not seem to spare any discipline. By starting from outlining the characteristics of a \"cartel,\" this study then builds an agent-based model in an attempt to define the extent to which colluding behavior affects progress in a given discipline by operating on citation counts."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should mention some evidences of encouraging citation inflation by pressuring authors to cite articles published in their journal",
            "weight": 0.0923076923076923,
            "evidence": [
              "Among studies that have analyzed INAP, some have focused their attention on specific disciplinary contexts, measuring, for instance, the growth in the average number of authors in a specific discipline or among certain journals (Kapoor et al., 2015; Papatheodorou et al., 2008; Tilak et al., 2015). Whilst these studies have quantified INAP, they have not adequately analyzed the ethical implications of this phenomenon beyond the claim that it represents authorship inflation and could have resulted from increased pressure for publication as well as honorary and guest authorship practices."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should mention some evidences of manipulation by citation stacking",
            "weight": 0.0923076923076923,
            "evidence": [
              "## Manipulation by Citation StackingCitation stacking happens when authors cite other works in a less direct but still manipulative manner to inflate metrics artificially:1. **Bartneck and Kokkelmans (2011)** showed that strategically \"stacking\" citations to certain works can inflate metrics even when the cited works do not significantly contribute to the citing papers, effectively gaming the system [7].2. **Pan et al. (2018)** performed analyses showing how particular \"stacked\" citation sources are disproportionately targeted, likely aiming to boost specific authors or journals [8]."
            ]
          },
          {
            "name": "most_important_item_5",
            "criterion": "The answer should mention some evidences of honorary authorship",
            "weight": 0.0923076923076923,
            "evidence": [
              "Honorary authorship: Adding authors who contributed little or nothing to a paper can inflate individual citation metrics  (5, Chakraborty et al., 2020)."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should mention about debate and criticism on the use of citation metrics in research evaluation",
            "weight": 0.04615384615384615,
            "evidence": [
              "#### **Debate and Criticism**1. **Criticism of Metrics**: The use of citation metrics in research evaluation has been criticized for creating an environment where gaming is possible and for not accurately reflecting research quality.2. **Need for Responsible Metrics Use**: Scholars argue for a responsible use of metrics, with careful design to avoid unintended consequences, such as gaming."
            ]
          }
        ]
      }
    },
    "case_id": "08a151282687d2f0cd4f3d2c9c13d67d",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "How have open-source publishing platforms impacted the global distribution of academic publishing?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "How have open-source publishing platforms impacted the global distribution of academic publishing?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should provide a brief overview of open access and name arXiv as one of the most famous platforms for that.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Open Access (OA) refers to the removal of major obstacles to accessing, sharing and re-using the outputs of scholarly research.",
              "Open access (OA) publishing has dramatically altered the landscape of academic publishing, making research outputs freely available to readers worldwide (49, Bar-Anan et al., 2012). This model, which funds publishing through publication fees rather than reader subscriptions, has emerged as a cost-effective option for disseminating scholarly research  (21, Johnson, 2005).",
              "The best known of these is arXiv, in which researchers during its eleven year old history have deposited 225,000 publications from the fields of physics, mathematics, and information technology (arXiv)."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention some advantages of open access platforms that can affect the publication landscape, such as increased accessibility and reach, and localized publishing.",
            "weight": 0.19999999999999998,
            "evidence": [
              "### **Increased Accessibility and Reach**1. **Global Participation**: Open-source platforms have enabled researchers from diverse backgrounds and regions to publish their work, increasing global participation in academic publishing. For instance, the Public Knowledge Project's (PKP) open-source software has facilitated the publication of over 1.46 million articles in 60 languages from more than 146 countries, with a significant proportion from the Global South.2. **Language Diversity**: These platforms support multilingual publishing, allowing researchers to publish in their native languages, thereby promoting linguistic diversity and inclusivity. This is evident in the use of PKP's software, which supports publications in multiple languages.",
              "OA publishing breaks traditional financial barriers and allows unrestricted, equal access to scholarly information to people all over the globe. Due to the high prices of journal subscriptions, developing countries struggle with access just as in developed countries, but to a greater extent and consequently with greater negative repercussions.",
              "### **Bibliodiversity and Localized Publishing**1. **Decentralization**: Open-source platforms have decentralized publishing, allowing local institutions and communities to establish their own journals and publishing initiatives. This has promoted bibliodiversity, as local voices and perspectives are now more easily represented in the global scholarly landscape."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should argue that despite these advantages, OA can not guarantee even distribution of publications due to the author-pays model. In addition, the author-pays model can decrease the quality of publications.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Critics argue that the author-pays model may incentivize quantity over quality and create new barriers for researchers from less-funded institutions or developing countries."
            ]
          }
        ]
      }
    },
    "case_id": "2a84ae2f6196e574f7b9a9121187a37b",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "How do large language models like ChatGPT impact the diversity of published scientific ideas?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "How do large language models like ChatGPT impact the diversity of published scientific ideas?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define large language models like ChatGPT and their role in scientific publishing",
            "weight": 0.12,
            "evidence": [
              "Since the release of ChatGPT in November 2022 [1], academia has expressed divergent opinions about the use of this technology. This artificial intelligence (AI)-based chatbot interacts with users in a conversational way, using human-like language to answer questions and generate content. It is also trained to create computer code. ChatGPT tracks previous prompts and responses, correcting and adapting subsequent answers given the sequence of inputs and outputs. ChatGPT is powered by a LLM, a type of deep learning model that emerged around 2018 [2]. These models are trained on massive amounts of publicly available text data, such as books, articles, and webpages, to generate human-like responses in conversations. Academia faces a technological evolution driven by ChatGPT and LLMs. On the one hand, the potential of ChatGPT and LLMs in education and research is exciting.",
              "ChatGPT is one such technology that has shown promising prospects in academic research. ChatGPT is a large language model (LLM) that has been trained on an extensive corpus of text, enabling it to generate human-like text responses. For a few years now, it has been evident that AI can produce coherent language, and it is becoming increasingly challenging to distinguish AI sentences from those created by humans. In 2022, the journal Nature reported that scientists were already using chatbots as research assistants to help them organize their thoughts, receive feedback on their work, write codes, and even summarize research literature [2].ChatGPT has the ability to create well-written student essays, summarize research papers, answer questions well enough to pass medical exams, and generate helpful computer codes, for instance [2]. It has even created research abstracts that scientists found difficult to distinguish from those written by a human [2]."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention the amplification of existing biases as one of the impacts of LLMs on published scientific ideas.",
            "weight": 0.12,
            "evidence": [
              "These harms are forms of \"social bias,\" a subjective and normative term we broadly use to refer to disparate treatment or outcomes between social groups that arise from historical and structural power asymmetries, which we define and discuss in Section 2.1 Though LLMs often reflect existing biases, they can amplify these biases too; in either case, the automated reproduction of injustice can reinforce systems of inequity (Benjamin, 2020). From negative sentiment and toxicity directed towards some social groups, to stereotypical linguistic associations, to lack of recognition of certain language dialects, the presence of biases of LLMs have been well-documented",
              "The choice of whichuse casesto prioritize or the design of user interfaces can also contribute to biases in large language models. For example, if a language model is primarily designed to generate content for a certain demographic or industry, it may inadvertently reinforce existing biases and exclude different perspectives (Benjamin, 2019; Kleinberg,et al., 2016). Lastly,policy decisionscan play a role in the manifestation of biases in language models."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should mention ethical issues as one of the impacts of LLMs on published scientific ideas.",
            "weight": 0.12,
            "evidence": [
              "Debates on legal and ethical concerns of using AI for medicine and healthcare have been carried out continuously in recent years [129]. The widespread interests in ChatGPT recently also raised many concerns on legal and ethical issues regarding the use of LLMs like ChatGPT in medical research and practices [130, 131]. It was advocated to establish a robust legal framework encompassing transparency, equity, privacy and accountability. Such framework can ensure safe development, validation, deployment and continuous monitoring of LLMs, while taking into account limitations and risks [132].",
              "The use of LLMs in scientific research raises important ethical considerations that can impact the diversity of published ideas.5.1 Attribution and AuthorshipQuestions about how to attribute ideas generated or significantly influenced by LLMs may arise, potentially affecting how diverse contributions are recognized in scientific publishing [9].5.2 Transparency in AI UseThe scientific community will need to develop standards for transparently reporting the use of LLMs in research, which could impact how diverse ideas are perceived and evaluated [10].",
              "#### 2. Risk of Plagiarism and Redundancy: The ease with which LLMs generate text may lead to unintentional plagiarism or the rephrasing of existing ideas without significant novelty. This creates a flood of similar studies that can overwhelm truly novel contributions.**Example:** GPT-3 has been shown to produce text that, while coherent and contextually appropriate, may closely mirror its training data, increasing the risk of redundancy (Brown et al., 2020)."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should mention the LLMs\u2019 Impact on creativity and originality.",
            "weight": 0.12,
            "evidence": [
              "4. Impact on Creativity and OriginalityThe use of LLMs in scientific research raises questions about creativity and originality in idea generation.4.1 Augmenting Human CreativityLLMs can serve as powerful tools to augment human creativity, providing researchers with new perspectives and ideas to build upon [7].4.2 Risk of HomogenizationThere is a potential risk that overreliance on LLMs could lead to a homogenization of ideas, as researchers may gravitate towards suggestions provided by these models rather than pursuing more unconventional thoughts [8]."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should also mention some advantages of using LLMs on published scientific ideas such as work efficiency, acceleration of Idea generation, and rapid literature review",
            "weight": 0.12,
            "evidence": [
              "The introduction of large language models (LLMs) that allow iterative \"chat\" in late 2022 is a paradigm shift that enables generation of text often indistinguishable from that written by humans. LLM-based chatbots have immense potential to improve academic work efficiency, but the ethical implications of their fair use and inherent bias must be considered.",
              "3. Acceleration of Idea Generation and SynthesisLLMs can significantly speed up the process of idea generation and synthesis in scientific research.",
              "3.1 Rapid Literature ReviewLLMs can quickly summarize and synthesize large volumes of scientific literature, potentially leading to the discovery of new connections and ideas that human researchers might overlook [5]."
            ]
          }
        ]
      }
    },
    "case_id": "ab3651c422a54f40aa4bac3fa630c13e",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are good practices for detecting AI-generated texts in situations where false positives are extremely expensive?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are good practices for detecting AI-generated texts in situations where false positives are extremely expensive?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly discuss how AI detectors work.",
            "weight": 0.13333333333333333,
            "evidence": [
              "The most common approach is to train a classification model on samples of text that are known to be AI-written or human-written. One example of this approach is OpenAI's AI classifier, which is a LLM fine tuned to perform classification. Another example is the popular tool GPTZero, which is a logistic regression model that uses the text's perplexity and burstiness as input features. (Perplexity is a measure of how \"surprising\" or improbable a certain sequence of words is, while burstiness is a measure of variation in the length and structure of sentences.)"
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should briefly discuss that the existing AI detector tools are not reliable and robust and can lead to high false positives and false negatives based on empirical results.",
            "weight": 0.13333333333333333,
            "evidence": [
              "Detection tools for AI-generated text do fail, they are neither accurate nor reliable (all scored below 80% of accuracy and only 5 over 70%). In general, they have been found to diagnose human-written documents as AI-generated (false positives) and often diagnose AI-generated texts as human-written (false negatives). Our findings are consistent with previously published studies (Gao et al.2022; Anderson et al.2023; Elkhatat et al.2023; Demers2023; Gewirtz2023; Krishna et al.2023; Pegoraro et al.2023; van Oijen2023; Wang et al.2023) and substantially differ from what some detection tools for AI-generated text claim (Compilatio2023; Crossplag.com2023; GoWinston.ai2023; Zero GPT2023).",
              "They are neither robust, since their performance worsens even more with the use of obfuscation techniques such as manual editing or machine paraphrasing, nor are they able to cope with texts translated from other languages.",
              "Originality.AI has a 99% accuracy rate for detecting GPT-4 generated content, but it still produces false positives approximately 2% of the time."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should briefly highlight consequences of false positives by AI detectors.",
            "weight": 0.13333333333333333,
            "evidence": [
              "Relying on inconsistent detectors can cause trouble, especially when there's a high chance of false positives. Professors using AI detectors in higher education has already led to false cheating accusations when tools likeTurnitinincorrectly flagged students' work as AI-generated.In the marketing world, accusing writers and agencies of using AI tools when they've written the content themselves can damage valuable working relationships."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should provide alternatives/remedies for detecting AI generated texts, such as having communication in the academic domain, using multiple tools, context aware analysis, including a human in the loop with explainable AI methods, and version history.",
            "weight": 0.13333333333333333,
            "evidence": [
              "Know before you go--make sure you consider the possibility of a false positive upfront and have a plan for what your process and approach will be for determining the outcome. Even better, communicate that to students so that you have a shared set of expectations.Assume positive intent--in this space of so much that is new and unknown, give students the strong benefit of the doubt. If the evidence is unclear, assume students will act with integrity.Be open and honest--it is important to acknowledge that there may be false positives upfront, so both the instructor and the student should be prepared to have an open and honest dialogue. If you don't acknowledge that a false positive may occur, it will lead to a far more defensive and confrontational interaction that could ultimately damage relationships with students.",
              "3. Context-Aware Analysis. Consider the context in which the text appears, as this can provide valuable clues about its origin.3.1 Metadata Analysis: Examine metadata associated with the text, such as - Creation time and data - Author information - Edit history- Source platform. Inconsistencies in metadata can be red flags for AI-generated content. 3.2 Contextual Relevance: Assess how well the text fits within its broader context, including:- Relevance to the topic or discussion - Consistency with the author's known style or expertise- Appropriateness for the intended audience. AI-generated text might struggle to maintain perfect contextual relevance, especially in nuanced situations.",
              "1. Multi-Model Approach. One of the most effective strategies is to use a multi-model approach for detection. This involves using multiple AI models or algorithms to analyze the text, each focusing on different aspects or using different techniques.",
              "5. Probabilistic Scoring and Threshold Adjustment. Instead of binary classification, use probabilistic scoring to indicate the likelihood of a text being AI-generated.5.1 Confidence Levels. Assign confidence levels to detections, allowing for more nuanced decision-making based on the specific context and risk tolerance.5.2 Adjustable Thresholds. Implement adjustable thresholds for flagging content as potentially AI-generated. In situations where false positives are extremely costly, these thresholds can be set higher to reduce false positive rates at the expense of potentially missing some AI-generated content.",
              "**Version History**: Utilize tools like Google Docs' Version History feature to track the writing process. This can help identify human-like writing behavior and reduce the likelihood of false positives.",
              "6. Explainable AI Techniques. Utilize explainable AI techniques to provide insights into why a particular text was flagged as potentially AI-generated.6.1 Feature ImportanceHighlight specific features or portions of the text that contributed most significantly to the detection decision.6.2 Comparison Benchmarks: Provide comparisons to known human-written and AI-generated texts to illustrate why a particular text was flagged. These explanations can aid human reviewers in making more informed decisions and reduce the likelihood of false positives being accepted without scrutiny."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer could mention some of the most famous AI detection tools.",
            "weight": 0.06666666666666667,
            "evidence": [
              "These can also be used to check for quality or plagiarism issues and to spot-check if you're likely getting human writing (not AI drafts) from employees,freelancers, andagency partners. Some of the most popular AI detectors includeOriginality AI,Content at Scale, andCopyleaks.",
              "The research covers 12 publicly available tools and two commercial systems (Turnitin and PlagiarismCheck)"
            ]
          }
        ]
      }
    },
    "case_id": "6f526f72804ce3eb59feb7046f319ccc",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "How does in-context learning for LLMs differ from traditional machine learning model training?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "How does in-context learning for LLMs differ from traditional machine learning model training?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define in-context learning and traditional machine learning training.",
            "weight": 0.13333333333333333,
            "evidence": [
              "With the increasing scale of these LLMs, in-context learning (ICL) has emerged as a striking property of LLMs [5, 6]. Unlike learning methods that require updating model parameters, in-context learning allows for good model performance with a prompt that only includes natural language instructions and/or a few demonstration examples",
              "Brown et al. (2020) have shown that Large Language Models (LLMs) (Radford et al., 2019; Chowdhery et al., 2022; Hoffmann et al., 2022; Zhang et al., 2022a) can perform so-called in-context learning (ICL) of supervised tasks. In contrast to standard in-weights learning, e.g. gradient-based finetuning of model parameters, ICL requires no parameter updates. Instead, examples of the input- label relationship of the downstream task are simply prepended to the query for which the LLM predicts. This is sometimes also referred to as few-shot ICL to differentiate from other ICL variants that do not use example demonstrations (Liu et al., 2023). Few-shot ICL is widely used, e.g. in all LLM publications cited above, to improve predictions across a variety of established NLP tasks, such as sentiment or document classification, question answering, or natural language inference.",
              "Transformer-based [ 41] pre-trained large language models (LLMs) have demonstrated significant advancements in a variety of natural language processing (NLP) tasks. As the size of these LLMs increases, they gain \"in-context learning\" capabilities, whereby the models achieve state-of-the-art (SOTA) or near-SOTA performance by conditioning on a small number of demonstration examples at inference time, without any need for updating model parameters [4 ]."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should highlight that in-context learning requires significantly less data compared to traditional machine learning.",
            "weight": 0.13333333333333333,
            "evidence": [
              "In-context learning (ICL)  (Brown et al., 2020) is an emerging learning paradigm that allows LLMs to perform tasks with few-shot examples, without requiring any updates to the model parameters. This approach stands in stark contrast to traditional machine learning, where models are typically trained on large datasets of labeled examples  (Devlin et al., 2019). In-context learning offers a significant advantage in domains where labeled data is scarce or expensive to obtain, as it greatly reduces the amount of required labeled data."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should emphasize that in-context learning does not involve updating the model's parameters compared to traditional machine learning models",
            "weight": 0.13333333333333333,
            "evidence": [
              "As the size of these LLMs increases, they gain \"in-context learning\" capabilities, whereby the models achieve state-of-the-art (SOTA) or near-SOTA performance by conditioning on a small number of demonstration examples at inference time, without any need for updating model parameters [4].",
              "In-context learning is a surprising and important phenomenon that emerged when modern language models were scaled to billions of learned parameters. Without modifying a large language model's weights, it can be tuned to perform various downstream natural language tasks simply by including concatenated training examples of these tasks in its input."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should mention task adaptation, and model size dependence features of ICLs compared to traditional machine learning models",
            "weight": 0.13333333333333333,
            "evidence": [
              "3. Task adaptation: LLMs can quickly adapt to novel tasks by conditioning on a prompt containing relevant task examples  (41, Singh et al., 2023)  (79, Cachola et al., 2023).4. Emergent behavior: ICL is considered an emergent property of LLMs, arising from their pretraining on vast amounts of data, though the exact mechanisms are not fully understood  (23, Wang et al., 2023).5. Model size dependence: The effectiveness of ICL tends to improve with larger model sizes and more extensive training data  (94, Tang et al., 2024)."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should include some applications of ICLs",
            "weight": 0.06666666666666667,
            "evidence": [
              "### **Applications**1. **Natural Language Processing**: In-context learning excels in various natural language processing tasks, such as sentiment analysis, text classification, machine translation, question-answering systems, text summarization, content generation, conversational AI, and knowledge extraction.2. **Real-World Scenarios**: Its ability to operate without extensive training data and parameter updates makes it suitable for real-world applications where data is limited or diverse."
            ]
          }
        ]
      }
    },
    "case_id": "803bc7891917f823a52948ebee89cf9d",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "Describe what is known about overfitting in in-context learning.",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "Describe what is known about overfitting in in-context learning.",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define overfitting in the context of traditional machine learning and contrast it with overfitting in in-context learning",
            "weight": 0.24,
            "evidence": [
              "When machine learning algorithms are constructed, they leverage a sample dataset to train the model. However, when the model trains for too long on sample data or when the model is too complex, it can start to learn the \"noise,\" or irrelevant information, within the dataset. When the model memorizes the noise and fits too closely to the training set, the model becomes \"overfitted,\" and it is unable to generalize well to new data. If a model cannot generalize well to new data, then it will not be able to perform the classification or prediction tasks that it was intended for.",
              "### Overfitting in In-Context LearningIn the realm of in-context learning, overfitting takes on a different form. Since in-context learning does not involve updating the model parameters, the risk of overfitting pertains to the model's ability to appropriately use the context provided. Overfitting in in-context learning can arise in several ways:1. **Contextually-Specific Patterns**: The model may learn to exploit specific patterns or keywords in the context examples that are not generalizable to broader instances. For example, it might latch onto the format or specific wording of examples provided in the input, giving an illusion of competence.2. **Mimicking Input Examples**: When given multiple exemplars in the input, the model might over-rely on their structure or content, thereby failing to generalize to new contexts that deviate slightly in format or phrasing. This mimicking behavior can limit the model's ability to adapt to varying input scenarios."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention methods such as increasing the diversity, number of in-context examples, and order randomization, as some ways to mitigate overfitting in in-context learning",
            "weight": 0.24,
            "evidence": [
              "To mitigate the risk of overfitting on the samples used during the refinements of the system and user prompts, we ensured that they were not included in the generation of inference test data. More specifically, we performed an initial investigation of zero-shot and random few-shot performance using an initial dataset comprising 30 random samples, collected exclusively from the CRC100K dataset; each containing either tumor or normal colon epithelium. This initial dataset served a dual purpose: developing effective prompts and providing an early insight into model responses. For the following evaluation phase, we collected a new subset of 30 samples. This way, we prevented sample leakage from our prompt creation dataset into our final evaluation testset. This was critical to prevent overfitting that could arise from sample-specific biases we might have included into the prompt.",
              "### Mitigating Overfitting in In-Context LearningMitigating overfitting in in-context learning involves carefully designing the input context:1. **Diverse Example Selection**: Providing varied examples that comprehensively represent the task can help the model generalize better. Avoiding repetition and ensuring variability in the context can reduce the chances of overfitting.2. **Contextual Regularization**: Techniques analogous to regularization in traditional ML might be adapted for context inputs. For instance, introducing controlled noise or variability in examples, or using dropout-like methods on context tokens, might enhance generalization.3. **Order Randomization**: Randomizing the order of examples in the context in multiple runs can potentially reduce the model's sensitivity to specific sequential patterns, thereby promoting robustness."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should mention that in-context learning reduces the risk of overfitting by avoiding model parameter updates and restricting learning from limited fine-tuning datasets",
            "weight": 0.12,
            "evidence": [
              "In-context learning was primarily proposed as an alternative to fine-tuning a pre-trained language model on a task-specific dataset as it offers several advantages over it. Unlike fine-tuning, in-context learning does not involve updating the model parameters, which means that the model itself does not learn anything new. In-context learning, however, employs prompts to prime the model for subsequent inference within a specific conversation or context. This approach has two main benefits: it requires less task-specific data and reduces the risk of overfitting by avoiding narrow learning from a limited fine-tuning dataset. In fact, in-context learning enables large language models to demonstrate competitive few-shot performance on a variety of natural language processing (NLP) tasks, including translation and question-answering."
            ]
          }
        ]
      }
    },
    "case_id": "928646729f5d824dd9a52e9ddff70e58",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "How good are LLMs at solving traditional tabular ML datasets using ICL?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "How good are LLMs at solving traditional tabular ML datasets using ICL?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define In-Context Learning (ICL) in the context of LLMs and describe how tabular data is used in traditional machine learning tasks.",
            "weight": 0.15,
            "evidence": [
              "Large language models (LLMs) have an impressive ability to perform in-context learning (ICL) [1]. Conditioned on natural language prompts containing question-answer pairs (called demonstrations or exemplars), LLMs can execute natural language inference tasks in new domains without having to update the model's parameters while achieving similar downstream performance as full model fine-tuning [2], [3]. The inference tasks can range from simple fact-retrieval [1] to complex reasoning and problem-solving [4] and are adaptable across a wide range of domains. The ease of usage and cost benefit has motivated several organizations to integrate LLMs into their operations and services to supplement their private data with knowledge from the large corpus of texts that LLMs are trained on.",
              "Tabular data stands as one of the pervasive and essential data formats in machine learning (ML), with widespread applications across diverse domains such as finance, medicine, business, agriculture, education, and other sectors that heavily rely on relational databases (Sahakyan et al., 2021; Rundo et al., 2019; Hernandez et al., 2022; Umer et al., 2019; Luan & Tsai, 2021). Tabular data, commonly known as structured data, refers to data organized into rows and columns, where each column represents a specific feature. In this section, we first introduce the characteristics of tabular data, then provide a brief review of traditional, deep-learning and LLM methods tailored for this area. At last, we articulate the contribution of the paper and provide a layout of the following sections."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention the challenges LLMs face when applying ICL to tabular ML datasets",
            "weight": 0.15,
            "evidence": [
              "Heterogeneity: Tabular data can contain different feature types: categorical, numerical, binary, and textual. Therefore, features can range from being dense numerical features to sparse or high-cardinality categorical features (Borisov et al., 2022).",
              "Context-based interconnection: In tabular data, features can be correlated. For example, age, education, and alcohol consumption from a demographic table are interconnected: it is hard to get a doctoral degree at a young age, and there is a minimum legal drinking age. Including correlated regressors in regressions lead to biased coefficients, hence, a modeler must be aware of such intricacies (Liu et al., 2023d).",
              "Order invariant: In tabular data, examples can be sorted. However, as opposed to text-based and image-based data that is intrinsically tied to the position of the word/token or pixel in the text or image, tabular examples are relatively order-invariant. Therefore, position-based methodologies (e.g., spatial correlation, impeding inductive bias, convolutional neural networks (CNN)) are less applicable for tabular data modeling (Borisov et al., 2022).",
              "Firstly, some models have short context lengths (E.g. Flan-UL2 (Tay et al., 2023b) supports 2048 tokens, Llama 2 (Touvron et al., 2023b) supports 4096 context tokens) and even models that support large context lengths might still be insufficient if the table is over say 200K rows (Claude 2.1 supports up to 200K tokens).",
              "Secondly, even if the table could fit the context length, most LLMs are inefficient in dealing with long sentences due to the quadratic complexity of self-attention (Sui et al., 2023b; Tay et al., 2023a; Vaswani et al., 2017). When dealing with long contexts, performance of LLMs significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models (Liu et al., 2023b). For tabular data, Cheng et al. (2023); Sui et al. (2023c) highlights that noisy information becomes an issue in large tables for LMs. Chen (2023) found that for table sizes beyond 1000 tokens, GPT-3's performance degrades to random guesses.",
              "Thirdly, longer prompts incur higher costs, especially for applications built upon LLM APIs."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss how LLMs can be adapted to handle tabular data using ICL",
            "weight": 0.15,
            "evidence": [
              "We discuss common techniques, like serialization, table manipulations, prompt engineering, and building end-to-end systems in this section.",
              "Recent advances in foundation models and natural language processing (NLP) domains have encouraged researchers to transform or serialize tabular data into natural language formats thus enabling ICL to be performed with tabular data. Authors in [6] empirically verified nine different methods of serializing tabular data into human-readable strings for use in tabular data classification with LLMs such as manual template-based conversions, table-to-text methods using finetuned LLMs, and LLM inference-based methods."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should compare the performance of LLMs on tabular ML datasets using ICL in comparison to traditional ML models",
            "weight": 0.15,
            "evidence": [
              "We formulate two private ICL frameworks with provable privacy guarantees in both the local (LDP-TabICL) and global (GDP-TabICL) DP scenarios via injecting noise into individual records or group statistics, respectively. We evaluate our DP-based frameworks on eight real-world tabular datasets and across multiple ICL and DP settings. Our evaluations show that DP-based ICL can protect the privacy of the underlying tabular data while achieving comparable performance to nonLLM baselines, especially under high privacy regimes.",
              "Using TabPFN as the base model - currently the best tabular in-context learner - and applying our retrieval and fine-tuning scheme on top results in what we call a locally-calibrated PFN, or LoCalPFN. We conduct extensive evaluation on 95 datasets curated by TabZilla from OpenML, upon which we establish a new state-of-the-art with LoCalPFN - even with respect to tuned tree-based models. Notably, we show a significant boost in performance compared to the base in-context model, demonstrating the efficacy of our approach and advancing the frontier of deep learning in tabular data.",
              "### Strengths1. **Flexibility**:LLMs can be very flexible when dealing with diverse kinds of input formats. This flexibility allows them to perform reasonably well on a range of tasks without the need for retraining.2. **Semantic Understanding**:Due to their training on vast amounts of data, LLMs capture intricate semantic relationships that traditional ML models might miss unless specifically engineered to do so.### Weaknesses1. **Scalability**:LLMs can be computationally expensive, especially when dealing with large datasets. Specialized models are usually more efficient both in terms of computation and memory, which is crucial for real-world applications.2. **Accuracy**:For tabular datasets, models like XGBoost or CatBoost are typically more accurate because they are designed to handle the specific nuisances of tabular data (e.g., handling missing values, capturing non-linear relationships).",
              "Recent research has shown that LLMs can perform surprisingly well on tabular ML tasks using ICL: a) Classification Tasks: LLMs have demonstrated competitive performance on various classification problems, sometimes matching or surpassing traditional ML models [3]. b) Regression Tasks: While generally performing well, LLMs may struggle with precise numerical predictions compared to specialized regression models [4]. c) Time Series Forecasting: LLMs have shown promise in time series tasks, especially when temporal patterns can be effectively communicated through text [5].",
              "In conclusion, LLMs have demonstrated significant potential in handling tabular data tasks, particularly in few-shot and zero-shot scenarios. Their ability to perform competitively with traditional machine learning methods, especially when limited training data is available, makes them a promising tool for researchers and practitioners working with tabular datasets.",
              "In conclusion, while LLMs have shown promise in competing with traditional ML methods for tabular data tasks, especially in few-shot and zero-shot scenarios, their performance can vary depending on the specific task, dataset, and amount of available training data. As the field continues to evolve, further research is needed to fully understand the strengths and limitations of LLMs compared to traditional methods across a wide range of tabular data applications."
            ]
          }
        ]
      }
    },
    "case_id": "1f384c4d3942c46b676b5bfc66447192",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What insights can be obtained about developer behavior from their GitHub commits and pull requests?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What insights can be obtained about developer behavior from their GitHub commits and pull requests?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should highlight that GitHub and websites like StackOverflow include a valuable amount of information that can be mined to extract developer\u2019s behavior.",
            "weight": 0.12,
            "evidence": [
              "Nowadays, software developers are increasingly involved in GitHub and StackOverflow, creating a lot of valuable data in the two communities. Researchers mine the information in these software communities to understand developer behaviors"
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should define commits and pull requests in Github briefly.",
            "weight": 0.12,
            "evidence": [
              "Pull requests let you tell others about changes you've pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch.",
              "You can commit changes on a pull request branch that was created from a fork of your repository with permission from the pull request creator."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss different insights that can be extracted from Github analytics regarding developer behavior, such as work patterns, collaboration habits, code quality, expertise, performance, sentiment, technical and social skills, activity, etc.",
            "weight": 0.12,
            "evidence": [
              "Performance monitoring: By tracking how and when code is being contributed, tech leads can monitor a team's performance and ensure that the projects are on track.",
              "The projects were also mined for developer sentiment. Finally, a regression model shows how sentiment varies with behavioral differences -- a change in behavior is correlated with a change in sentiment.",
              "Saxena et. al. annotate GitHub code with tags of Stack Overflow and created detailed technology skill profile for developers based on code repository contributions [18].",
              "1. **Commit Frequency and Patterns**:- Analyzing commit frequency can help identify patterns of developer activity, such as peak hours or days of the week when most commits are made.- This information can be used to optimize collaboration and review processes.",
              "2. **Code Quality and Complexity**:- Commit comments and code reviews can provide insights into code quality and complexity, helping developers identify areas that need improvement.- This analysis can also help in setting coding standards and best practices.",
              "3. **Developer Experience and Efficiency**:- Commit history can be used to measure developer efficiency and experience, such as the time taken to resolve issues or complete tasks.- This information can help in identifying training needs or areas where developers need support.",
              "1. **Pull Request Success and Failure**:- Analyzing pull requests can help identify factors that contribute to their success or failure, such as the quality of code reviews or the experience of the developer.- This information can be used to improve the pull request process and reduce the number of unsuccessful pull requests.",
              "2. **Code Review Quality and Effectiveness**:- Pull request comments and reviews can provide insights into the quality and effectiveness of code reviews, helping identify areas where reviews need improvement.- This analysis can also help in setting standards for code reviews and ensuring that they are thorough and constructive.",
              "3. **Team Collaboration and Communication**:- Pull request discussions can provide insights into team collaboration and communication, helping identify areas where communication needs improvement.- This information can be used to optimize team workflows and ensure that all team members are aligned."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should discuss the application of acquiring these insights from GitHub data in project management and resource allocation.",
            "weight": 0.12,
            "evidence": [
              "By analyzing these artifacts, researchers and managers can gain valuable insights to improve software development processes, enhance collaboration, and drive project success."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer could mention some of the tools that can be used to do this data analysis.",
            "weight": 0.06,
            "evidence": [
              "This analysis can be done using various tools and techniques, including the OpenSearch User Behavior Insights plugin and GitHub Actions like Pull Request Analytics."
            ]
          },
          {
            "name": "nice_to_have_item_1",
            "criterion": "The answer could discuss that user profiles might differ across different repositories and different online sources should be considered to have an accurate measurement.",
            "weight": 0.06,
            "evidence": [
              "However, it is of great challenge to accurately profile the expertise of developers over the Internet as their activities often disperse across different online communities. In this regard, the existing works either merely concern a single community, or simply sum up the expertise in individual communities. The former suffers from low accuracy due to incomplete data, while the latter impractically assumes that developer expertise is completely independent and irrelavant across communities."
            ]
          }
        ]
      }
    },
    "case_id": "456d3c2c757f4b874abddb18a62f2ff8",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are the best practices to protect a software against vulnerabilities from third party libraries?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are the best practices to protect a software against vulnerabilities from third party libraries?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should emphasizethe severe implications that vulnerabilities in third-party libraries can have.",
            "weight": 0.24,
            "evidence": [
              "Vulnerabilities in third-party software modules have resulted in severe security flaws, including remote code execution and denial of service.",
              "Vulnerabilities in third-party libraries are among the top 10 security risks in web applications (see OWASP Top 10).",
              "Malicious attackers can exploit these vulnerabilities to infiltrate systems, execute unauthorized operations, or steal sensitive information, posing a severe threat to software security. Research on third-party libraries in software becomes paramount to address this growing security challenge."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should discuss best practices that can be used to prevent these implications, such as reliable source, update monitoring, code review, minimizing dependencies, sandboxing, security testing, and team training.",
            "weight": 0.24,
            "evidence": [
              "1. Vet Your Sources: When selecting third-party libraries, it's crucial to assess their source. Opt for libraries with a strong reputation in the development community. Evaluate the library's history: review its release frequency, responsiveness to issues, and community engagement in forums or platforms like GitHub.",
              "2. Stay Updated. Keeping libraries up-to-date is vital for security and functionality. Implement a routine to regularly update libraries to their latest versions, ideally through automated processes.",
              "3. Understand the Code. Incorporating a library with an understanding of its functionality can be safe. Take time to understand the purpose and implementation of the library.",
              "4. Minimize Dependencies. Adopt a minimalistic approach to dependencies. Scrutinize each library to determine if it's essential. Employ tools to detect and eliminate unused or redundant libraries, reducing the overall attack surface. This practice not only enhances security but also improves application performance and maintainability.",
              "Dependency Isolation. Minimize the risk posed by third-party libraries through isolation strategies:",
              "Security Reviews:Implement a structured process for regular security audits of your dependencies.",
              "Train Your Team: Finally, it is important to ensure that your development team is aware of the importance of keeping third-party libraries up-to-date and secure.",
              "## 3. Security Testing and Vulnerability Scanning. ### a. Static Analysis. Perform static analysis on third-party libraries to identify potential security vulnerabilities. Many static analysis tools can scan both your code and third-party dependencies for security issues.### b. Dynamic Analysis. Conduct dynamic analysis and runtime testing on your application, including its third-party components. Penetration testing and fuzz testing can help find vulnerabilities that might not be apparent from static analysis alone.## 4. Implementing Least Privilege and Access Controls### a. Least Privilege Principle. Follow the principle of least privilege when integrating third-party libraries. Ensure that the library only has the minimal permissions necessary to perform its function."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer could provide some examples of famous third-party libraries that can be used in different programming languages.",
            "weight": 0.12,
            "evidence": [
              "In the JavaScript language, the most popular third-party library ecosystem is npm (Node Package Manager). npm, as one of the essential tools in the JavaScript community, allows developers to install, publish, and manage JavaScript packages.",
              "The third-party library ecosystem in C/C++ is quite diverse, with no clearly dominant ecosystem but many options such as Conan, CMake, etc."
            ]
          }
        ]
      }
    },
    "case_id": "36d9a01cd11997e14deb22537f52f266",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "How do external events influence software projects? Explain with a few recent examples.",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "How do external events influence software projects? Explain with a few recent examples.",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should define events/risks and highlight the importance of risk management for project success.",
            "weight": 0.19999999999999998,
            "evidence": [
              "A risk factor is defined as a condition that can pose a serious threat to the successful completion of a software development project (March & Shapira 1987). The inefficiency in risk identification process in the development of complex systems is considered one of the main causes of project failures (Reeves et al. 2013)."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide some examples of external events that can affect software projects.",
            "weight": 0.19999999999999998,
            "evidence": [
              "IT projects are complex and dynamic, and they often face various external factors that can affect their success. These factors can be political, economic, social, technological, legal, or environmental, and they can have positive or negative impacts on the project objectives, scope, budget, schedule, quality, or risks."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide some recent examples of events that have changed software projects, such as COVID-19 (remote working, recession, healthcare focus), AI revolution (AI assistants and ethical considerations), and Russia-Ukraine War (cybersecurity focus and supply chain shortage).",
            "weight": 0.19999999999999998,
            "evidence": [
              "1. Global Pandemics: The COVID-19 ExampleThe COVID-19 pandemic has been one of the most impactful external events in recent history, affecting software projects across the globe.1.1 Remote Work AccelerationThe pandemic forced many companies to rapidly adopt remote work practices, leading to an increased demand for collaboration and communication software. For example:- Zoom experienced exponential growth, with daily meeting participants increasing from 10 million in December 2019 to 300 million by April 2020 [1].- Microsoft Teams saw a surge in development to add new features like Together Mode and Breakout Rooms to enhance remote collaboration [2].\u000b1.2 Healthcare Software PrioritizationThe pandemic also led to the rapid development and deployment of healthcare-related software:- Contact tracing apps were developed and launched in many countries, such as Germany's Corona-Warn-App [3].- Telemedicine platforms saw accelerated development and adoption, with companies like Teladoc Health experiencing significant growth [4].",
              "3. Technological Advancements: The AI RevolutionRecent advancements in artificial intelligence, particularly in large language models, have significantly influenced software projects:3.1 Integration of AI CapabilitiesMany software projects are now incorporating AI capabilities:- GitHub Copilot, an AI pair programmer, has influenced how developers write code and approach software development [7].- Companies across various industries are integrating ChatGPT and similar models into their products, such as Snapchat's My AI feature [8].3.2 AI Ethics and GovernanceThe rapid advancement of AI has also led to new projects focused on AI ethics and governance:- The development of AI auditing tools and frameworks has become a priority for many organizations [9].- Projects aimed at detecting AI-generated content have gained prominence, such as GPTZero [10].",
              "2. Geopolitical Events: The Russia-Ukraine ConflictThe ongoing conflict between Russia and Ukraine has had various impacts on software projects:2.1 Cybersecurity FocusThe conflict has heightened concerns about cybersecurity, leading to increased investment and development in this area:- Many organizations have accelerated their adoption of zero-trust security models [5].- There has been increased development of threat intelligence platforms to monitor and respond to potential cyber attacks.2.2 Supply Chain DisruptionsThe conflict has disrupted global supply chains, affecting hardware availability and indirectly impacting software projects:- Chip shortages have led to delays in projects requiring specialized hardware, such as IoT initiatives [6].- Some companies have had to redesign their software to work with alternative hardware components."
            ]
          }
        ]
      }
    },
    "case_id": "9b4f043c1561f830b2033f51ca87a648",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What kinds of tools and data sources are availabe to serve the information needs of developers when responding to critical vulnerabilities?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What kinds of tools and data sources are availabe to serve the information needs of developers when responding to critical vulnerabilities?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define what critical vulnerabilities are and why they are important to consider in software development.",
            "weight": 0.12,
            "evidence": [
              "Software vulnerabilities are weaknesses in source code that can be potentially exploited to cause loss or harm. While researchers have been devising a number of methods to deal with vulnerabilities, there is still a noticeable lack of knowledge on their software engineering life cycle, for example how vulnerabilities are introduced and removed by developers.",
              "A software vulnerability is a security flaw, glitch, or weakness found in software code that could be exploited by an attacker [ 23 ] to cause harm to the stakeholders of a software system [ 28 ]. To avoid vulnerabilities in software systems, organizations are shifting security \"left,\" that is, to earlier stages of software development, such as during code review [1]. Code review is a widely agreed-on practice [ 12] recognized as a valuable tool for reducing software defects and improving the quality of software projects [2, 3, 8 ]."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention automated vulnerability scanning tools which help developers identify vulnerabilities in their codebase, dependencies, and infrastructure.",
            "weight": 0.12,
            "evidence": [
              "Note that embedding security into an application development lifecycle (DLC) encompasses a set of different techniques [11] and assessments at different stages, e.g. Static Application Security Testing (SAST) [12] at an early stage of DLC, and Dynamic Application Security Testing (DAST) [13] at testing and operation stages. SAST scans source code like a white box testing from the inside out, while DAST implements black box testing of the runtime behavior while executing it from the outside in. Comprehensive application security solutions are highly desirable to maximise the coverage of ever-evolving cyberattacks.",
              "2.3 Software Composition Analysis (SCA) Tools SCA tools help identify and manage vulnerabilities in third-party libraries and open-source components. Examples include Snyk, WhiteSource, and Black Duck [5]."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should mention vulnerability databases and repositories which are essential sources of information for developers to stay updated on known vulnerabilities and emerging threats.",
            "weight": 0.12,
            "evidence": [
              "The Common Vulnerabilities and Exposures (CVEs) is a repository of publicly known cybersecurity vulnerabilities. This repository can be used in many ways including vulnerability assessment, intrusion detection, security information management, etc. Mining this repository also help us understand the trend of vulnerabilities and exposures.",
              "Community. The best possible community support seems to come from CVE while the second best is OWASP Ten. In rest of the efforts, the community is smaller and sometimes confined in the form of a single organisation.",
              "We must appraise the existence of certain organisations, i.e., NIST and OWASP, that promote in the best possible way security software. NIST focuses more on developing standards aiming to bridge interoperability issues, while OWASP focuses on promoting innovative tools, creating tool benchmarks plus classifying both tools and vulnerabilities.",
              "We present an in-depth case study by comparing the analysis reports of 9 industry-leading SCA tools on a large web application, OpenMRS, composed of Maven (Java) and npm (JavaScript) projects.",
              "1.1 National Vulnerability Database (NVD)The National Vulnerability Database, maintained by the National Institute of Standards and Technology (NIST), is a comprehensive repository of vulnerability information [1]. It provides standardized vulnerability data, including Common Vulnerabilities and Exposures (CVE) identifiers, severity scores, and affected software versions.1.2 Common Vulnerabilities and Exposures (CVE)CVE is a list of publicly disclosed cybersecurity vulnerabilities and exposures, maintained by MITRE Corporation [2]. It serves as a standard identifier for known vulnerabilities, making it easier for developers to reference and track specific issues."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should mention threat intelligence platforms which can provide contextual information about current threats and attacker tactics.",
            "weight": 0.12,
            "evidence": [
              "Threat intelligence platforms provide contextual information about current threats and attacker tactics, techniques, and procedures (TTPs).3.1 Open-source Intelligence (OSINT) ToolsOSINT tools gather publicly available information to provide insights into potential threats. Examples include Maltego, Shodan, and TheHarvester [6].3.2 Commercial Threat Intelligence PlatformsPlatforms like Recorded Future, FireEye Threat Intelligence, and IBM X-Force Exchange offer curated threat intelligence data and analysis [7]."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should mention patch management tools that can help developers track and manage the process of addressing vulnerabilities across their software ecosystem.",
            "weight": 0.12,
            "evidence": [
              "## Patch Management ToolsAutomated patch management tools ensure that vulnerabilities are mitigated swiftly:- **WSUS (Windows Server Update Services):** Updates released by Microsoft.- **PDQ Deploy:** Deploys patches to Windows environments.- **Chef and Puppet:** Automates the deployment of patches across various environments."
            ]
          }
        ]
      }
    },
    "case_id": "f4cdd43e14e8ede7bb1a2b7776ff5a58",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "Apart from preventing overfitting, are there any side effects (desirable or otherwise) of applying dropout in deep neural networks?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "Apart from preventing overfitting, are there any side effects (desirable or otherwise) of applying dropout in deep neural networks?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly explain the dropout technique in deep neural networks and the main motivation behind its usage for overfitting prevention.",
            "weight": 0.13333333333333333,
            "evidence": [
              "The term \"dropout\" refers to dropping out units (hidden and visible) in a neural network. By dropping a unit out, we mean temporarily removing it from the network, along with all its incoming and outgoing connections, as shown in Figure 1. The choice of which units to drop is random. In the simplest case, each unit is retained with a fixed probability p independent of other units, where p can be chosen using a validation set or can simply be set at 0.5, which seems to be close to optimal for a wide range of networks and tasks.",
              "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem",
              "The central idea of dropout is to take a large model that overfits easily and repeatedly sample and train smaller sub-models from it. Since all the sub-models share parameters with the large model, this process trains the large model which is then used at test time. We demonstrated that this idea works in the context of feed forward neural networks."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should enumerate desirable side effects of using dropout technique, such as improved generalization, robustness, and quality of features.",
            "weight": 0.13333333333333333,
            "evidence": [
              "1. Improved Generalization. One of the most significant desirable side effects of dropout is improved generalization. By randomly dropping out neurons during training, dropout creates an ensemble of subnetworks, which leads to better generalization on unseen data [1].",
              "1. **Sparse Activations**:Dropout can lead to sparse activations in hidden units, even without sparsity-inducing regularizers. This is because the random dropping of nodes forces other nodes to take on more responsibility, resulting in a sparse representation of the data.",
              "2. **Robustness to Co-Adaptation**:Dropout can break up situations where network layers co-adapt to correct mistakes from prior layers, making the model more robust. This is because nodes are forced to adapt to different configurations during training, reducing the likelihood of complex co-adaptations that do not generalize well to unseen data."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should enumerate places where the use of dropout technique can lead to undesirable side effects, such as when training time and data is limited and when the drop probability is not set properly relative to the size of the network.",
            "weight": 0.13333333333333333,
            "evidence": [
              "#### Increased Training Time.Applying dropout increases the training time because each update is noisier, leading to longer convergence times. This can be particularly challenging in resource-constrained environments or when working with very large datasets [Goodfellow et al., 2016].",
              "#### Hyperparameter Sensitivity. Dropout introduces an additional hyperparameter: the dropout rate. Choosing an inappropriate dropout rate can either negate the benefits of dropout or degrade performance. Hence, additional effort in hyperparameter tuning is required [Hinton et al., 2012].",
              "#### Possible Underfitting with High Dropout Rates. Using a very high dropout rate can lead to underfitting since too much information is discarded in each training iteration. This considerably hampers the network's ability to learn and represent underlying patterns in the data [Goodfellow et al., 2016].\"\"2. **Need for Larger Networks**:Dropout can require a wider network (more nodes) to compensate for the reduced capacity during training. This is because the random subsampling of layer outputs reduces the effective capacity of the network."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should discuss that determining the correct dropout rate, network size, data size, and training time is very important for this technique to be effective.",
            "weight": 0.13333333333333333,
            "evidence": [
              "Therefore, it is important to determine an appropriate dropout rate and the number of nodes in designing neural network models to apply to real-world problems.",
              "This suggests that for any given architecture and dropout rate, there is a \"sweet spot\" corresponding to some amount of data that is large enough to not be memorized in spite of the noise but not so large that overfitting is not a problem anyways."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer could compare dropout technique with standard regularization methods for overfitting prevention.",
            "weight": 0.06666666666666667,
            "evidence": [
              "Dropout can be seen as another way of regularizing neural networks. In this section we compare dropout with some of these regularization methods. The MNIST dataset is used to compare these regularizers. The same network architecture (784-1024-1024-2048-10) was used for all the methods. Table. 2.5 shows the results. The KL-sparsity method used a target sparsity of 0.1 at each layer of the network. It is easy to see that dropout leads to less generalization error.\"https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pd"
            ]
          }
        ]
      }
    },
    "case_id": "baee287ff68f0fd60dcbd0d8b9b741b8",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are the advantages and limitations of applying bias mitigation algorithms during pre-processing, training and inference stages of a model?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are the advantages and limitations of applying bias mitigation algorithms during pre-processing, training and inference stages of a model?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define bias in machine learning and bias mitigation algorithms.",
            "weight": 0.13333333333333333,
            "evidence": [
              "However, like humans, ML algorithms are vulnerable to biases that make their predictions and decisions \"unfair\" (Angwin et al., 2016). In the context of ML decision-making, fairness is the absence of any prejudice or favoritism toward an individual or group based on their inherent or acquired characteristics (Mehrabi et al., 2019). Thus, a biased and unfair ML algorithm makes decisions that are skewed toward a particular group of people. Although ML algorithms operate in the digital domain, ML biases have many real-world consequences and may cause substantive harm to both consumers and companies."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention that applying bias mitigation during pre-processing, such as re-balancing, sampling, and latent variables can be effective in mitigating bias",
            "weight": 0.13333333333333333,
            "evidence": [
              "Relabeling and Perturbation. This section presents bias mitigation methods that applychanges to the values of the training data. Changes have been applied to the ground truth labels (relabeling) or the remaining features (perturbation).",
              "Sampling. Sampling methods change the training data by changing the distribution of samples (e.g., adding, removing samples) or adapting their impact on training. Similarly, the impact of training data instances can be adjusted by reweighing their importance",
              "Latent Variables. Latent variables describe the augmentation of training data with additional features that are preferably unbiased.",
              "Representation. Representation learning aims at learning a transformation of the training data such that bias is reduced while maintaining as much information as possible",
              "One key advantage of pre-processing techniques is their ability to address bias at its source, potentially preventing it from being encoded into the model during training  (35, Rajan et al., 2021). This early intervention can make it easier to achieve fairness algorithmically compared to addressing bias later in the pipeline.",
              "However, pre-processing methods also have limitations. For instance, RW is limited to considering only a single sensitive attribute when mitigating bias and assumes that each sensitive attribute is equally important  (130, An et al., 2023). Additionally, different pre-processing algorithms may employ varying concepts of fairness, which can lead to conflicting strategies and trade-offs between fairness and accuracy  (107, Collante et al., 2022).",
              "In summary, pre-processing bias mitigation techniques offer the advantage of addressing bias early in the machine learning pipeline, potentially preventing it from being encoded into the model. However, their effectiveness may be limited by the complexity of bias in real-world datasets and the potential for conflicting fairness concepts. As such, they are often used as part of a multi-pronged approach to bias mitigation in machine learning systems.",
              "**Advantages:**1. **Data Fairness**: Pre-processing techniques transform the input data to ensure it is unbiased before being fed into the model. This can involve re-weighting samples, sampling strategies, or representation learning methodologies to mitigate biases related to sensitive attributes (Kamiran & Calders, 2012).2. **Model-agnostic**: Since modifications happen before the input data is used for any model, these techniques can be applied irrespective of the choice of the subsequent models.3. **Legal Compliance**: Pre-processed data adheres more closely to ethical and legal standards since biases present in the raw data (historical and societal) are addressed.**Limitations:**1. **Data Loss**: Transformations in data to remove bias might lead to loss of important information, affecting the overall accuracy and performance of the model (Calders et al., 2009).2. **Complexity**: Identifying the exact nature and source of biases in large datasets is complex and time-consuming.3. **Not Dynamic**: As the mitigation is done statically before training, it doesn't adapt to changes in the data distribution over time."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should mention that while applying bias mitigation during training, for instance, by regularization and constraints in the loss function or using adversarial learning to minimize the model's ability to predict sensitive attributes, can be effective, but it may come at the cost of model accuracy or performance",
            "weight": 0.13333333333333333,
            "evidence": [
              "Regularization and Constraints. Regularization and constraints are both approaches that apply changes to the learning algorithm's loss function. Regularization adds a term to the loss function. While the original loss function is based on accuracy metrics, the purpose of a regularization term is to penalize discrimination (i.e., discrimination leads to a higher loss of the ML algorithm).",
              "Adversarial Learning. Adversarial learning simultaneously trains classification modelsand their adversaries [92]. While the classification model is trained to predict ground truth values the adversary is trained to exploit fairness issues.\"Compositional. Compositional approaches combat bias by training multiple classification models. Predictions can then be made by a specific classification model for each population group",
              "Adjusted Learning. Adjusted learning methods mitigate bias via changing the learningprocedure of algorithms or the creation of novel algorithms",
              "In summary, in-processing bias mitigation techniques offer the advantage of directly addressing bias during the learning process, potentially leading to more robust and fair models. However, their effectiveness must be balanced against computational costs and potential impacts on model performance. As such, they are often used as part of a comprehensive approach to bias mitigation in machine learning systems, potentially in combination with pre-processing or post-processing methods.",
              "**Advantages:**1. **Dynamic Adjustment**: In-training bias mitigation approaches, such as regularization techniques or adversarial training, allow for adaptive adjustment as the model learns. This can produce a robust model that balances bias mitigation with performance (Zhang et al., 2018).2. **Optimization Problem**: Bias mitigation goals can be integrated into the optimization objective of the learning algorithm, enabling a more balanced and fair model outcome.3. **Model-specific**: Techniques can be tailored to the architecture and peculiarities of a specific model, potentially improving effectiveness (Madras et al., 2018).**Limitations:**1. **Resource Intensive**: Training complex models with additional bias mitigation constraints can require significantly more computational resources and time.2. **Complexity of Implementation**: Combining fairness objectives with traditional accuracy-driven objectives can be difficult and may require sophisticated algorithmic strategies.3. **Performance Trade-offs**: Balancing between fairness and accuracy often leads to trade-offs where improving fairness metrics could harm the standard performance metrics of the model (Woodworth et al., 2017)."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should mention that applying bias mitigation during inference, such as input/output corrections, can be helpful but may raise ethical concerns",
            "weight": 0.13333333333333333,
            "evidence": [
              "Input Correction. Input correction approaches apply a modification step to the testingdata. This is comparable to pre-processing approaches (Section 4.1) [108], which conduct modifications to training data (e.g., relabeling, perturbation, and representation learning)",
              "Classifier Correction. Post-processing approaches can also directly be applied to classification models, which Savani et al. [323] called intra-processing. A successfully trained classification model is adapted to obtain a fairer one. Such modifications have been applied to Naive Bayes",
              "Output Correction. The latest stage of applying bias mitigation methods is the correctionof the output. In particular, the predicted labels are modified.",
              "In summary, post-processing bias mitigation techniques offer advantages in terms of flexibility and applicability to existing models, including black-box systems. They can be particularly effective for correcting bias in models that initially show higher levels of unfairness. However, their effectiveness may be limited when addressing biases deeply embedded in the model's learned representations. As such, they are often used as part of a comprehensive approach to bias mitigation in machine learning systems, potentially in combination with pre-processing or in-processing methods.",
              "**Advantages:**1. **Real-time Adjustment**: Techniques such as post-hoc re-weighting or modifying decisions dynamically at inference time can cater to varying bias requirements as they arise in practice, leading to more flexible and adaptable applications.2. **Non-disruptive**: These methods do not require changing the pre-trained model or retraining, making them easier to implement in existing systems (Hardt et al., 2016).3. **Specificity**: Bias mitigation at inference can target specific instances or populations, providing granular control over fairness in decision-making processes.**Limitations:**1. **Limited Scope**: Because they act only at the decision-making stage, these methods can rarely correct deep-rooted biases learned by models during training.2. **Consistency**: Ensuring fairness on a case-by-case basis can lead to inconsistencies, possibly contradicting the overall logic or patterns the model learned.3. **Overhead**: Adjusting outputs in real-time can add latency to the prediction process, potentially impacting the usability of the model in time-sensitive applications."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer might mention some example of bias mitigation algorithms during pre-processing, training and inference stages of a model",
            "weight": 0.06666666666666667,
            "evidence": [
              "We also identified 24 mitigation methods for addressing the aforementioned biases within the CRISP-DM process phases, as summarized in Table 2. Notably, a particular bias can be mitigated by several methods, and a particular method can mitigate multiple biases. In addition, a mitigation method that is applied in one phase can address biases that occur in the respective phase or in the later stages of the ML project."
            ]
          }
        ]
      }
    },
    "case_id": "0348920a58979e759af9081a6225ee0d",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "How do different fairness metrics correlate with each other across various datasets and model architectures?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "How do different fairness metrics correlate with each other across various datasets and model architectures?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should emphasize the importance of studying fairness in MLdue to the potential for inherent bias that may be ingrained in the data utilized for training.",
            "weight": 0.10909090909090909,
            "evidence": [
              "It is of critical importance to be aware of the historical discrimination embedded in the data and to consider a fairness measure to reduce bias throughout the predictive modeling pipeline."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should acknowledge the existence of different metrics for fairness in ML and highlight the importance of studying correlation between them.",
            "weight": 0.10909090909090909,
            "evidence": [
              "Given various notions of fairness defined in the literature, investigating the correlation and interaction among metrics is vital for addressing the unfairness.",
              "To motivate our method, we start by reemphasizing the importance of having correlated fairness across existing prompt-based fairness metrics for a more reliable fairness assessment (Section 4.1).",
              "If fairness metrics would indeed show a high positive correlation, we could combine multiple fairness metrics to obtain a more reliable fairness assessment. This increase in reliability intuitively stems from the use of several distinct and accurate sources of bias assessment.",
              "If fairness metrics are uncorrelated, an improvement in fairness using one metric will not necessarily lead to an improvement using other metrics (it could lead to fairness degradation on other metrics in the case of negative correlation).",
              "That is, we say a fairness measure fi represents a measure fj if fi and fj are highly correlated. Given a universe F of fairness metrics of interest, we seek to find a subset RF , with a significantly smaller size representing all metrics in F."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should list some of the most important metrics for measuring fairness.",
            "weight": 0.10909090909090909,
            "evidence": [
              "**Statistical Parity (Demographic Parity)**: Measures whether different groups receive positive outcomes at the same rate.",
              "**Equalized Odds**: Requires that the true positive rate and false positive rate are equal across groups.",
              "**Predictive Parity**: Conditions on the prediction and measures whether the accuracy is consistent across groups.",
              "Equal opportunity: Equal opportunity examines the true positive rate (TPR). Meaning it looks at the number of people that returned the loan to see the ratio of them that the model approved."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should discuss factors that lead to the correlation between various fairness metrics being dependent on the data and model and provide empirical evidence and justifications for that.",
            "weight": 0.10909090909090909,
            "evidence": [
              "In this paper, we propose a framework to identify representative fairness metrics for a given context (data and model). The underlying assumption behind this proposal is that correlations are data and model dependent. Having evaluated our correlation estimation quality, in this experiment, we verify that the data/model-dependent assumptions for the correlations are valid.",
              "#### Nature of Data. The correlations between fairness metrics can heavily depend on the characteristics of the datasets used:**Disparity in Base Rates**: Datasets with different base rates for different groups might show more significant conflicts between metrics like demographic parity and equalized odds **Feature Distributions**: The distribution of features can affect the fairness metrics. For example, datasets with biased historical data might show different correlations than synthetic datasets designed to be unbiased.",
              "#### Empirical Studies. Research by Binns (2018) indicates that real-world datasets often exhibit complex relationships between fairness metrics. For instance, in COMPAS recidivism data, improving one fairness metric also marginally improves others, but in datasets like Adult Income, the trade-offs are much steeper.",
              "Similarly, Figure 4 verifies that correlations are data-dependent. This is because different datasets represent different underlying distributions with different properties impacting the fairness values.",
              "### 4. Influence of Model Architectures#### Type of Models. Different model architectures (e.g., linear models, decision trees, neural networks) have distinct fairness characteristics and hence affect the correlations between fairness metrics:**Linear Models**: May show simpler relationships due to their inherent linearity and can sometimes improve fairness metrics in tandem.**Complex Models**: Such as neural networks, might display intricate and less predictable relationships between metrics due to their capacity to capture non-linear associations.#### Training ProceduresFairness-enhancing interventions during training can also impact metric correlations:**Regularization Techniques**: Techniques such as fairness constraints added during training might simultaneously improve multiple fairness metrics, according to Agarwal et al. (2018).**Adversarial Training**: Can specifically target and manipulate certain fairness metrics, improving one at the expense of others.",
              "Looking at Figure 3, it is clear that correlations are model-dependent. In particular, NN which is capable of constructing more complex boundaries, resulting in more flexible models with a wide range of fairness values for different metrics. As a result, the correlations between fairness metrics for NN was in general, less than the other models."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should mention some of the correlations that have been identified between different fairness metrics based on both empirical and statistical results.",
            "weight": 0.10909090909090909,
            "evidence": [
              "#### Statistical Correlations: Studies have explored the relationship between these metrics, revealing that they often conflict. For example, Kleinberg et al. (2016) have shown that ensuring both equalized odds and predictive parity simultaneously is generally impossible unless the base rates are equal across groups",
              "#### Empirical Findings: Empirical analyses show that correlations between fairness metrics can vary. Friedler et al. (2019) in \"A Comparative Study of Fairness-enhancing Interventions in Machine Learning\" demonstrate that fairness-enhancing interventions can affect different metrics in different ways. For instance, methods that improve statistical parity might worsen equalized odds and vice versa.",
              "For example, mitigating unfairness based on the Equal opportunity metric (E-Opp) and Equalized odds difference (odd-dif) in the Credit dataset, shown in Figure 7 (a-c) and (d-f), respectively, results in diminishing unfairness across highly correlated metrics such as Statistical Parity (SP), False negative rate ratio (FNR-rat). For example, mitigating unfairness based on the Equal opportunity metric (E-Opp) and Equalized odds difference (odd-dif) in the Credit dataset, shown in Figure 7 (a-c) and (d-f), respectively, results in diminishing unfairness across highly correlated metrics such as Statistical Parity (SP), False negative rate ratio (FNR-rat)."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should discuss that the choice of fairness metric is usually driven by business needs.",
            "weight": 0.05454545454545454,
            "evidence": [
              "fairness metrics are a business-driven constraint. What should be considered a protected field, and what fairness criteria to use should be driven by the business. Or, more precisely, the bias, compliance, and regulatory risks that your business is exposed to."
            ]
          }
        ]
      }
    },
    "case_id": "323e85c9052082358fc0c045fe20a537",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "How does doping impact integrated circuit design?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "How does doping impact integrated circuit design?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly explain what doping is in the context of semiconductor and its role in integrated circuit design",
            "weight": 0.19999999999999998,
            "evidence": [
              "Doping semiconductor materials by incorporating chemical impurities has been key to the development of today's cutting edge device technologies [1]. However, with device dimensions of only a few nanometers, the conventional impurity doping faces challenges. The formation of junctions with extremely high doping gradients (a few nm/decade) is practically difficult"
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention the types of doping: n-type doping which increases the concentration of free electrons, and p-type doping which increases the concentration of holes.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Semiconductors containing many mobile electrons and few holes are called N-type semiconductors because electrons carry negative (N) charge. As and P are the most commonly used donors in Si. Similarly, when boron, a group III impurity, is introduced into Si as shown in Fig. 1-6b, each B atom can accept an extra electron to satisfy the covalent bonds, thus creating a hole. Such dopants are called acceptors, for they accept electrons. Semiconductors doped with acceptors have many holes and few mobile electrons, and are called P type because holes carry positive (P) charge",
              "### Types of Doping1. **n-Type Doping:**- In n-type doping, donor atoms with more valence electrons than the semiconductor material (commonly silicon) are added. Elements such as phosphorus or arsenic are commonly used.- These donor atoms provide extra electrons, enhancing the conductivity by increasing the number of free electrons in the material.2. **p-Type Doping:**- In p-type doping, acceptor atoms with fewer valence electrons than the semiconductor material are introduced. Boron is a typical element used for this purpose.- These acceptor atoms create \"holes\" (absence of electrons) that act as positive charge carriers, facilitating the flow of current by allowing electrons to move."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should mention the impact of doping concentration on the electrical characteristics of semiconductors, such as conductivity and threshold voltage",
            "weight": 0.19999999999999998,
            "evidence": [
              "Doping of semiconductors is a versatile and powerful method to control important properties of the semiconducting material such as conductivity, chemical potential, and recombination rates. Particularly for organic semiconductor (OSC) materials, which often suffer from defect states, chemical doping by using small-molecule additives has been shown to be an effective way to increase the density of free charge carriers and eventually improve the charge-carrier transport. 1[?]3 Additionally, chemical doping of OSCs enabled the fabrication of efficient and longliving electronic devices such as organic solar cells,4 organic light-emitting diodes,5 and transistors6 by lowering interface barriers between adjacent layers, reducing contact and chargecarrier injection resistances, 7,8 and balancing the hole and electron transport.",
              "Nonetheless, doping is highly attractive for OFETs since it enables control of the threshold voltage which is of vital importance for many fundamental electronic circuits such as logic gates. 13,14 In particular, threshold voltage control in digital circuits is important in order to define the tripping point of inverters, thereby helping to increase the accessible voltage range and inverter gain. In this regard, precise tuning of the threshold voltage has been demonstrated using direct doping of the semiconductor material 15 or surface doping on the gate insulator, for example, via self-assembled monolayers or surface-dipole-creating entities.",
              "Tuning the threshold voltage of a transistor is crucial for realizing robust digital circuits. For silicon transistors, the threshold voltage can be accurately controlled by doping.",
              "Influence of fullerenes C60 on the threshold voltage, dielectric and conductivity properties of smectic A liquid crystal 4-hexyloxyphenyl ether 4'-hexyloxy 3'-nitrobenzoic acid is investigated. It is shown that the transverse component of the real part of dielectric permittivity increases at the additive of fullerenes while the longitudinal component decreases. In this case, a maximum of dielectric absorption shifts to the high-frequency region. Conductivity increases at low frequencies and decreases at the high ones. Experimental results are explained by location of fullerenes between liquid crystal molecules reducing their interaction. As a result, the order parameter and viscosity of the matrix decrease.",
              "## Impact on Integrated Circuit Design### Charge Carrier DensityThe density of charge carriers in the semiconductor material is directly affected by the type and level of doping. This, in turn, influences several aspects of IC design:- **Threshold Voltage (Vt):**- Doping concentration alters the threshold voltage of transistors, which is the minimum gate-to-source voltage that creates a conductive path between the source and drain terminals.- Precise control over threshold voltage is critical for ensuring the reliable operation of logic gates and other IC components.- **Conductivity and Mobility:**- Increased doping levels generally improve conductivity by providing more charge carriers. However, it can also reduce carrier mobility due to increased scattering from the dopant atoms.- Balancing conductivity and mobility is essential for optimizing transistor switching speeds and overall IC performance.",
              "### Junction FormationDoping profiles determine the formation of p-n junctions, which are pivotal in the behavior of diodes and transistors within an integrated circuit:- **Diode Characteristics:**- The p-n junction's properties, such as forward voltage drop and reverse breakdown voltage, are controlled by the doping levels.- This affects key parameters like the switching speed and efficiency of diodes used in power management and signal processing.- **Transistor Performance:**- For MOSFETs (Metal-Oxide-Semiconductor Field-Effect Transistors), the p-n junctions between the source, drain, and substrate must be precisely controlled to obtain the desired electrical characteristics.- The doping gradient, which forms the depletion region, crucially determines the transistor's switching characteristics and leakage currents.",
              "### Ion Implantation and Diffusion:- **Ion Implantation:**- A common doping technique where ions are accelerated and embedded into the semiconductor wafer.- Allows precise control over doping concentration and depth profiles, influencing the integration process and device scaling.- **Diffusion:**- An older method that relies on heating the semiconductor in the presence of dopant gases. Although less precise than ion implantation, it is still relevant for certain applications.",
              "### **4. Leakage Current Reduction**Doping can be used to reduce leakage current in ICs. Leakage current is the current that flows through a transistor when it is in the off state. By using doping to create regions with low carrier concentrations, leakage current can be minimized, reducing power consumption and increasing the overall efficiency of the IC."
            ]
          }
        ]
      }
    },
    "case_id": "46eecf06447c43657abf042004cdc115",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What sampling techniques are used for yield estimation in automated electronic circuit design?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What sampling techniques are used for yield estimation in automated electronic circuit design?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define yield estimation in the context of automated electronic circuit design and explain why it is important",
            "weight": 0.08571428571428572,
            "evidence": [
              "In modern circuit designs, particularly in scenarios like SRAM cell arrays where certain cells can be replicated millions of times, addressing yield concerns has become paramount. Efficient yield estimation methods are crucial for providing accurate and rapid failure rate assessments in the presence of specific process variations.",
              "Yield estimation is a crucial aspect of automated electronic circuit design, as it helps predict the percentage of manufactured circuits that will meet the desired specifications. This process is essential for:a) Cost optimizationb) Performance predictionc) Reliability assessmentd) Manufacturing feasibility analysis"
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention common sampling techniques such as Monte Carlo simulation and their role in yield estimation",
            "weight": 0.08571428571428572,
            "evidence": [
              "We describe a transistor-level Monte Carlo (MC) technique which makes final transistor-level timing verification practically feasible. The MC method is used as a golden reference in assessing the accuracy of other timing yield estimation techniques.",
              "The yield of an Integrated Circuit (IC) is commonly expressed as the fraction (in %) of working chips overall manufactured chips and often interpreted as the failure probability of its analog blocks. We consider the Importance Sampling Monte Carlo (ISMC) as a reference method for estimating failure probabilities.",
              "Monte Carlo (MC) simulation, the industry-standard baseline, is commonly employed for yield estimation. MC involves running SPICE (Simulation Program with Integrated Circuit Emphasis) simulations with parameters drawn from the process variation distribution millions of times, counting failures to obtain precise estimates. However, MC is computationally intensive and becomes impractical for practical problems where the yield can be as low as 10[?]5 , a common setup in a 65nm SRAM cell array."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should describe Latin hypercube sampling as a more efficient alternative to Monte Carlo",
            "weight": 0.08571428571428572,
            "evidence": [
              "A well known variance-reduction technique in Monte Carlo is Latin hypercube sampling, whose superiority over random sampling has been corroborated often already. A theoretical analysis of the performance of Latin hypercube and basic random sampling was brought first by McKay and co-authors [32]: they found that Latin hypercube sampling is more efficient than random sampling when the desired output is monotonic in all inputs.",
              "The Monte Carlo method exhibits generality and insensitivity to the number of stochastic variables, but is expensive for accurate Average Quality Measure (AQI) or Parametric Yield estimation of MOS VLSI circuits. In this contribution a new method of variance reduction technique, viz. the Latin Hypercube Sampling (LHS) method is presented which improves the efficiency of AQI estimation in integrated circuits especially for MOS digital circuits."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should discuss about importance sampling and how it focuses on critical regions of the design space.",
            "weight": 0.08571428571428572,
            "evidence": [
              "Importance sampling is a variance reduction method that focuses on generating more sample points in the critical fail regions [6]. Hence, instead of sampling, using the natural probability density function (PDF) f (x), one would sample a distorted PDF p(x) that biases the sampling to the important region, typically toward the tails of the distribution",
              "Importance Sampling is a well-known variance reduction methodology that aims at distorting the natural distribution to focus more on the important regions. The distorted distribution [4, 5, 15] is often chosen as a shifted version of the true distribution"
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should discuss about gaussian process regression for yield estimation in automated electronic circuit design",
            "weight": 0.08571428571428572,
            "evidence": [
              "### 2. Gaussian Process RegressionGaussian process regression is a statistical method that models the relationship between design parameters and the yield. It uses a set of sampled points to estimate the yield function, which can then be used to predict the yield for new designs. This technique has been applied in yield optimization for analog circuits, where it has shown to be efficient and accurate."
            ]
          },
          {
            "name": "most_important_item_5",
            "criterion": "The answer should discuss about bayesian inference for yield estimation in automated electronic circuit design",
            "weight": 0.08571428571428572,
            "evidence": [
              "### 3. Bayesian InferenceBayesian inference is a probabilistic approach that updates the yield estimation based on new data. It uses Bayes' theorem to combine prior knowledge with new data to estimate the yield. This method has been applied in yield estimation for analog circuits, where it has shown to be effective in handling uncertainty and variability."
            ]
          },
          {
            "name": "most_important_item_6",
            "criterion": "The answer should discuss about varying-sigma sampling for yield estimation in automated electronic circuit design",
            "weight": 0.08571428571428572,
            "evidence": [
              "### 5. Varying-Sigma SamplingVarying-sigma sampling is a technique that involves executing yield estimations at varying sigma levels of process variations. This method is useful for quickly determining the better design and has been applied in yield optimization for analog circuits."
            ]
          }
        ]
      }
    },
    "case_id": "b73090602419bdedaebd2a9d40e5a199",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "Compare the various algorithms for compressing trees in terms of computation complexity.",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "Compare the various algorithms for compressing trees in terms of computation complexity.",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly outline the unique challenges involved in compressing trees.",
            "weight": 0.08,
            "evidence": [
              "These trees are frequently very large, prompting a need for compression for on-disk storage.",
              "Most of the data compression algorithms regard the input as a sequence of binary numbers and represent the compressed data also as a binary sequence. However, in many areas such as programming language (e.g. LISP and C)and compiler design, it is more desirable to have a compression algorithm which compresses a data structure which is not a binary sequence and which keeps similar data structure in compressed form as the original data. In this paper, we study the problem of compressing tree and graphs into a similar but smaller form so that many properties of the original data structure are kept in the compressed form. Undirected and directed graph are widely used in representing data structures in programming languages"
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should specify that tree compression usually focuses on keeping the original runtime of operations on trees while reducing their size. So, the performance is usually measured in terms of the size of the compressed tree and the time complexity of tree operations. In addition to that, the construction time can also be important for some applications.",
            "weight": 0.08,
            "evidence": [
              "Tree compression can be seen as a trade-off problem between time and space in which we can choose different strategies depending on whether we prefer better compression results or more efficient operations in the compressed structure. Of special interest is the case where space can be saved while preserving the functionality of the operations; this is called data optimization.",
              "In the tree optimization problem, a term adopted from the work of Jacobson [4], the task is to maintain the functionality of a tree in the compressed form. That is, we want to perform some tree operations as efficiently as done in the uncompressed case (where an operation is a simple matter of pointer manipulation). We are mainly concerned with applications where the trees are manipulated in the internal memory of a computer. So, the mappings are to bit strings or memory words only."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss that tree compression involves two steps, compression of the structure and compression of the data. While the latter can be done using conventional data compression methods, compressing the structure is challenging.",
            "weight": 0.08,
            "evidence": [
              "The compression and optimization of trees is usually performed in two phases: the compression of the structure (linearization of the tree structure) and the compression of the data stored in it (linearization of data). Our general policy is to handle the data and the structure separately. This enables us to compress the plain data by using any of the known methods and independently find an efficient coding method for the tree structure irrespective of the form and contents of the data items stored in the nodes. We shall not consider normal data compression methods but assume that the reader is familiar with e.g. arithmetic coding [7] and Huffman coding [2];"
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should list some of the most important algorithms for compressing trees.",
            "weight": 0.08,
            "evidence": [
              "Lempel-Ziv compression (LZ) [8, 7] is a well known algorithm that uses a dictionary to encode the original input. LZ compression parses the original sequence into subsequences by maximum prex match and it is proven that LZ compressionachieves optimal compression ratio when the size of the input goes to infinity. LZWcompression is a popular version of LZ method and has been implemented as \\compress\" command on UNIX systems.",
              "Chen and Reif [108] decompose an input graph into several binary trees, and finally compress these trees with a proposed tree-compression algorithm. The key idea in compressing a single binary tree is to further decompose this tree into smaller subtrees. These subtrees are small enough that any such subtree can be found multiple times in the input tree. Thus, after the full binary tree decomposition, the authors calculate occurrence probabilities for each subtree and assign the corresponding Huffman code to it. Finally, the tree is encoded by traversing it and assigning the above codes. Now, the method to find and count respective subtrees is similar to counting words in texts. Specifically, the authors traverse the input tree with BFS and build a suffix tree in the process where each node of a suffix tree corresponds to one specific subtree",
              "2.1 DAG (Directed Acyclic Graph) Compression- Complexity: O(n), where n is the number of nodes in the tree- Description: DAG compression identifies and merges identical subtrees, creating a more compact representation.",
              "2.2 Top Trees- Complexity: O(n log n) for construction, O(log n) for many tree operations- Description: Top Trees use a hierarchical decomposition of the tree, allowing for efficient updates and queries.",
              "2.3 Tree Grammar Compression- Complexity: O(n) for construction, can achieve O(log n) query time for some operations- Description: This method represents the tree as a context-free grammar, exploiting repetitive structures.",
              "2.4 Succinct Tree Representations- Complexity: O(n) for construction, supports many operations in O(1) time- Description: Succinct representations aim to store the tree in space close to the information-theoretic lower bound while supporting efficient operations."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should conclude that LZ, binary tree, DAG Compression and Succinct Representations generally have the best construction time complexity, followed closely by Tree Grammar Compression. Top Trees have a slightly higher complexity due to their hierarchical nature.",
            "weight": 0.08,
            "evidence": [
              "Construction Time Complexity- DAG Compression and Succinct Representations: O(n)- Top Trees: O(n log n)- Tree Grammar Compression: O(n)"
            ]
          },
          {
            "name": "most_important_item_5",
            "criterion": "The answer should discuss  the query time of compression methods highlighting the Top tree, the grammar compression, binary tree, and LZ achieve O(log n) for many operations, followed by DAG.",
            "weight": 0.08,
            "evidence": [
              "3.2 Query Time Complexity- DAG Compression: Varies depending on the query, can be up to O(n)- Top Trees: O(log n) for many operations- Tree Grammar Compression: Can achieve O(log n) for some operations- Succinct Representations: O(1) for many basic operations"
            ]
          },
          {
            "name": "most_important_item_6",
            "criterion": "The answer should discuss the efficiency of different methods in terms of space complexity and provide insight into when each method should be used.",
            "weight": 0.08,
            "evidence": [
              "3.3 Space Efficiency- DAG Compression: Can be very efficient for trees with many repeated subtrees- Top Trees: Moderate space efficiency- Tree Grammar Compression: Highly efficient for trees with repetitive structures- Succinct Representations: Theoretically optimal space usageTree Grammar Compression and Succinct Representations generally achieve the best space efficiency, especially for trees with repetitive structures.",
              "The choice of tree compression algorithm depends on the specific requirements of the application, such as the nature of the tree data, the frequency of updates, and the types of queries to be performed.- For static trees with many repeated subtrees, DAG Compression or Tree Grammar Compression might be preferable.- For dynamic trees requiring frequent updates, Top Trees could be the best choice.- For applications requiring fast queries on basic operations and optimal space usage, Succinct Representations are often ideal."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer could point to some applications of tree compression.",
            "weight": 0.04,
            "evidence": [
              "Typical application areas of the tree compression methods include the representation of graphics as pixel trees [9-12] and program files as syntax trees [13,14]. Both applications are of practical importance, and the tree methods support excellent compression results. These applications are examined in greater detail in chapter 6."
            ]
          }
        ]
      }
    },
    "case_id": "2345ff0aa0b13fc87a46d5528be46060",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "Suggest some ways in which precise positioning and context-aware networking can be achieved in 6G mobile communication?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "Suggest some ways in which precise positioning and context-aware networking can be achieved in 6G mobile communication?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly point to the evolution of positioning in cellular networks.",
            "weight": 0.10909090909090909,
            "evidence": [
              "Positioning (aka localization) technology utilizing 3G, 4G and Wi-Fi systems has existed for at least the last 20 years. 5G New Radio (NR) Positioning is now evolving as a key feature that enables the operator of a 5G network to position devices for both indoor and outdoor applications with much better accuracy and reliability than those of previous generations. A wide variety of use cases require positioning services--from locating emergency callers' locations to autonomous driving to tracking automated forklifts moving in a factory. Many of these use cases, which will expand the applicability of 5G networks beyond traditional mobile broadband, require accurate positioning in order to function. 5G networks offer the exciting possibility to deploy a single technology that addresses both the positioning and data needs of these use cases in an integrated solution, all in an ultra-reliable Manner.",
              "Accurate and real-time positioning is highly demanded by location-basedservices and can be beneficial for radio resource management in 5G networks, which are deployed to achieve significant performance improvements over existing cellular networks.",
              "6G, the sixth generation of mobile communication technology, is expected to provide unprecedented levels of connectivity, speed, and intelligence. Two key features that will distinguish 6G from its predecessors are ultra-precise positioning and advanced context-aware networking. These capabilities will enable a wide range of new applications and services, from autonomous vehicles to immersive augmented reality experiences."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should briefly point to classical signal processing techniques used in cellular networks for positioning.",
            "weight": 0.10909090909090909,
            "evidence": [
              "In cellular-based localization, the downlink transmissions from the Base Station (BS) to the mobile device and uplink transmissions from the mobile device to BS can be utilized to determine the position of the UE [18]. The cellular-positioning methods can be classified into two main categories depending on the entity that computes the position: (1) mobile based, where the UE itself calculates its location, and (2) network-based, where the network location server computes the position of the UE. Most cellular-based positioning solutions are network-based due to its centralized nature that allows full control of the location service by the network operator, as well as its support to legacy devices.",
              "Classical signal processing techniques for cellular networks include-Trilateration: the position estimate is obtained by intersecting geometric forms,e.g. circles or hyperbolas, created by distance or angle measurements betweenthe terminal and the reference transmitters or receivers. Several types of measurements can be used, such as time of arrival (ToA), time difference of arrival(TDoA), direction or angle of arrival (DoA or AoA), and received signal strength(RSS).-Proximity: the known transmitter position is assigned to be the position of theterminal. An example is the cell-ID method, where the position provided is theone of the serving base station.-Fingerprinting: the algorithm is based on finding the best match for a certainsignal measurement, such as RSS, time delay or channel delay spread, from adatabase of fingerprints. Each fingerprint is associated with a specific location."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss new technologies, e.g., massive MIMO, mmWave communications, Device-to-Device (D2D) communications, and GNSS assisted methods, introduced in 5G networks that not only improve communication performance, but also provide the ability to significantly increase positioning accuracy.",
            "weight": 0.10909090909090909,
            "evidence": [
              "Many new technologies, e.g., massive MIMO, mmWave communications, Ultra-Dense Network (UDN), and Device-to-Device (D2D) communications, are being introduced in 5G networks to not only improve communication performance, but also provide the ability to significantly increase positioning accuracy [18]. It is envisioned that 5G networks will be able to locate User Equipment (UE) with sub-meter accuracy and provide high network utility [26], such as seamless coverage, low delay, and high throughput, placing higher requirements for cellular communication systems.",
              "The main breakthrough in 5G is due to the employment of massive multiple input-multiple-output (MIMO) beamforming and of millimeter wave (mmWave) signals. The use of mmWave brings a two-fold advantage: large available bandwidth and the possibility to pack a large number of antenna elements even in small spaces (e.g., in a smartphone). Wideband signals offer better time resolution and robustness to multipath thus improving the performance of OTDOA/UTDOA schemes, as well as paving the way to new positioning methods such as multipath-assisted localization exploiting specular multipath components to obtain additional position information from radio signals [8].",
              "A large number of antenna elements enables massive MIMO and very accurate beamforming (see Fig. 5). This will make possible the introduction of single-anchor approaches providing cm-level and degree-level accuracy in 6D positioning (3D position and 3D orientation) [9], thus overcoming the problem of deploying a redundant ad-hoc infrastructure which is, nowadays, a major bottleneck for the widespread adoption of indoor localization systems.",
              "In addition, device-to-device (D2D) are under consideration in Release 16 for ultra-dense networks enabling cooperative localization, for instance, in vehicle-to-everything (V2X) scenarios [10].",
              "3.2. Assisted Positioning in 5G Networks: In this section, we present the efficiency of 5G hybrid solutions. The following examples show that GNSS assisted 5G positioning is currently being extensively studied by multiple research groups. In addition, a promising solution is also reviewed, which utilizes 5G signals assisted by visible light communication. Sun et al. [43] proposed a hybrid 5G-GNSS positioning method (see Figure 5) based on combining AOA estimates from 5G base stations and TOA measurements from GNSS satellites."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should discuss the role of ML in positioning methods and assess whether it enhances the accuracy.",
            "weight": 0.10909090909090909,
            "evidence": [
              "The presented papers show how the emerging communication technologies (e.g., massive MIMO, mmWave communication) of 5G networks can be used for positioning with machine learning. These new technologies increase the amount of data and the number of features available for positioning. Most of these new features (e.g., AoD) can be handled with traditional techniques (e.g., triangulation), but the combination of these features and the increased amount of data make the ML techniques easily manageable compared to traditional techniques. The main advantage of ML algorithms is rapid model development and updating, as new data and measurement types can be used and combined immediately. This makes them particularly well suited as a replacement for traditional fingerprinting algorithms, especially in cases where measurements have a complex relationship to positions"
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should discuss emerging technologies anticipated for 6G that can meet its more stringent positioning requirements.",
            "weight": 0.10909090909090909,
            "evidence": [
              "Assuming that the maximum bandwidth for 6G will be up to 100 times that of 5G systems, its potential for positioning accuracy and efficiency will be much greater, so that it is envisioned that localization precision will be as high as 1 cm in 3D [13], [14].",
              "The 6G network will aim to satisfy the challenging requirements of these new applications based on its unprecedented technological advancements. These advancements include even higher frequency ranges, wider bandwidths, massive antenna arrays, intelligent surfaces, intelligent beam-space processing, AI and machine-learning-based techniques, sidelink solutions, architecture evolution, and beyond connectivity [25,69-75].",
              "2.1 Terahertz Frequency BandsOne of the most promising ways to achieve precise positioning in 6G is through the use of terahertz (THz) frequency bands. THz frequencies (0.1-10 THz) offer extremely high bandwidth and allow for very fine time resolution, which translates to improved positioning accuracy [1].Suggested approach:- Implement THz-based positioning systems that can achieve sub-centimeter accuracy- Develop new signal processing algorithms optimized for THz frequencies",
              "2.2 Massive MIMO and Intelligent SurfacesMassive Multiple-Input Multiple-Output (MIMO) technology, combined with intelligent reflecting surfaces (IRS), can significantly enhance positioning accuracy.Suggested approach:- Deploy large-scale antenna arrays for improved spatial resolution- Utilize IRS to manipulate signal propagation and overcome non-line-of-sight limitations- Develop advanced beamforming techniques for precise angle-of-arrival estimation",
              "2.3 Artificial Intelligence-Enabled PositioningAI and machine learning algorithms can be leveraged to improve positioning accuracy by learning from historical data and adapting to changing environments.Suggested approach:- Implement deep learning models for enhanced multipath resolution- Develop AI-driven sensor fusion techniques to combine data from multiple positioning technologies (e.g., GNSS, inertial sensors, and cellular signals)",
              "3. Context-Aware Networking in 6G3.1 Edge Intelligence and Distributed LearningContext-aware networking in 6G can be achieved through the integration of edge computing and distributed learning techniques.Suggested approach:- Deploy edge servers with AI capabilities to process local context information- Implement federated learning algorithms for privacy-preserving context sharing among network nodes [2]",
              "3.2 Semantic CommunicationSemantic communication goes beyond traditional data transmission by incorporating meaning and context into the communication process.Suggested approach:- Develop semantic coding schemes that can compress information based on its contextual relevance- Implement natural language processing techniques for context extraction and interpretation",
              "Recently, increasing attention has been paid to device-free localization [83-85], i.e., theability to detect and track objects that do not communicate with the localization infrastructure or do not want to be detected and localized at all. These technologies rely on signals designed for target detection and localization (active radar) or signals emitted by other sources of opportunity (passive radar) that are used for localization. Unlike UE localization, device-free localization can use any modulated signal at any operating frequency. As the wireless industry moves toward frequencies above 90 GHz (and eventually Terahertz frequencies) in the future, several Gigahertz wide frequency ranges will become available."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should briefly discuss positioning methods used indoors.",
            "weight": 0.05454545454545454,
            "evidence": [
              "The lack of service coverage of GNSSs in indoor environments has generated a richresearch activity on the design of indoor localization solutions in the last two decades.Some solutions exploit acoustic, infrared, laser, inertial, and vision technologies,whereas others are based on measurements of specific features of radio signals (e.g.,TOA, RSSI, etc.) [11]. In the context of radio-based positioning technologies, research efforts followed two main directions: exploitation of existing standards designed only for communication; and design of ad-hoc standards/solutions for positioning. Recently, particular emphasis has been given to technologies for IoT applications which typically use low-cost, low-complexity, and low-energy devices."
            ]
          }
        ]
      }
    },
    "case_id": "f0e4b81f93c36652a08a8160e12c9922",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "Have large langauge models been applied to the schema matching problem in databases and are they effective?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "Have large langauge models been applied to the schema matching problem in databases and are they effective?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should confirm that LLMs have recently been used in schema matching problems, requiring only minimal fine-tuning and demonstrating promising results.",
            "weight": 0.10909090909090909,
            "evidence": [
              "TLDR: Large Language Models (LLMs) have been applied to schema matching tasks in databases, showing promising initial results. Their ability to understand semantics and context makes them potentially powerful tools for various data integration challenges.Recent research has explored the application of Large Language Models (LLMs) to schema matching and related data integration tasks in databases. The use of LLMs for these purposes is still in its early stages, but initial results are promising  (1, Matentzoglu et al., 2023)  (2, Mishaeli et al., 2024). LLMs' capability to understand and interpret the semantics and context of data schemas makes them particularly well-suited for schema matching tasks  (2, Mishaeli et al., 2024).",
              "Our study shows that LLMs have potential in bootstrapping the schema matching process and are able to assist data engineers in speeding up this task solely based on schema element names and descriptions without the need for data instances.",
              "A benefit of LLMs over the string similarity baseline is that they can be instructed to provide an explanation as to why they identify a certain attribute pair as a match or a non-match. We believe that such explanations can be a valuable instrument for a data engineer tasked to construct a schema mapping, to identify and rectify misclassifications."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should discuss some of the LLMs used for schema mapping by quoting relevant works.",
            "weight": 0.10909090909090909,
            "evidence": [
              "Several studies and implementations have demonstrated the effectiveness of LLMs in schema matching:3.1 BERT-based ApproachesResearchers have utilized BERT (Bidirectional Encoder Representations from Transformers) for schema matching tasks. For instance, a study by Li et al. (2020) proposed a BERT-based method for schema matching in data integration scenarios, showing significant improvements over traditional approaches [1].3.2 GPT-based SolutionsMore recent studies have explored the use of GPT (Generative Pre-trained Transformer) models for schema matching. These models have shown promise in generating schema mappings and understanding complex relationships between schema elements [2].3.3 Multi-modal ApproachesSome researchers have combined LLMs with other techniques, such as graph neural networks, to enhance schema matching performance. This multi-modal approach leverages both textual and structural information in schemas [3].",
              "Mishaeli et al. introduced ReMatch, an approach inspired by information retrieval techniques and LLMs, specifically designed for schema matching  (2, Mishaeli et al., 2024).",
              "In this paper, we aim to exploit this information and present an experimental study on schema matching using an off-the-shelf generative Large Language Model (LLM). We investigate how LLMs can be prompted to generate a set of match candidates. We focus on the use of schema documentation as the sole signal and evaluate the performance against a newly defined real-world benchmark."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss the limitations of LLMs for schema matching and suggest directions for future works.",
            "weight": 0.10909090909090909,
            "evidence": [
              "### Challenges and ConsiderationsDespite their advantages, applying LLMs to schema matching is not without challenges:1. **Computational Resources**:LLMs require significant computational resources, which might be prohibitive for smaller organizations.2. **Domain-Specific Training Data**:Fine-tuning LLMs requires substantial domain-specific data, which might not always be available.3. **Interpretability**:The decision-making process of LLMs can be opaque, making it difficult to understand why certain matches were made."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should discuss other ML-based methods used for schema matching and their limitations.",
            "weight": 0.10909090909090909,
            "evidence": [
              "Other methods rely on machine learning techniques to create models to predict if a matching candidate is a true matching or not.",
              "Also related is SMAT [13] which uses an attention-based neural network to match GLoVe embeddings [14] of schema elements, but requires a majority of the data to be labelled: 80% of the data that needs to be matched is used for training, and subsequently an additional 10% is used for tuning weights, leaving only 10% to evaluate the model. For practical applications, this presents a significant limitation, as requiring 90% of the input schemas to be labeled, amounts to almost completely matching the schemas by hand.",
              "Recent work, such as SMAT [28] and LSM [29], leverages advancements in natural language processing to achieve semantic mappings between source and target schemas. While these methods improve previous results, they still require extensive data tagging, limiting their practicality in real-world applications. Finally, the majority of previous methods dealt with schema matching as a binary classification task over A1 x A2. While simplifying training and evaluation, in normalized schemas positive labels scale by (), whereas negative labels scale by ( 2 ), which is problematic when inference is expensive, as in the case of LLMs",
              "Since learning classifiers require a training set, the user must often classify a large number of instances, which may be hard to carry out.."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should briefly define the core problem in schema matching.",
            "weight": 0.05454545454545454,
            "evidence": [
              "Schema matching is the task of finding semantic correspondences between elements (or attributes) of two given database schemas [1-5]. Such a task is essential for enabling data integration and systems interoperability in domains such as e-commerce, geospace, biology, health, etc.",
              "Schema matching [1] constitutes a core task in data integration [2]. It refers to the problem of identifying semantic correspondences between elements of two relational schemas that represent the same real-world concept. For example, a schema matching system may conclude that an attribute admittime from one table in a medical information system semantically corresponds to an attribute visit_start_date in another table. Once correspondences are identified, they can be used to translate data from the source schema into data conforming to the target schema [2], a process known as schema mapping. In this paper, we focus on schema matching."
            ]
          },
          {
            "name": "nice_to_have_item_1",
            "criterion": "The answer should discuss the challenges behind manual schema matching.",
            "weight": 0.05454545454545454,
            "evidence": [
              "The schema matching task is challenging for many reasons. First, schema elements, e.g., attributes representing the same concept, may have different names in different schemas. On the other hand, elements with similar names may refer to distinct concepts. Also, equivalent elements in two schemas may have a different structure. Finally, there may be the case in which many elements from one schema represent a concept represented by a single element in the other schema.",
              "To generate match candidates, a wide variety of signals that hint at element correspondence have been considered in the research literature. These include syntactic similarity of attribute names; consulting thesauri; looking at data values and their distributions in concrete database instances; and exploiting database constraints [1, 3, 4, 5]. Unfortunately, many such signals remain unavailable in real-world schemas [6]: attribute names are often cryptic and involve domain-specific abbreviations not occurring in thesauri. Use of actual data values and concrete database instances may be restricted for legal reasons; e.g., this is the case in the health domain where real database instances are problematic to obtain due to privacy constraints.",
              "Traditionally, the schema matching task is performed manually by specialists with extensive knowledge about the schemas and their domain. However, even for a specialist, this task may be time-consuming, costly, and error-prone."
            ]
          },
          {
            "name": "nice_to_have_item_2",
            "criterion": "The answer should mention schema matching network as a more complex version of binary schema mapping.",
            "weight": 0.05454545454545454,
            "evidence": [
              "As the problem evolved, schema matching tasks have appeared in settings where more than two data sources (e.g., database, query forms) need to be matched [9,16-20], as if they compose a network of schemas. The task that involves more than two schemas is known as the schema matching network task."
            ]
          }
        ]
      }
    },
    "case_id": "0919a8528cd20166163de3fdcb089efa",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are leading approaches for evaluating complex scientific question answering systems in NLP?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are leading approaches for evaluating complex scientific question answering systems in NLP?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should provide a brief definition of complex scientific question answering systems.",
            "weight": 0.10909090909090909,
            "evidence": [
              "To address the limitations of current technology, researchers have begun to explore techniques for answering more complex questions such as the following: * Who is Aaron Copland? * How have South American drug cartels been using banks in Liechtenstein to launder money? * What was the Pentagon panel's position with respect to the dispute over the US Navy training range on the island of Vieques? The first is an example of a so-called \"definition\" question, where the goal is to generate a profile of a person, entity, or event that integrates information from multiple sources within a given text collection. The second is an example of a \"relationship\" question, focused on the ties (economic, familial, etc.) between different entities. The last is an example of a so-called \"opinion\" question, which might involve sentiment detection and analysis of language use.",
              "Natural Language Processing (NLP) for scientific question answering (QA) is a challenging task that requires specialized approaches for effective evaluation. These systems must comprehend and reason over intricate and domain-specific content, often requiring more sophisticated evaluation metrics and methodologies than general QA systems. Below are the leading approaches for evaluating such systems."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should list some of the benchmark datasets for evaluating complex scientific question answering systems in NLP.",
            "weight": 0.10909090909090909,
            "evidence": [
              "The evaluation of complex scientific question answering systems relies heavily on diverse benchmark datasets. These datasets serve as standardized measures to assess the performance and capabilities of QA models across various domains and task types. Here is a list of notable benchmark datasets:1. Traditional QA Benchmarks:- Natural Questions- TriviaQA- WebQuestions- SearchQAThese datasets have been widely used but are now considered largely solved by modern large language models  (104, Chau et al., 2024).2. Comprehensive Science QA Benchmarks:- ScienceQA- C-EVAL- AGIEVAL- MMMU- SciBenchThese more recent benchmarks provide a broader scope of assessment for scientific QA tasks  (105, Zhou et al., 2024)."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should list important performance metrics for evaluating question answering systems.",
            "weight": 0.10909090909090909,
            "evidence": [
              "The evaluation of complex scientific question answering (QA) systems in natural language processing (NLP) employs a variety of metrics and methods, each with its own strengths and limitations. Traditional evaluation approaches often rely on lexical matching techniques, such as exact match (EM) and F1 scores  (1, Hashimoto et al., 2019)  (86, Jiang et al., 2023). These metrics, along with top-n-accuracy, have been widely used due to their simplicity and ease of implementation  (57, Risch et al., 2021). However, as QA tasks become more complex and require more nuanced responses, the limitations of these traditional metrics have become apparent. Researchers have noted that simple lexical matching can be easily gamed and may not accurately reflect the quality of generated answers  (53, Krishna et al., 2021). This has led to a growing interest in more sophisticated evaluation approaches.",
              "Overall, these scores can be broadly classified into two categories, see Figure 8: (i) Human Centric Evaluation Score (HCES) (ii) Automatic Evaluation Score (AES)",
              "HCES is considered the best evaluation that gives the most trustworthy score. Typically, HCES relies on a group of people (e.g. experts or specialists) to evaluate the output of a QA system based on certain guidelines and criteria, e.g., regarding adequacy, fluency, and coherence of a text. However, it is also prone to human errors caused by subjectivity and bias in opinion. Frequently, a 5-point Springer Nature 2021 LATEX template 26 Article Title Likert-scale [164] or numerical ranking criteria is used as a measurement tool for HCES evaluations [165]. Conceptually, HCES can be further classified into two sub-categories: Absolute evaluation and relative evaluation.",
              "Unlike HCES, the AES are cheaper evaluation techniques (e.g., do not require domain experts as the HCES do) and found to be more practical when dealing with a large task that may involve some dynamic behavior [171]. Importantly, such scores are usually correlated well with human judgments [162]. AES can be further sub-categorized into: context-based and context-free evaluation",
              "Both context-free and context-based AES scores can be further subcategorized into: (i) Untrained Automatic Evaluation Score (UAES) (ii) Machine-Trained Evaluation Score (MTES)",
              "A UAES does not require any pre-training because such scores do not depend on pre-defined parameters. The lack of parameters makes UAES easy to use, so it becomes very popular. Such category can be further sub-categorized based on the operation level among the text units, which can be: * Character-based * Word-based * Embeddings-based",
              "In contrast to UAES, a MTES contains adjustable components, i.e., parameters, which are specifically estimated for a given task via a learnable model. That means an MTES is a parametric model which needs to be trained based on data. MTES can be classified into two sub-categories: (i) Composite measures (ii) Holistic measures"
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should discuss some of the proposed automatic evaluation methods using Machine-Trained Evaluation Scores proposed in the literature.",
            "weight": 0.10909090909090909,
            "evidence": [
              "Composite measures: This category of error scores combines features using a parametric model whereas the features can correspond to any score in the categories S-UAES or A-UAES, e.g, precision, recall, or BLEU. Typically, a large number of such heuristic scores are used. For combining those features either simple parametric models are used, e.g., linear regression (see BEER [184] or BLEND [185]), or complex models based on neural networks (see [186]).",
              "Holistic measures: This category of error scores utilizes end-to-end models based on neural networks, e.g., transformers, to perform the evaluation. The end-to-end models can be directly applied to statistical or contextualized features that exist within the response or reference to compute the scores. Such models may use different strategies to perform the evaluation process: (i) referenced scores, which compare the generated response with a provided gold answer, by learning an alignment score between context and response to approximate human judgments [170], (ii) hybrid referenced-unreferenced evaluation score [187], where the score is trained without needs for human responses by smoothing the negative samples directly from the dataset, or (iii) unreferenced scores (i.e., learned in an unsupervised manner) that uses large pre-trained language models to extract latent representations of text units (i.e., utterances or words), and take advantage of the time shifts that exist between them [188]. The MTES based on end-to-end models include a wide range of error scores such as BertScore[183], RUSE [189], RUBER [187], BLEURT [190], ADEM [170], MaUde [188].",
              "AVA uses Transformer-based language models to encode question, answer, and reference texts. This allows for effectively assessing answer correctness using similarity between the reference and an automatic answer, biased towards the question semantics."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should discuss some other methodologies for evaluating QS systems such as adversarial testing, cross-domain evaluation, and logical consistency.",
            "weight": 0.10909090909090909,
            "evidence": [
              "### Adversarial TestingAdversarial examples can be used to test the robustness of scientific QA systems. This involves presenting the system with tricky, ambiguous, or intentionally misleading questions to evaluate its resilience and accuracy in less-than-ideal scenarios.",
              "### Cross-Domain EvaluationEvaluating the system's performance across different scientific domains can provide insights into its generalizability. A robust scientific QA system should maintain high performance when subjected to diverse topics and types of scientific literature.",
              "### Explanation Quality: Given the importance of explanation in scientific reasoning, evaluating how well a QA system can justify its answers is crucial. This can be assessed through:**Rationales and Justifications**: Evaluating the quality and relevance of the explanations provided by the system for its answers.**Chain-of-Thought Analysis**: Examining the system's reasoning process, which can provide insights into its understanding and inference capabilities [4].",
              "### Causality and Logical ConsistencyFor scientific questions often requiring causal relationships and logical consistency, specific evaluation criteria include:**Causal Inference**: How well the system can extract and reason about causal relationships within the text [5].**Consistency Checks**: Assessing whether the system's responses are logically consistent with known scientific knowledge and principles."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should discuss some general guidelines for assessing the response to complex problems.",
            "weight": 0.05454545454545454,
            "evidence": [
              "By viewing complex questions as requests for collections of facts that address a particular information need, it becomes possible to characterize the properties of a \"good\" answer. From this basic idea, one can develop the desiderata for an evaluation metric: * Answers that contain more relevant facts are preferred over answers that contain fewer relevant facts. * Shorter answers are preferred over longer answers containing the same number of facts. Verbosity should be punished. * Some facts are more important than others, and this should be reflected accordingly."
            ]
          }
        ]
      }
    },
    "case_id": "948b6cb986a5d7732722975dbed9d420",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "For different modulation types like MPSK, QPSK, and MQAM, I'm measuring the error rate but want to understand and measure the Error Vector Magnitude (EVM) more accurately. I'm using a general equation for all modulation types. Is this the right approach, and what should I expect from the EVM vs. Eb/No curve?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "For different modulation types like MPSK, QPSK, and MQAM, I'm measuring the error rate but want to understand and measure the Error Vector Magnitude (EVM) more accurately. I'm using a general equation for all modulation types. Is this the right approach, and what should I expect from the EVM vs. Eb/No curve?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should formally define Error Vector Magnitude (EVM).",
            "weight": 0.08,
            "evidence": [
              "Error Vector Magnitude (EVM) is a widely used metric for measuring the performance of digital communications systems. It quantifies the difference between the ideal transmitted signal and the actual received signal. Mathematically, it is the root mean squared (RMS) error between these two signals in a complex vector space. EVM is expressed in percentage or decibels (dB).EVM reflects all kinds of distortions within a communication system, including noise, non-linearities, phase noise, and other impairments.",
              "Typically, the EVM for a received signal \\( r \\) with respect to an ideal signal \\( s \\) is calculated as:\\[ EVM = \\sqrt{\\frac{\\sum_{i=1}^{N} |r_i - s_i|^2}{\\sum_{i=1}^{N} |s_i|^2}} \\]Here:- \\( r_i \\) is the received symbol.- \\( s_i \\) is the ideal transmitted symbol.- \\( N \\) is the total number of symbols."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention that in order to be able to use EVM for all modulation types, normalization should be utilized.",
            "weight": 0.08,
            "evidence": [
              "Since different modulation systems viz. BPSK, 4-QAM, 16-QAM etc. have different amplitude levels, to calculate and compare EVM measurements effectively some normalization is typically carried out [7]. The normalization is derived such that the mean square amplitude of all possible symbols in the constellation of any modulation scheme is one. Thus, EVM is defined as the root-mean-square (RMS) value of the difference between a collection of measured symbols and ideal symbols."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should mention that EVM is a valid representation of bit error rate (BER), and discuss its advantages and disadvantages compared to BER.",
            "weight": 0.08,
            "evidence": [
              "RMS value of error vector magnitude is also a direct function of the bit error rate. Since error vector magnitude can be directly measured from the down converted signals using vector signal analyzers, it can save the extra calculations that may be required to find out the bit error rates. In many adaptive systems, this can also simplify the cost function calculation greatly.",
              "EVM is closely related to the bit error rate (BER) of a given system. As the receivedsymbols fall far from the target constellation point, the probability of them falling within the decision boundary of another constellation point increases. Thistranslates to a larger BER. One important distinction between BER and EVM is thatthe BER for a transmitted signal is calculated based on the transmitted bit pattern,while the EVM is calculated based on the distance of the symbol's closest constellation point and the symbol location. In some cases, symbols may cross the decisionboundary and are assigned an incorrect bit pattern. If the symbol falls closer toanother ideal symbol location, this may result in a better EVM for that symbol.Therefore, while EVM and BER are closely related, this relationship may not hold forvery high levels of signal distortion."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should discuss that while using the general EVM calculation approach should be effective across different modulation schemes. However, noise models and distortion should be taken into account for measuring EVM accurately..",
            "weight": 0.08,
            "evidence": [
              "1. **Noise Models**: Ensure the noise in your system is appropriately modeled. AWGN (Additive White Gaussian Noise) is commonly assumed, but real-world systems might have other noise types.2. **Distortion Components**: Identify all possible sources of distortion in the system (phase noise, frequency drift) to measure EVM accurately."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should define Eb/No and discuss its relation to EVM and BER.",
            "weight": 0.08,
            "evidence": [
              "\\[ Eb/No \\] stands for the ratio of Energy per Bit to Noise Power Spectral Density. It is a fundamental metric for analyzing the performance of communication systems. Higher Eb/No values typically correspond to better signal quality and lower error rates.",
              "Eb/No is the measure of signal to noise ratio for a digital communication system. It ismeasured at the input to the receiver and is used as the basic measure of how strongthe signal is. Different forms of modulation -- BPSK, QPSK, QAM, etc. -- havedifferent curves of theoretical bit error rates versus Eb/No as shown in Figure 1.These curves show the communications engineer the best performance that can beachieved across a digital link with a given amount of RF power."
            ]
          },
          {
            "name": "most_important_item_5",
            "criterion": "The answer should mention that, under specific assumptions, EVM is generally inversely proportional to Eb/No, and has a similar curve across different modulation types due to normalization.",
            "weight": 0.08,
            "evidence": [
              "Extended relationships among the bit error rate, signal to noise ration and error vector magnitude are shown in this paper. Due to normalization, the EVM is the same for a given SNR, and they maintain an inverse relationships between them",
              "When relating EVM to SNR, one assumption that has been made is that the EVM is measured using known data sequences (e.g. preambles or pilots) or that the SNR is high enough that symbol errors are negligible [9, 10]. Another assumption was that one or two system imperfections are dominant; the remaining imperfections are modeled as Gaussian noise [3, 8]."
            ]
          },
          {
            "name": "most_important_item_6",
            "criterion": "The answer should discuss that the exact shape of EVM vs. Eb/No curve may depend on factors such as system architecture, and the presence of interference or distortion.",
            "weight": 0.08,
            "evidence": [
              "When analyzing the EVM vs. Eb/No curve, one should expect to see a decreasing trend in EVM as Eb/No increases. However, the exact shape and characteristics of this curve may vary depending on factors such as modulation type, system architecture, and the presence of interference or distortion (LLM MEMORY, 2024)."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should discuss the advantages of relating different signal performance metrics.",
            "weight": 0.04,
            "evidence": [
              "Relating EVM to other performance metrics such as SNR and BER is an important research topic as well [9, 10]. These relations are quite useful since it allows the reuse of already available EVM measurements to infer more information regarding the communication system. Moreover, using EVM measurements could reduce the system complexity by getting rid of the need to have separate modules to estimate or measure other metrics."
            ]
          }
        ]
      }
    },
    "case_id": "dc761baf0fc9fead9889d9eb0a258f0e",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What is the difference between FMRI datasets and MRI datasets in terms of data analysis?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What is the difference between FMRI datasets and MRI datasets in terms of data analysis?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define MRI and fMRI and discuss their applications.",
            "weight": 0.12,
            "evidence": [
              "### MRI (Magnetic Resonance Imaging)MRI is a non-invasive imaging technology that produces highly detailed anatomical images of the body's internal structures, particularly soft tissues. It leverages strong magnetic fields and radio waves to produce detailed images. Typically, MRI data is used to study anatomical features.### FMRI (Functional Magnetic Resonance Imaging)FMRI extends traditional MRI techniques to measure and map brain activity. This is accomplished by detecting changes in blood flow and oxygenation, which correlate with neuronal activity. Specifically, FMRI captures Blood Oxygen Level Dependent (BOLD) contrasts.",
              "## Applications- **MRI**: Commonly used in clinical diagnostics, structural brain analysis, tumor detection, and studies related to brain anatomy.- **FMRI**: Widely used in cognitive neuroscience to understand brain function, brain mapping, studying resting-state connectivity, and psychiatric research."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should highlight that MRI datasets include static images of the brain while fMRI datasets include a sequence of MRI images and hence are dynamic and more complex.",
            "weight": 0.12,
            "evidence": [
              "2.3 From MRI to fMRI: The data acquisition and reconstruction techniques outlined in this section provide the means for obtaining a static image of the brain. However, changes in brain hemodynamics in response to neuronal activity impact the local intensity of the MR signal. Therefore, a sequence of properly acquired brain images allows one to study changes in brain function over time. An fMRI study consists of a series of brain volumes collected in quick succession. The temporal resolution of the acquired data will depend on the time between acquisitions of each individual volume.",
              "It quickly becomes clear that fMRI data analysis is a time series analysis problem of massive proportions.",
              "### Temporal Aspects- **MRI**: Generally involves static images with no temporal dimension to consider.- **FMRI**: Requires analysis of temporal changes in signal intensity to derive brain activity patterns. Dynamic connectivity and time-series analysis play crucial roles.",
              "### Volume and Complexity of Data- **MRI**: Produces large 3D data but with less computational demand compared to FMRI.- **FMRI**: Results in 4D datasets (3 spatial dimensions + time), significantly increasing the volume and complexity of the data. This demands more computational resources and sophisticated algorithms for analysis."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss that the spatial resolution of fMRI is less compared to MRI since it sacrifices some spatial detail for temporal information.",
            "weight": 0.12,
            "evidence": [
              "One of the benefits of MRI as an imaging technique is its ability to provide detailed anatomical scans of gray and white matter with a spatial resolution well below 1 mm3 . However, the time needed to acquire such scans is prohibitively high and currently not feasible for use in functional studies. Instead, the spatial resolution is typically on the order of 3 x 3 x 5 mm3 , corresponding to image dimensions on the order of 64x 64x 30, which can readily be sampled in approximately 2 seconds."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should highlight that fMRI data analysis is more complex than MRI and involves more complex preprocessing steps due to its temporal nature.",
            "weight": 0.12,
            "evidence": [
              "TLDR: Preprocessing is crucial for both fMRI and MRI data, but fMRI requires more complex steps due to its temporal nature. These steps aim to correct for acquisition-related issues and prepare the data for analysis.",
              "### **Preprocessing Steps**- **FMRI**:- **Motion Correction**: Essential due to the time-series nature of the data. Any movement by the subject can introduce artifacts.- **Slice Timing Correction**: Adjusts for the fact that different slices of the brain are captured at slightly different times.- **Spatial Smoothing**: Applied to enhance the signal-to-noise ratio.- **Normalization**: Aligns the data to a standard brain template to allow for group analysis.- **MRI**:- **Bias Field Correction**: Corrects for intensity inhomogeneities in the MRI images.- **Skull Stripping**: Removes non-brain tissues from the images.- **Segmentation**: Divides the brain into different tissue types (e.g., gray matter, white matter, cerebrospinal fluid).- **Normalization**: Similar to fMRI, MRI data is often normalized to a standard template for comparative studies."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should compare the data analysis techniques performed on MRI data versus fMRI data.",
            "weight": 0.12,
            "evidence": [
              "TLDR: Analysis techniques for fMRI and MRI data differ due to the nature of the data acquired. While MRI analysis focuses on structural features, fMRI analysis involves more complex methods to handle temporal and functional information.",
              "- **FMRI**:- **Functional Connectivity Analysis**: Examines the temporal correlation between spatially remote brain regions.- **Regional Homogeneity (ReHo)**: Measures the similarity or synchronization of the time series of a given voxel with its neighbors[1].- **Amplitude of Low-Frequency Fluctuations (ALFF)**: Quantifies the amplitude of low-frequency oscillations in the BOLD signal.- **Task-Based Analysis**: Identifies brain regions activated by specific tasks or stimuli.- **Resting-State Analysis**: Studies the brain's functional connectivity when the subject is not performing any task.- **MRI**:- **Voxel-Based Morphometry (VBM)**: Assesses differences in brain anatomy, such as gray matter volume.- **Diffusion Tensor Imaging (DTI)**: Analyzes the diffusion of water molecules in brain tissue to study white matter tracts.- **Surface-Based Morphometry (SBM)**: Examines the cortical surface for metrics like thickness and curvature."
            ]
          }
        ]
      }
    },
    "case_id": "5e6e212dc63d2064ba909a08576cb3ec",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "How effective are language models at automatically generating textual descriptions of scientific concepts?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "How effective are language models at automatically generating textual descriptions of scientific concepts?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should mention that LLMs have been largely used in scientific contexts for generating text due to their success in other domains.",
            "weight": 0.0923076923076923,
            "evidence": [
              "Recent advancements in large language models (LLMs) have demonstrated an impressive capability in storing and recalling world knowledge, continuously expanding the boundaries of artificial intelligence. Their exceptional performance has permeated diverse specialized domains, including the scientific domain, leading to the emergence of scientific LLMs, such as Galactica [33], SciGLM [42], and ChemLLM [43].",
              "Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%). In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Our findings suggests that LLMs are being broadly used in scientific writings.",
              "Language models have shown significant promise in automatically generating textual descriptions of scientific concepts, but their effectiveness varies depending on the specific task and domain. Here's an overview of their capabilities and limitations:## Capabilities**Scientific text generation**: Large language models (LLMs) have demonstrated the ability to generate plausible scientific text across various domains. For example:- In gene set summarization, LLMs can generate biologically valid summary term lists for input gene sets[10].- LLMs can be used to automatically generate programming exercises and code explanations for educational purposes[8].- In the field of urban renewal, fine-tuned LLMs have shown improved performance in knowledge question-answering tasks[15].**Hypothesis generation**: One of the most intriguing applications of LLMs in science is their potential for hypothesis formation:- A study using GPT-4 to generate hypotheses for breast cancer treatment discovered several drug combinations with positive synergy scores, demonstrating the model's ability to propose novel scientific ideas[11].**Knowledge summarization**: LLMs can effectively summarize scientific knowledge from various sources:- The TALISMAN approach uses generative AI to perform gene set function summarization, complementing standard enrichment analysis methods[10].- LLMs can generate detailed image descriptions and even create websites from hand-drawn drafts, showcasing their multi-modal capabilities in scientific contexts[9]."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention the name of datasets which are usually used for evaluating the performance of LLMs in the scientific domain and discuss their properties and limitations.",
            "weight": 0.0923076923076923,
            "evidence": [
              "While several existing LLM benchmarks [22, 45, 7] have incorporated scientific questions into their evaluations, and some benchmarks [31, 36, 4, 37, 24, 13] are specifically tailored for the scientific domain, we argue that the current benchmarks do not fully evaluate the potential of LLMs in scientific research due to their inherent limitations. Firstly, many existing benchmarks, such as AGIEval [45], SciQ [37], and ScienceQA [24], include science questions only up to the high school level, failing to tap into the deeper capability of LLMs",
              "Recentscientific domain benchmarks like ChemLLMBench [13], SciBench [36], and SciAssess [4], despite involving more specialized scientific tasks, lack a comprehensive evaluation system, resulting in a limited understanding of capabilities. Lastly, most benchmarks overlook the assessment of safety issues in scientific research, even those attempting a multi-dimensional comprehensive evaluation such as SciEval [31].",
              "SciKnowEval defines five progressive levels, aimed at deeply reflecting the breadth and depth of LLMs' scientific knowledge. It focuses on biology and chemistry as two representative domains, encompassing 50K multi-level problems and solutions.",
              "Inspecting existing LLM benchmarks [12, 13, 14] in science field through three-level ability assessment framework, we find that they mostly focus on Memorization (L1) - the foundational knowledge base for scientific facts - while overlooking the higher-level abilities of Comprehension (L2) and Analysis & Reasoning (L3)."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss important metrics for evaluating the performance of LLM on generative scientific questions.",
            "weight": 0.0923076923076923,
            "evidence": [
              "For generative questions, we adopted different evaluation methods tailored to the characteristics of each task. For the molecule generation task, we utilize an assessment based on the average of Exact Match and Fingerprint-based Tanimoto Similarity [41]. For molecule captioning, 6 protein captioning, protein design, and single cell analysis, we calculated the average scores using BLEU [29] and ROUGE [23] metrics. For other relatively flexible generative tasks, we designed meticulous prompts for GPT-4 to evaluate the responses of LLMs"
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should discuss the result of case studies on evaluating the performance of LLM on scientific datasets.",
            "weight": 0.0923076923076923,
            "evidence": [
              "Proprietary LLMs, such as the GPT-4 series, Gemini1.5-Pro, and Claude3- Sonnet, have demonstrated superior performance in both biology and chemistry domains, securing their highest overall rankings. Notably, GPT-4o has shown the most outstanding performance across most proficiency levels, highlighting its exceptional capability and adaptability in the scientific domains. Open-source general-purpose LLMs, including Llama3-8B-Inst and the Qwen1.5 series, have exhibited robustness across a wide range of tasks, indicating their potential for scientific research applications. While open-source scientific LLMs generally performed moderately, they have displayed strengths surpassing proprietary LLMs in certain specific tasks.",
              "Overall, GPT-4o consistently ranks high across all ability levels, particularly excelling in memorization tasks. GPT-4 and Claude3 also demonstrate strong overall performance, especially in comprehension and reasoning. GPT-3.5, despite being an earlier model, shows limitations compared to its successors. Command R + , while competitive in some areas, generally underperforms compared to other models. Based on these observations, we suggest the following recommendations: (1) For tasks heavily reliant on memorization, GPT-4o and Qwen2 are recommended due to their high accuracy and ranking; (2) For comprehension tasks, particularly those involving complex data extraction and generation, GPT-4o is ideal choices. (3) For analysis and reasoning tasks, Claude3 and GPT-4 provide reliable performance and should be considered."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should discuss some of the limitations of LLMs in scientific domains.",
            "weight": 0.0923076923076923,
            "evidence": [
              "TLDR: Despite their impressive capabilities, language models face significant challenges in scientific domains. These include difficulties with specialized knowledge, abstract reasoning, and providing comprehensive information beyond given prompts.- Language models, while proficient in generating human-like text for general applications, often struggle with specialized scientific topics, particularly in fields like mathematics and chemistry  (130, Fauber, 2024).- In biomedical applications, scientific language models (SLMs) can process texts containing both human language and chemical structures. However, they perform poorly in zero-shot activity prediction tasks, indicating limitations in their ability to generalize scientific knowledge  (55, Seidl et al., 2023).- Large language models (LLMs) tend to generate texts that overly focus on given prompts, often failing to provide sufficient background and detailed information compared to human-generated content. This \"over-concentrated information\" problem limits their ability to produce comprehensive scientific descriptions  (112, Zhao et al., 2023).- Current LLMs face challenges in comprehending and utilizing abstraction knowledge, especially in zero-shot and few-shot settings. While training on rich abstraction knowledge can improve their performance, generalization to unseen events remains a significant hurdle  (79, Fang et al., 2023).- In the biomedical domain, even the best-performing models struggle to produce acceptable explanations of biomedical mechanisms, succeeding in only 32% of instances. This highlights the difficulty LLMs face in understanding and explaining complex scientific concepts  (36, Balasubramanian et al., 2022)."
            ]
          },
          {
            "name": "most_important_item_5",
            "criterion": "The answer should discuss existing concerns regarding the use of LLMs in scientific contexts.",
            "weight": 0.0923076923076923,
            "evidence": [
              "Measuring the extent of LLM-use on scientific publishing has urgent applications. Concerns about accuracy, plagiarism, anonymity, and ownership have prompted some prominent scientific institutions to take a stance on the use of LLM-modified content in academic publications. The International Conference on Machine Learning (ICML) 2023, a major machine learning conference, has prohibited the inclusion of text generated by LLMs like ChatGPT in submitted papers, unless the generated text is used as part of the paper's experimental analysis (ICML, 2023). Similarly, the journal Science has announced an update to their editorial policies, specifying that text, figures, images, or graphics generated by ChatGPT or any other LLM tools cannot be used in published works (Thorp, 2023). Taking steps to measure the extent of LLM-use can offer a first-step in identifying risks to the scientific publishing ecosystem."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should discuss the applications of LLMs in scientific domain.",
            "weight": 0.04615384615384615,
            "evidence": [
              "Despite LLMs not yet fully replacing scientific researchers in generating creative discoveries, they have demonstrated substantial potential in enhancing researchers' efficiency in scientific literature analysis [9]. Specific applications such as automatic literature summarization and knowledge extraction have seen practical deployments, significantly boosting researchers' productivity and expanding the range of literature that can be effectively utilized [10]",
              "Applications in Scientific WritingTLDR: Language models are increasingly being applied to various aspects of scientific writing, from literature reviews to hypothesis generation. These tools show promise in automating and enhancing multiple stages of the scientific writing process, though their outputs still require human oversight.- Language models can assist in automating various aspects of scientific writing, including generating text for research papers, abstracts, and literature reviews  (131, Olshevska et al., 2024).- LLMs demonstrate capabilities in summarizing research literature, completing literature review tasks, and even creating full literature review articles. They also show potential in peer review processes, with their evaluations often aligning with those of human experts  (132, Research et al., 2024).- These models can be used to automatically evaluate the quality of generated texts, potentially streamlining the review process for scientific writing  (137, Tian et al., 2024).- LLMs have the potential to contribute significantly to scientific hypothesis generation by interpreting and exploiting current knowledge in a given scientific domain, effectively digesting relevant literature  (78, Abrahao et al., 2023)."
            ]
          }
        ]
      }
    },
    "case_id": "15dec998cf77887f870ebf9a55bb7e89",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "How are user demonstrations represented in domain-specific-language programs and used for automation tasks?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "How are user demonstrations represented in domain-specific-language programs and used for automation tasks?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define domain-specific-language programs and discuss their advantages compared to general programming languages.",
            "weight": 0.0923076923076923,
            "evidence": [
              "Domain-specific languages (DSLs) are a proven approach to bring programming closer to application domains. DSLs focus on specific application domains, strive for presenting software in the notations of the domain experts, and allow straightforward mapping of application concepts to software solutions. They have the capability to significantly improve the productivity and quality of software engineering in the focused domain. The ultimate goal of a DSL approach is to empower domain experts and end users by providing languages and tools that enable them to realize and adapt software systems by themselves"
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should explain some of the most common ways user demonstrations can be represented using DSL in an abstract and simple manner.",
            "weight": 0.0923076923076923,
            "evidence": [
              "A domain-specific language provides an abstraction mechanism to deal with complexity in a given domain. This is done by providing concepts and rules within a language that represents things in the application domain, rather than concepts of a given programming language. In that, it allows expressing problems of the given domain in a more natural and intuitive way, raises the level of abstraction in the given domain, and brings programming closer to the domain experts.",
              "DSLs are programming languages that are tailored to a specific problem domain, such as finance, healthcare, or manufacturing. They use terms and concepts that are familiar to the domain experts, rather than generic programming constructs. For example, a DSL for invoice processing might have commands like create_invoice , send_invoice , or record_payment , rather than if , while , or print . DSLs can be either textual or graphical, depending on the preference and skill level of the users.",
              "TLDR: User demonstrations for automation tasks come in various forms, including natural language instructions, visual interactions, and physical actions. These demonstrations can range from simple input-output examples to complex sequences of actions, allowing both expert and non-expert users to teach machines new skills across diverse domains.1. Natural Language Instructions: Users can provide demonstrations through natural language descriptions, which are then translated into executable code or programs [35, 86]. This approach allows naive users to teach robots a wide variety of novel tasks without requiring programming expertise [35].2. Input-Output Examples: Demonstrations can be presented as pairs of input and expected output, often in a specific syntax that language models can parse and learn from [102, 110]. This method is particularly useful for teaching language models to perform specific tasks or use new tools [101].3. GUI Interactions: Users can demonstrate tasks by interacting with graphical user interfaces (GUIs) of applications. These demonstrations capture mouse-keyboard signals and screenshots, which are then used to synthesize automation scripts [24, 26, 49]. Systems like VASTA use computer vision techniques to label these interactions accurately [46].4. Mobile App Interactions: Some systems, like PUMICE, allow users to define concepts and teach new procedures through demonstrations with mobile apps, enabling end-users to create custom automations [42].5. Web Browser Actions: Demonstrations can involve chains of actions performed on web browsers, with each state representing the entire webpage document object model (DOM) [112]. These demonstrations can be used to generate prompts for language models to learn web-based tasks."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss why DSL is a good choice for task automation.",
            "weight": 0.0923076923076923,
            "evidence": [
              "DSLs can provide a variety of advantages for process automation, such as making the automation logic more readable and understandable through domain-specific terminology and syntax. Furthermore, they can decrease development time and effort by abstracting away low-level details and focusing on high-level goals and requirements. Additionally, DSLs enable domain experts, such as business analysts, accountants, or engineers, to join in on the automation design and validation without requiring any programming knowledge or training. Moreover, they can make it easier to maintain and evolve automation solutions since they can be easily modified and extended to fit changing needs and preferences."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should discuss how user representation can be transformed into executable actions for automation.",
            "weight": 0.0923076923076923,
            "evidence": [
              "4. Using User Demonstrations for Automation TasksOnce captured and represented, user demonstrations can be utilized for various automation tasks:4.1 Code GenerationDemonstrations are often used to generate executable code in the target DSL. This process involves translating the demonstration representation into syntactically correct DSL code.Example (in a hypothetical DSL for web automation):```pythonbrowser.navigate(\"https://example.com\")form = browser.find_element(id=\"login-form\")form.input(name=\"username\").type(\"user123\")form.input(name=\"password\").type(\"password123\")form.submit()```4.2 Macro CreationUser demonstrations can be used to create macros or scripts that automate repetitive tasks. These macros can be executed on demand or scheduled to run automatically.4.3 Program SynthesisMore advanced systems use demonstrations as input for program synthesis algorithms, which attempt to infer general programs from specific examples [1].4.4 Interactive RefinementSome systems allow users to interactively refine the generated code or automation scripts, combining the benefits of demonstration and manual programming.",
              "One approach to this conversion is the use of large language models (LLMs). Practitioners can now provide prompts with instructions and demonstrations, which LLMs can use to generate desired outputs or executable code  (108, Zhao et al., 2023). This method bridges the gap between natural language descriptions and programmatic representations of tasks."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should mention the name of some DSLs used for automation with their application domain.",
            "weight": 0.0923076923076923,
            "evidence": [
              "Using DSLs for process automation involves writing, executing, and monitoring the code that defines the automation logic and behavior.",
              "There are numerous examples of DSLs for process automation, both in the academic and industrial contexts, such as Business Process Model and Notation (BPMN), Decision Model and Notation (DMN), Robot Framework, and Ansible. BPMN is a graphical DSL for modeling and executing business processes, like order fulfillment or customer service. DMN is a graphical DSL for modeling and executing business decisions such as pricing or eligibility. Robot Framework is a textual DSL for automating web, desktop, and mobile applications using keywords and data tables. Ansible is a textual DSL for automating the configuration and deployment of servers, applications, and networks using tasks and roles.",
              "2.7.2 CAST: Automated Software Tests for Embedded Systems Wahler [9] introduces CAST (Computer-Aided Specification and Testing) an approach to tests automation in embedded systems. CAST consists of three parts, a DSL named as TESLA (TEst Specification LAnguage) which allows specifying test cases using familiar syntax, a test execution engine which allows executing tests either automatically or with human interaction and an interface which is a form of connection between engine and embedded systems.",
              "2.7.3 Habitation: A DSL for Home Automation Home automation uses MDE (Model-Driven Engineering) approach in reactive systems. It offers management of energy, security and communications through interaction with the environment.",
              "2.7.4 A Domain-Specific Language for Ubiquitous Healthcare Aspect Language for Pervasive Healthcare (ALPH) is a domain-specific language in ubiquitous healthcare domain.",
              "2.7.5 Domain Specific language for Cellular Interactions CellSys is a DSL embedded in Haskell (GPL) specific to bioinformatics domain, is used to model life cycle of microorganisms like bacteria. The objective of this DSL is to allow biologist to create a model which can describe complex interactions between tissues and organisms with abstraction and accuracy, visualize organism's development by executing these models, help language user to improve understanding of organism's behaviour and structure by suggesting refinements and compare cellular system's models between 29 different organisms or stages of development of an organism."
            ]
          },
          {
            "name": "most_important_item_5",
            "criterion": "The answer should discuss the limitations of DSL for user representation and automating tasks such as generalizability, and the difficulty of building and debugging DSLs.",
            "weight": 0.0923076923076923,
            "evidence": [
              "Regardless of the lower final cost of the overall development, a higher starting price of the application development is often pointed out as a disadvantage. Developing application that involves building appropriate DSL is a hard process that requires programmers to be language experts as well. In such International Journal of Computer Applications (0975 - 8887) Volume 115 - No. 2, April 2015 43 cases the creation of DSL requires complete knowledge of domain constraints. Debugging and unit testing is hard to perform when DSL is used in implementation. DSLs can lead to language cacophony. Proper selection of DSLs and adequate usage is crucial.",
              "Furthermore, every language workbench has some restrictions with respect to the notation (e.g. textual, visual, tabular) resulting in some compromises (gap) between the representation of domain concepts in the DSL and the expectation of the domain expert. If this gap is too large, the acceptance of the DSM tool by end-users is not given any more and the approach as a whole will fail",
              "Several challenges arise when using user demonstrations in DSL programs:5.1 GeneralizationDemonstrations often capture specific instances of a task, but automation requires generalization to handle various scenarios.5.2 Error HandlingAutomated scripts need to handle errors and edge cases that may not have been present in the original demonstration.5.3 MaintainabilityAs systems evolve, demonstrations may become outdated and require updates or re-recording.5.4 Privacy and SecurityCapturing user demonstrations may involve sensitive data, requiring careful handling and protection."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should compare the expressiveness of text-based DSLs with graphical DSLs.",
            "weight": 0.04615384615384615,
            "evidence": [
              "There are many advantages of text-based modelling over graphical modelling for the user of DSL: e.g. it takes more space for graphical models to represent some information which is time consuming, writing and printing text is easy while for graphical models the size of graph can exceed the size of paper. During development process sometimes things can be described more efficiently by using text instead of drawing models like conditions and actions. Formatting text is easier and results of automatic algorithms are of good quality. Writing, reading, modifying text does not need any specific platform and can be done almost in every text editor. No additional tools or plug-ins are required.",
              "Text-based models also have some disadvantages like graphics are more intuitive to give first orientation which is slightly compensated by text-based models by giving outline of code in the form of list or tree. Simulation and animation is more easy using graphics [11]."
            ]
          }
        ]
      }
    },
    "case_id": "3d1ff2a80239a37908a6f4c4abc04486",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "How can I use an hybridization of ontology and machine learning for text summarization ?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "How can I use an hybridization of ontology and machine learning for text summarization ?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define ontology.",
            "weight": 0.10909090909090909,
            "evidence": [
              "Ontology is defined as a formal and explicit specification of a shared conceptualization. Generally, ontologies are defined for particular domains. Since information extraction is essentially concerned with the task of retrieving information for a particular domain, formally and explicitly specifying the concepts of that domain through an ontology can be helpful to this process. Ontology together with a set of individual instances of classes constitutes a knowledge base [3]",
              "### What is an Ontology?An ontology in the context of computer science and information science is a formal representation of a set of concepts within a domain and the relationships between those concepts. It typically includes:- **Classes**: Categories of objects or concepts.- **Properties**: Characteristics or attributes of the classes.- **Relationships**: How classes and properties relate to one another.Ontologies are used to encode domain knowledge so that it can be utilized by computational systems to improve understanding and processing of information."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should briefly explain the motivation behind the use of ontology for text summarization.",
            "weight": 0.10909090909090909,
            "evidence": [
              "Ontology has been used in different domains for providing metadata of concepts and their relationships. This metadata can be used for ensuring interoperability among different systems, modeling contextual information, inferencing, reasoning and efficient searching of contents and resources.",
              "2.1. Reasons for developing Ontology x Sharing common understanding of the structure of information among people or software agents is one of the goals in developing ontologies. For example, suppose several different Web sites contain medical information or provide medical e-commerce services. If these Web sites share and publish the same underlying ontology of the terms they all use, then computer agents can extract and aggregate information from these different sites. The agents can use this aggregated information to answer user queries or as input data to other applications",
              "### Role of Ontologies in Text SummarizationOntologies can be used in text summarization to:- **Semantic Understanding**: Enhance the semantic understanding of the text by providing domain-specific knowledge.- **Relevance Filtering**: Filter out non-relevant information and ensure that the summary contains the most pertinent facts.- **Disambiguation**: Help in disambiguating terms that might have multiple meanings by using context provided by the ontology."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should highlight the difficulty of building an ontology and discuss that in addition to human experts, ML and LLMs can be used to build ontologies automatically.",
            "weight": 0.10909090909090909,
            "evidence": [
              "Creating a topic-oriented ontology (TOO) can pose a significant challenge for researchers and may require a substantial amount of time to accomplish. Despite the difculties involved in creating a TOO, researchers are still actively seeking solutions for developing it, given its potential applicability to a wide range of areas, particularly for topic-driven research.",
              "Similar to the rule-based method, this method is also time-consuming. Okumura et al. [248] proposed a Wordnet ontology in his research work. In other work, Mohan et al. [249] proposed some methods for evaluating ontology, such as; ontometric, ontoclean and evalexon. A suitable ontology preparation is a very time-consuming process and cannot be generalized to other domains.",
              "At first, our approach for constructing the TOO revolved around utilizing the SVM technique to classify the members or instances of the ontology into various groups [3]. However, we found that this approach was not optimal due to the complex computations involved and the extensive eforts required to prepare the training dataset. As a solution to these issues, we turned to the tools of natural language processing to construct the TOO.",
              "The conventional process of building Ontologies and Knowledge Graphs (KGs) heavily relies on human domain experts to define entities and relationship types, establish hierarchies, maintain relevance to the domain, fill the ABox (or populate with instances), and ensure data quality (including amongst others accuracy and completeness). On the other hand, Large Language Models (LLMs) have recently gained popularity for their ability to understand and generate human-like natural language, offering promising ways to automate aspects of this process. This work explores the (semi-)automatic construction of KGs facilitated by open-source LLMs."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should list potential ways ontology can be integrated with ML for text summarization. Also, it should mention that the way ontology can be used together with ML for summarization should be similar to the way it is used for other information retrieval tasks.",
            "weight": 0.10909090909090909,
            "evidence": [
              "Use an IE process guided by an ontology: we believe that 'guide' is a suitable word to describe the interaction between the ontology and the IE process in an OBIE system. In all OBIE systems,the IE process is guided by the ontology to extract things such as classes, properties and instances.This means that no new IE method is invented but an existing one is oriented to identify the components of an ontology."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should mention that in one case study for news summarization, text summarization based on ML has been assisted by ontology for information extraction.",
            "weight": 0.10909090909090909,
            "evidence": [
              "The search engine collects all domain information from web document to the information extractor extracts all the information from those documents and submits it to the ontology. From the ontology all domain related documents are clustered separately with the related domain names. The information extractor again extracts all needed information from the ontology clusters and passes it to the multi document summarizer.",
              "The summarizes all the important relevant information from multiple documents without any redundant data and stores it into the database. From the database according to user query the query extractor extracts the data and submits it to the user. The user will give the query in the search engine. The query extractor extracts the given query and hits the database to search and fetch relevant data's for the given query. Once the data's are fetched from the database the query extractor passes those query to the report generator. The report generator generates report from the extracted data and produces only relevant abstract documents and posts it to the user."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should mention some of the ML models used for text summarization.",
            "weight": 0.05454545454545454,
            "evidence": [
              "## Machine Learning in Text Summarization\u000b### Supervised and Unsupervised Learning- **Supervised Learning**: Involves training summarization models on a labeled dataset, where the input text and corresponding summarized output are provided. Popular algorithms include sequence-to-sequence models with attention mechanisms, such as BERT, GPT, and T5.- **Unsupervised Learning**: Techniques that do not rely on labeled datasets. Methods like extractive summarization using clustering algorithms or latent semantic analysis fall into this category.### Sequence-to-Sequence (Seq2Seq) ModelsSeq2Seq models are a type of neural network particularly effective for text generation tasks, including summarization. They consist of two main parts: an encoder that processes the input text and a decoder that generates the summary."
            ]
          }
        ]
      }
    },
    "case_id": "00bdd80debc8549198001289188c6bea",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What data link layer problems can cause to termination of a connection?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What data link layer problems can cause to termination of a connection?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define the roles/functions of the data link layer in the OSI model.",
            "weight": 0.17142857142857143,
            "evidence": [
              "1. FramingThe data packets received from the network layer are encapsulated in frames by the data-link layer for bit-to-bit sharing over the channel.It is also responsible for restructuring the framed data in the network model, and each data frame is different from the others.",
              "2. AddressingThe task of adding a physical address to the frame in the header format is known as addressing. It acts as an identification service for transmitting the frames to multiple network models over the channel.",
              "3. Flow ControlDuring data transmission, the sender or receiver's data flow may differ, causing network traffic in the channel. The Data-link layer in such situations acts as a flow control for the sender side to prevent data overflow at the receiver side.",
              "4. Access ControlIn this network model, when multiple devices share the same communication channel, this leads to data collision in the network channel. To prevent such data collision, the data-link layer performs checks on the devices with the same network channel to avoid data loss.",
              "5. Error ControlDuring data transmission, due to noise or signal loss, errors might occur in the data being transmitted. To minimize such data error rate, the data-link layer performs error detection and correction techniques on the transmitted data."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should list issues with the data link layer that commonly result in network connectivity or performance problems at wired or wireless networks, such as link failure, congestion, and interference.",
            "weight": 0.17142857142857143,
            "evidence": [
              "2. Data Link Layer (Layer 2): The data link layer is responsible for error detection and correction within individual frames of data. Issues at this layer may include collisions, frame errors, and MAC address mismatches. Troubleshooting data link layer problems typically involves checking for proper configuration of network interfaces, verifying MAC addresses, and diagnosing network congestion.",
              "- Link Failures: Communication links between nodes can experience permanent or transient failures, rendering them unavailable for data transmission  (9, Ivanov et al., 2009). These failures can be spontaneous, with links going down or coming back up at any time  (20, Bunn et al., 2013).\"- Data Corruption: Random channel errors or malicious jamming attacks can cause data corruption on communication links  (26, Cetinkaya et al., 2017). This can lead to issues such as flit corruption in network-on-chip systems  (46, Venkatesha et al., 2022).- Packet Dropouts: Along with data corruption, packet dropouts on communication links can significantly impact network reliability  (26, Cetinkaya et al., 2017).- Congestion: Network overload can cause transient congestion, resulting in increased packet latencies  (9, Ivanov et al., 2009).- Timing Issues: Asynchronous transmission times across links can vary, potentially causing timing jitter  (20, Bunn et al., 2013)  (46, Venkatesha et al., 2022).- Faulty Transceivers: Omission faults can be caused by faulty transceivers, leading to network failures  (32, Swain et al., 2019).",
              "Encapsulation errors - An encapsulation error occurs because the bits placed in a particular field by the sender are not what the receiver expects to see. This condition occurs when the encapsulation at one end of a WAN link is configured differently from the encapsulation used at the other end.",
              "Address mapping errors - In topologies, such as point-to-multipoint, Frame Relay, or broadcast Ethernet, it is essential that an appropriate Layer 2 destination address be given to the frame. This ensures its arrival at the correct destination. To achieve this, the network device must match a destination Layer 3 address with the correct Layer 2 address using either static or dynamic maps. In a dynamic environment, the mapping of Layer 2 and Layer 3 information can fail because devices may have been specifically configured not to respond to ARP or Inverse-ARP requests, the Layer 2 or Layer 3 information that is cached may have physically changed, or invalid ARP replies are received because of a misconfiguration or a security attack.",
              "Framing errors - Frames usually work in groups of 8-bit bytes. A framing error occurs when a frame does not end on an 8-bit byte boundary. When this happens, the receiver may have problems determining where one frame ends and another frame starts. Too many invalid frames may prevent valid keepalives from being exchanged. Framing errors can be caused by a noisy serial line, an improperly designed cable (too long or not properly shielded), or an incorrectly configured channel service unit (CSU) line clock.",
              "STP failures or loops - Most STP problems are related to forwarding loops that occur when no ports in a redundant topology are blocked and traffic is forwarded in circles indefinitely, excessive flooding because of a high rate of STP topology changes. A topology change should be a rare event in a well-configured network. When a link between two switches goes up or down, there is eventually a topology change when the STP state of the port is changing to or from forwarding. However, when a port is flapping (oscillating between up and down states), this causes repetitive topology changes and flooding, or slow STP convergence or re-convergence. This can be caused by a mismatch between the real and documented topology, a configuration error, such as an inconsistent configuration of STP timers, an overloaded switch CPU during convergence, or a software defect.",
              "SNR (Signal-to-noise ratio)It is the difference between the received signal power and the noise power expressed in decibels. The retransmissions at layer 2 increase when the background noise is close to the received signal power or if the signal is too low. Stats to live by for WLANs: A good signal quality should be between 20 and 25 dB. Anything below these ranges is considered low signal quality.",
              "RF interferenceIt plays a significant role in the retransmissions in layer 2. Excessive retransmissions will happen when frames are corrupted because of RF interference, and therefore, throughput is reduced significantly. If these retransmissions occur frequently, it's essential to understand the source to remove the interference device.",
              "Channel interferenceLet's go back to basics. When designing the 2.4GHz WLAN channel allocation plan, make sure to use the channels available for 2.4GHz properly. When there's an overlapping coverage cell, and overlapping frequency space, the chances of having corrupted data and layer 2 retries are remarkably high. Remember to set up a reuse pattern for 2.4GHz channels 1, 6, and 11 (US) or 1, 5, and 9 -sometimes 13 is also used in deployments for Europe. In this way, you prevent adjacent cell interference in your WLANs."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss various data link layer attacks that can lead to connection termination, such as ARP Spoofing and VLAN Hopping for wired networks, and hidden node attack for wireless networks.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Typically, a layer 2 problem that can cause the loss connection is when your network has an MAC spoofing and MAC flooding attack, a broadcast storm or an ARP spoofing/poisoning attack.",
              "1. **MAC Address Spoofing**: Attackers can forge MAC addresses to impersonate legitimate devices, potentially leading to unauthorized access and connection disruptions[1].\u000b2. **ARP Poisoning**: This attack can redirect traffic, causing connections to fail or allowing malicious interception of data[1].3. **VLAN Hopping**: Exploiting VLAN configurations can lead to unauthorized access to different network segments, potentially terminating legitimate connections[1].4. **STP (Spanning Tree Protocol) Manipulation**: Attackers can exploit STP to create network loops or redirect traffic, causing connection instability or termination[1].5. **CAM Table Overflow**: Flooding the switch's CAM (Content Addressable Memory) table can cause the switch to act like a hub, leading to performance issues and potential connection drops[1].6. **DHCP Starvation**: By exhausting the DHCP server's IP address pool, new devices may fail to obtain network configurations, effectively terminating their ability to establish connections[1].7. **Port Security Violations**: Improperly configured port security features can shut down switch ports, terminating connections for legitimate devices[1].",
              "Hidden node: In wireless networking, a 'hidden node' means that a specific node 'talks' to a WiFi access point but can't 'talk' directly with other nodes already having a 'conversation' with that access point. This should ring all the bells, because it leads to problems in the MAC sublayer as multiple nodes send data packets to the access point at the same time, thus creating interference at the AP level, resulting in data packet loss."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should mention that the data link layer is often considered the weakest link in the OSI model.",
            "weight": 0.08571428571428572,
            "evidence": [
              "It's important to note that Layer 2 is often considered the weakest link in the OSI model, particularly in LANs. This vulnerability stems from the historical assumption that all devices and persons within a LAN could be trusted[1]. However, with the evolution of network threats, this assumption no longer holds true, making data link layer security crucial."
            ]
          }
        ]
      }
    },
    "case_id": "dda770e4b5c6b0933de17c4ba3aa3a39",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are the leading approaches for anomoly detection in process mining?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are the leading approaches for anomoly detection in process mining?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define process mining.",
            "weight": 0.08,
            "evidence": [
              "Process mining is a set of techniques that analyze event logs to extract knowledge about business processes, aiming to improve productivity and reduce costs  (10, Moreira, 2015)  (66, Zhong et al., 2022). "
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should highlight the importance of anomaly detection in process mining.",
            "weight": 0.08,
            "evidence": [
              "As real processes (e.g. logistics, loan application, payment) become more and moredynamic and complex, it is crucial to be able to analyze these processes in real time and to react adequately to deviations and inconsistencies. The real-time reaction to inconsistencies within the processes highlights new potentials, such as more efficient process design, which goes hand in hand with reducing and preventing losses.\"https://www.researchgate.net/publication/349279735_Variational_Autoencoder_for_Anomaly_Detection_in_Event_Data_in_Online_Process_Minin",
              "While a business process is most often executed following a normal path, anomalies may sometimes arise and can be captured in event logs. Event log anomalies stem, for instance, from system malfunctioning or unexpected behavior of human resources involved in a process. To identify and possibly fix these, anomaly detection has emerged recently as a key discipline in process mining.",
              "Anomaly detection in process mining focuses on identifying deviations from expected business processes. This can help organizations in diagnosing issues, improving processes, and ensuring compliance."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should briefly introduce some of the statistical methods used for anomaly detection in process mining.",
            "weight": 0.08,
            "evidence": [
              "2.1 Statistical-based ApproachesStatistical-based approaches use statistical methods to identify anomalies by comparing process instances or events to expected patterns or distributions.a) Control-flow Based:- These methods focus on detecting anomalies in the sequence of activities.- Example: Bezerra et al. proposed a method that uses frequency-based metrics to identify infrequent traces as potential anomalies [2].b) Performance-based:- These approaches detect anomalies related to process performance, such as unusual durations or waiting times.- Example: Swinnen et al. developed a method that uses statistical process control techniques to identify abnormal process durations [3]."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should mention that deep learning methods, in general, perform better than statistical methods for anomaly detection because feature selection is done automatically.",
            "weight": 0.08,
            "evidence": [
              "Hence, existing approaches often exploit traditional process discovery techniques, defining anomalies as traces that deviate from the behavior captured by the discovered process models, e.g., (Bezerra and Wainer 2011; Myers et al. 2018). Other methods have relied on statistical or machine learning (ML) techniques (Rullo et al. 2020; Nolle et al. 2019), exploiting the concept of distance among traces, or reconstructive techniques.",
              "In addition to the disadvantages of statistical techniques, the required specialized process knowledge and the extraction of suitable features from the examined data also poses a relevant challenge. The development of new models in the areas of deep learning and time series analysis make it possible to reduce precisely this need. With the help of special deep learning methods a specific selection of data features is possible. This includes methods such as convolutional neural networks, recurrent neural networks, generative adversarial networks and variational autoencoder. Especially in the context of anomaly detection in event streams, the superiority of deep learning techniques compared to statistical techniques is shown (Gamboa, 2017) (Ahn et al., 2019) (OMeara et al., 2018).",
              "Approaches that address the filtering of event data in an online setting are very limited (van Zelst et al., 2018). These mostly use statistical methods that make a priori assumptions about the underlying relationship of the variables used (Ahn et al., 2020). This leads to the difficulty that underlying probability distributions and variable relationships have to be adjusted for each use case. Furthermore, statistical models are often not suitable for processing high-dimensional data (Ahn et al., 2020)"
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should briefly mention that the type of ML model used for anomaly detection can be supervised, semi-supervised, or unsupervised. It should also argue that, due to the difficulty of identifying and labeling abnormal behavior, unsupervised methods are generally preferred.",
            "weight": 0.08,
            "evidence": [
              "The type of learning algorithm adopted can be supervised, semi-supervised, or unsupervised.",
              "In the supervised learning, it is assumed that a large number of labeled observations is available. Anomaly detection is then reduced to a classification problem. Since it is generally hard to obtain labeled data in real-life settings, this type of approach is scarce in the literature.",
              "Anomaly detection is generally an unsupervised problem that handles unlabeled data (Koschmider et al. 2021)."
            ]
          },
          {
            "name": "most_important_item_5",
            "criterion": "The answer should mention deep autoencoders as one of the leading deep learning methods for time-series anomaly detection in process mining.",
            "weight": 0.08,
            "evidence": [
              "**Deep Autoencoders**: One of the most promising techniques is the use of deep autoencoders for anomaly detection in process mining. The PMiner framework, proposed in 2024, utilizes a deep autoencoder to automatically detect anomalies in business processes[4]. This approach has shown superior performance compared to state-of-the-art methods when tested on the BPI Challenge dataset.**Variational Autoencoders (VAEs)**: VAEs have been applied for anomaly detection in event data within online process mining environments[2]. This unsupervised approach uses mass-volume and excess-mass scores as metrics for identifying anomalies. The VAE method has been compared favorably to established algorithms such as one-class support vector machines, isolation forests, and local outlier factors.",
              "Neural network-basedmethods use techniques such as autoencoders (Nguyen et al.2019; Nolle et al.2018; Vijayakamal and Vasumathi2020) to create models that can learn the underlying hidden distribution of process data, or sequential predictive models, such as recurrent neural networks (Nolle et al.2019), where the prediction of the next activity to be executed in a process instance is used for anomaly detection. Anomalous cases are then the ones with higher reconstruction error. While neural network-based methods can provide high performance when properly designed and trained using a sufficient amount of data, they often require large amounts of computational power and memory during training."
            ]
          },
          {
            "name": "most_important_item_6",
            "criterion": "The answer should discuss challenges of anomaly detection in process mining such as noisy data, the need for online anomaly detection, and multi-layered structure of data.",
            "weight": 0.08,
            "evidence": [
              "Existing analysis methods typically assume that the input event data is completely free of incorrect data and infrequent behavior, which does not usually correspond to reality (van der Aalst et al., 2004) (Leemans et al., 2013) (Hassani et al., 2015). Incorrect data in event streams can lead to incorrect results during further processing.",
              "Discovering anomalies in an event log is challenging due to the multi-layered structure of event logs, where the observation unit is an event recorded for each activity execution, while the unit of analysis is often a process case, represented by a sequence of events. This characteristic of event logs has led traditional data cleaning techniques on standard tabular and time-series datasets to perform poorly on event logs (Ko and Comuzzi 2021).",
              "Approaches that have been conducted in this area using filtering techniques to eliminate erroneous events from event data show an improvement in the quality of process mining techniques which leads to an optimization of the analysis of the processes (Ghionna et al., 2008) (Wang et al., 2015) (Conforti et al., 2017).",
              "The majority of these approaches on event data based anomaly detection addresses batch processing, i.e. the processing of historical data. In order to take full advantage of the possibilities offered by anomaly detection methods, it is necessary to transfer these methods to an online setting. This enables the use of anomaly detection methods and subsequent process mining techniques in operational support and allows processes to be influenced in real-time due to deviations in process flow."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should describe the type of data collected from the process mining systems for operational support and anomaly detection.",
            "weight": 0.04,
            "evidence": [
              "Process Mining evaluates event data that was recorded during the execution of a process. An event is any data that is recorded during the execution of a process and is considered the smallest unit within a process. The granularity of an event depends on the application domain as well as the way it is recorded. For example, an event can describe which activity of a process was executed at what time. In the same way, it can also describe the different stages of the execution of an activity, e.g. events refer to the scheduling, starting, suspending, continuing or completing of an activity (van Zelst et al., 2018)."
            ]
          }
        ]
      }
    },
    "case_id": "be5c0337461175e55f2a8fa9bcce5732",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "Global Navigation Satellite Systems (GNSS) receivers can provide a reference clock signal known as Pulse-per-Second (PPS or 1-PPS). Is the PPS physically generated through a digitally-controlled oscillator (or line driver) whose offset is periodically re-initialized by the estimated clock bias (retrieved by means of PVT algorithms)?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "Global Navigation Satellite Systems (GNSS) receivers can provide a reference clock signal known as Pulse-per-Second (PPS or 1-PPS). Is the PPS physically generated through a digitally-controlled oscillator (or line driver) whose offset is periodically re-initialized by the estimated clock bias (retrieved by means of PVT algorithms)?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly mention what a GNSS receiver is and discuss different types of receivers.",
            "weight": 0.10909090909090909,
            "evidence": [
              "A GNSS receiver, in general, is an electronic device that receives and digitally processes the signals from a navigation satellite constellation in order to provide position, velocity and time (of the receiver).GNSS receivers have been traditionally implemented in hardware: a hardware GNSS receiver is conceived as a dedicated chip that have been designed and built (from the very beginning) with the only purpose of being a GNSS receiver.In a software GNSS receiver, all digital processing is performed by a general purpose microprocessor. In this approach, a small amount of inexpensive hardware is still needed, known as the frontend, that digitizes the signal from the satellites. The microprocessor can then work on this raw digital stream to implement the GNSS functionality.\"https://en.wikipedia.org/wiki/GNSS_software-defined_receiver#:~:text=A%20GNSS%20receiver%2C%20in%20general,time%20(of%20the%20receiver)"
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should confirm that digitally controlled oscillators (or line driver) can be used in GNSS receivers to produce or fine-tune PPS, but the exact implementation may vary between different types of receivers.",
            "weight": 0.10909090909090909,
            "evidence": [
              "An SDR time board provides the clock distribution for all receiver and transmitter ADCs/DACs and the FPGA to work synchronously. This internal time board consists of an oven-controlled crystal oscillator (OCXO) that delivers a very stable (5 parts/billion) and accurate 10-MHz signal. Such oscillators also offer high frequency stability, a low noise floor, and the low phase noise required for accurate timing and synchronization of functions. The unmatched frequency stability of this oscillator makes the SDR clock ideal for applications with strict timing requirements, such as GNSS systems.The SDR clock distribution ensures that various functions of a GNSS system are properly synchronized. Apart from sampling-related functions, this clock-distribution system also can synchronize other processing operations like upconverting and downconverting functions.https://www.mwrf.com/technologies/embedded/systems/article/21168561/per-vices-corp-sdrs-as-a-reference-and-common-clock-source-for-gnss-timing-apps\"(LLM MEMORY, 2024) The generation of Pulse-per-Second (PPS) signals in Global Navigation Satellite System (GNSS) receivers is a complex process that involves both hardware and software components. While the specific implementation can vary between different receiver models and manufacturers, the general principle remains consistent.Typically, GNSS receivers use a combination of a stable local oscillator and sophisticated timing algorithms to generate the PPS signal. The local oscillator, which is often a temperature-compensated crystal oscillator (TCXO) or an oven-controlled crystal oscillator (OCXO), provides a stable frequency reference. However, this oscillator alone is not sufficient to generate an accurate PPS signal aligned with GNSS time.To achieve the required accuracy, the receiver's software continuously processes the incoming satellite signals to estimate the receiver's position, velocity, and time (PVT). This PVT solution includes an estimate of the receiver's clock bias relative to GNSS time. The software uses this clock bias information to adjust the timing of the PPS signal output.(LLM MEMORY, 2024) While a digitally-controlled oscillator (DCO) or a numerically-controlled oscillator (NCO) could potentially be used in this process, it's important to note that the exact implementation can vary. Some receivers might use a DCO/NCO to fine-tune the timing, while others might use different methods to adjust the PPS output based on the calculated clock bias.",
              "3. PPS Generation in GNSS ReceiversThe PPS signal is indeed generated within the GNSS receiver, but the exact mechanism can vary depending on the receiver design. In many cases, the PPS is not directly generated by a digitally-controlled oscillator. Instead, it is typically derived from the receiver's internal clock, which is continuously adjusted based on the GNSS timing information [3]."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should highlight the role of digitally-controlled oscillators in gaining high clock accuracy in receivers.",
            "weight": 0.10909090909090909,
            "evidence": [
              "The DCO helps maintain the stability and accuracy of the receiver's internal clock, which in turn affects the accuracy of the PPS output.",
              "(LLM MEMORY, 2024) Digitally-controlled oscillators (DCOs) can play a role in generating and fine-tuning Pulse-per-Second (PPS) signals in Global Navigation Satellite System (GNSS) receivers, though their specific implementation and importance can vary among different receiver designs. DCOs offer the advantage of precise digital control over frequency output, which can be valuable for adjusting the timing of PPS signals based on computed clock bias."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should explain how PVT can be used to adjust receivers\u2019 clock and fix clock offset.",
            "weight": 0.10909090909090909,
            "evidence": [
              "### Clock Bias and PVT AlgorithmsGNSS receivers compute the Position, Velocity, and Time (PVT) using signals from multiple satellites. One critical aspect of this computation is determining the clock bias, which is the offset between the receiver's internal clock and the true GPS time.1. **Clock Bias Estimation:** The receiver calculates the clock bias periodically using signals received from the satellites by solving the PVT algorithms. This involves comparing the internal clock time against the satellite signals, which have precise timestamps.2. **Offset Correction:** The estimated clock bias is then used to adjust the digitally-controlled oscillator or the timing signal generation mechanism to correct the PPS offset. This re-initialization ensures that the 1-PPS signal remains aligned with the true GPS second."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should define what clock offset is.",
            "weight": 0.05454545454545454,
            "evidence": [
              "it helps to remind ourselves what the clock offset actually is. In brief, the offset represents the difference between what time the receiverthinksit is, and the true time, with the latter determined by the underlying GNSS atomic time scale."
            ]
          },
          {
            "name": "nice_to_have_item_1",
            "criterion": "The answer should explain how signal travel time is calculated by using the onboard clock of the GNSS system.",
            "weight": 0.05454545454545454,
            "evidence": [
              "Each one of these satellites is equipped with onboard atomic clocks. Atomic clocks are the most precise time measurement instruments known, losing a maximum of one second every 30,000 to 1,000,000 years. In order to make them even more accurate, they are regularly adjusted or synchronized from various control points on Earth. GNSS satellites transmit their exact position and onboard clock time to Earth. These signals are transmitted at the speed of light (300,000km/s) and therefore require approx. 67.3ms to reach a position on the Earth's surface directly below the satellite. The signals require a further 3.33s for each additional kilometer of travel. To establish position, all that is required is a receiver and an accurate clock. By comparing the arrival time of the satellite signal with the onboard clock time the moment the signal was transmitted, it is possible to determine the signal travel time (Figure 6)."
            ]
          },
          {
            "name": "nice_to_have_item_2",
            "criterion": "The answer should discuss the advantages of DCO compared to analog clocks.",
            "weight": 0.05454545454545454,
            "evidence": [
              "DCOs are similar to their analog counterpart, the voltage-controlled oscillator (VCXO), but instead of analog input signals, they use digital input signals via an I2C serial communications bus or serial peripheral interface (SPI) to drive the voltage pin.Relative to the analog-driven VCXOs, digitally driven DCO benefits include:Finer frequency resolution to minimize accumulated time error, critical for synchronizing timing with greater precision, as low as 5E-12.",
              "Lower system cost because DCOs don't need a digital-to-analog converter (DAC) when operating across a serial interface. This reduces the number of components and the board space. Lower noise in the output signal reduces frequency deviations. A VCXO pin as well as the signal routing trace is vulnerable to noise coupling from the system. Because DCO frequency control is performed over a digital interface, they do not suffer from analog noise coupling. Perfectly linear frequency pull improves the dynamic performance in closed-loop operations. Extremely wide pull range of up to +-3200 pulse position modulation (ppm) can be achieved with a fractional feedback divider in the PLL, eliminating any pull non-linearity."
            ]
          }
        ]
      }
    },
    "case_id": "e2491bb6e4147c18762e7fb25d0bf1fd",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "I want to measure the distance between two Bluetooth devices (A Master and a slave) using the corresponding RSSI value. Is there any algorithm or popular approach that maps RSSI values directly to distance?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "I want to measure the distance between two Bluetooth devices (A Master and a slave) using the corresponding RSSI value. Is there any algorithm or popular approach that maps RSSI values directly to distance?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should define what RSSI is and explain how it can be related to distance.",
            "weight": 0.0923076923076923,
            "evidence": [
              "Received Signal Strength (RSS, also called Received Signal Strength Indication, RSSI) based method (Fig. 2 d)) utilise the characteristic of radio propagation over space. Using a proper propagation model, we can calculate the distances between a mobile to base stations, thus derive the location of the mobile.",
              "Generally, the RSSI is a measurement of the strength of an incoming radio signal. It is a relative indicator and its units are arbitrary, but the higher the value of the RSSI, the stronger is the signal. In Bluetooth, the RSSI is used to tell whether the received signal is within the Golden Receiver Power Range (GRPR), which is the name used to describe the ideal range of incoming signal strengths. The RSSI is measured in dB, and a signal strength within the GRPR results in an RSSI of zero dB. A positive or negative RSSI indicates that the signal strength is above or below the GRPR, respectively. The Bluetooth specification does not specify the upper or lower limit of the RSSI but simply states that it must be possible to tell whether the incoming signal is within, above or below the GRPR, hence this value is device specific."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention the Free space model, Two-ray ground model, and log-normal shadowing model as traditional ways to relate the RSSI with the distance between transmitter and receiver devices",
            "weight": 0.0923076923076923,
            "evidence": [
              "The traditional signal propagation models include free space model (FSM), two-ray ground model, and log-normal shadowing model (LNSM). Specifically, FSM is an ideal model, and the received power decays as a function of transmitter-receiver (T-R) distance. The FSM can be expressed as follows [34]:(1)where PL,d, andfare the path loss of signal energy, the signal transmission distance, and the wireless signal frequency, respectively. The two-ray ground model can be represented as follows [35]:(2)wherehtandhrare the antenna heights of transmitter and receiver, respectively. The LNSM can be represented as follows [36]:(3)whered0is the near-earth reference distance which depended on the experimental value,nis the path loss index in a specific environment, andXsis a zero-mean Gaussian random variable."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss other methods like fingerpriting and ML-base methods for RSSI-based distance estimation.",
            "weight": 0.0923076923076923,
            "evidence": [
              "4. Fingerprinting:This technique involves creating a database of RSSI values at known locations during a calibration phase. During positioning, the current RSSI values are compared to the database to estimate location[2][4].",
              "5. Machine Learning Approaches:Recent advancements involve using machine learning algorithms to map RSSI to distance:a) Random Forest: This technique has shown promise in modeling the relationship between RSSI and distance, especially when combined with other sensor data[9][11].b) K-Nearest Neighbor (KNN): Used to estimate distance based on similarity to known RSSI-distance pairs[6]."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should discuss that RSSI-based methods for calculating distance are prone to errors due to environmental conditions and inaccurate RSSI measurements.",
            "weight": 0.0923076923076923,
            "evidence": [
              "Never-theless, such BLE- and RSS-based indoor positioning systems are prone to inaccuracies, mostly due to signal fluctuations, poor quantity of anchors deployed in the environment, and/or inappropriate anchor distributions, as well as mobile device hardware variability.",
              "While RSSI-based distance estimation is a popular approach for Bluetooth-enabled devices, it comes with several inherent challenges and limitations. One of the primary issues is the non-trivial nature of translating RSSI values to accurate distance measurements. This difficulty arises from numerous factors that can influence RSSI readings, including phone hardware, drivers, operating systems, and environmental interference  (103, Kiran et al., 2020)."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should provide some suggestions for improving the accuracy of RSSI-based methods, such as the use of hybrid approaches and filtering.",
            "weight": 0.0923076923076923,
            "evidence": [
              "6. Hybrid Approaches:Some advanced systems combine multiple techniques for improved accuracy:a) Trilateration with fingerprinting: Uses both geometric calculations and pre-recorded RSSI maps[6].b) Sensor fusion: Combines RSSI data with other sensors (e.g., IMU) for more robust positioning[6][9].",
              "- **Calibration**: It is crucial to calibrate RSSI readings in specific environments to improve accuracy. Calibration involves measuring RSSI at known distances and fitting the path loss model parameters accordingly.- **Averaging and Filtering**: RSSI can fluctuate due to noise. Using averaging or filtering techniques like a Kalman filter can help smooth out the measurements."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should discuss how RSSI of a device can be obtained.",
            "weight": 0.04615384615384615,
            "evidence": [
              "The RSSI of a Bluetooth device is obtained by starting the inquiry procedure from a second device. The RSSI will then be included in the first devices' response to the inquiry"
            ]
          },
          {
            "name": "nice_to_have_item_1",
            "criterion": "The answer should briefly explain indoor positioning based on Wifi and compare it with Bluetooth.",
            "weight": 0.04615384615384615,
            "evidence": [
              "Compares to WiFi based positioning, Bluetooth has several advantages: a) Cost: the cost of Bluetooth chip is lower than WiFi and b) Power consumption: Bluetooth consumes much lower power than WiFi, Bluetooth uses a fifth of the power of WiFi as it requires a lower transmission power and provides a mechanism for automatic power control. This makes Bluetooth a more attractive positioning technology as power is critical in mobile devices. Therefore, in some scenarios where positioning precision and response time is not critical but device operation time (or battery power) is more important, Bluetooth positioning is a more preferable technology than WiFi. For example, city and tourism guild programs for smartphones, daily people and logistic tracking in hospitals, company and large firms etc."
            ]
          },
          {
            "name": "nice_to_have_item_2",
            "criterion": "The answer should discuss that GPS is the dominant technology for outdoor positioning.",
            "weight": 0.04615384615384615,
            "evidence": [
              "Wireless position tracking has been exploited in many areas. A classic example is the Global Positioning System (GPS). Orbit satellites are used to send signals to GPS receivers on Earth (such as \"Satellite Navigation\" used in vehicles), and these signals are used by the receivers to compute navigation information. Position tracking enables the possibility of delivering personalised and location-based services, which have been increasingly popular in recent years. For instance, Google Maps allows users who have GPS on their mobile devices to search for their current locations as well as place-of-interest nearby. However, as the communication between the satellites and GPS receivers requires line-of-sight radio propagation, GPS generally only works well in outdoor environments"
            ]
          }
        ]
      }
    },
    "case_id": "5079291508735025cf2643c7a59d0293",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "Have specialized approaches been developed for providing LLM assistance when people author SQL queries?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "Have specialized approaches been developed for providing LLM assistance when people author SQL queries?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly explain the challenge of text-to-SQL conversion, such as understanding user questions, comprehending database schemas, and accurately generating SQL queries, and highlight the potential for LLMs to assist in this process.",
            "weight": 0.075,
            "evidence": [
              "Generating accurate SQL according to natural language questions (text-to-SQL) is a long-standing problem since it is challenging in user question understanding, database schema comprehension, and SQL generation. Conventional text-to-SQL systems include human engineering and deep neural networks. Subsequently, pre-trained language models (PLMs) have been developed and utilized for text-to-SQL tasks, achieving promising performance. As modern databases become more complex and corresponding user questions more challenging, PLMs with limited comprehension capabilities can lead to incorrect SQL generation. This necessitates more sophisticated and tailored optimization methods, which, in turn, restricts the applications of PLM-based systems. Most recently, large language models (LLMs) have demonstrated significant abilities in natural language understanding as the model scale remains increasing. Therefore, integrating the LLM-based implementation can bring unique opportunities, challenges, and solutions to text-to-SQL research.",
              "However, it is much harder for text-to-SQL to get test cases than code generation. For code generation, the test cases are code assertion statements, which are also code snippets, so they can be generated by the same code generation models. For text-to-SQL, the input to a text-to-SQL model consists of an NL question and a database. A test case for text-to-SQL should contain a test input and an expected execution result. Test input is the NL question and a new database that has the same schema as the given database, while the expected execution result is the execution results on the new database of the ground truth SQL query. Because test cases are different from SQL queries, we cannot utilize existing text-to-SQL models to generate them.",
              "One key challenge is the ordering issue also known as the \"order-matters\" problem [2], where the order of predicates within the WHERE clause in SQL queries does not affect the resultant execution outcomes thus the same query can be expressed in different orders. The SQL queries \"SELECT * FROM Employees WHERE Salary >50000 AND Department = 'IT\"' and \"SELECT * FROM Employees WHERE Department = 'IT' AND Salary >50000\" yield the same result. Solutions like Seq2SQL [1], SQLNet [2], and IncSQL [14] employ different approaches, such as reinforcement learning, sequence-to-set and sequence-to-action models, to tackle this issue.",
              "Another challenge is the complex and cross-domain Text-to-SQL task, which involves handling complex SQL queries across diverse domains and databases. The Spider [15] Text-to-SQL dataset is a prime example of such complexity, requiring models to generalize across different subjects such as movie databases, geography, and sports. SyntaxSQLNet [3] and RYANSQL [4] address this challenge through syntax tree networks and sketch-based slot-filling methods.",
              "The lack of information challenge arises due to the lack of domain-specific knowledge and rare entities in natural language queries, leading to inaccurate translations. In a query asking for the highest-scoring player in a sports database without specifying the sport, the model might struggle without additional context. TypeSQL [5] assigns types to words as entities, while DialSQL [6] incorporates user interaction to enhance query accuracy by identifying and revising potential errors.",
              "In some cases, there is a mismatch problem [7] where SQL column names do not align with their natural language descriptions, confusing, especially in GROUP BY queries. In a GROUP BY query, the natural language might refer to \"total sales\" while the SQL column name is \"revenue\". STAMP [16] and IRNet [7] address this by considering the structure of table and the syntax of SQL and schema linking module.",
              "The lexical problem [7] arises in cross-domain benchmarks like Spider and WikiSQL, where a significant portion of words in the development set is absent in the training set. This poses a challenge as models lack accurate representations for these out-of-domain (OOD) words. For example; a model trained on a dataset of scientific articles may struggle with terms specific to a dataset about movie reviews. IRNet tackles this problem with schema linking solutions.",
              "Lastly, the schema representation problem pertains to generalizing models to unseen database schemas in cross-domain Text-to-SQL tasks. This challenge involves encoding schema information, including table and column details, and building a link between natural language and database schema. Adapting a model trained on a dataset about e-commerce to generate queries for a medical database, which requires understanding different table structures and relationships. GNN [8], RAT-SQL [9] and SADGA [10] models are presented to address this challenge.",
              "For many decades, the text-to-SQL problem has been referred to as a sub- problem of semantic parsing, and this view of the problem works fine for simpler queries that just include simple keywords like \"where,\" \"select,\" etc., but not for complex queries that include keywords like \"various joins,\" \"nested queries,\" etc. And some of the models are domain-specific, and there is no general label guidance for many of the SQL database queries"
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention ensemble LLM that translate natural language questions into SQL queries",
            "weight": 0.075,
            "evidence": [
              "Although in-context learning with advanced LLMs such as GPT-4, Claude-3 Opus, or fine-tuning GPT-3.5 yields excellent RS0, errors still seem inevitable. The model's ability to solve a specific task is heavily influenced by the training data. Repeatedly generating using the same prompt (or) the same model to validate often fails to minimize errors since hallucinations mainly originate from the training data. Fine-tuning GPT-3.5 resulted in different error tendencies compared to pre-trained models, even when using the same prompt. Therefore, ensemble LLMs, particularly those with a fine-tuned model, offer a superior approach for SQL validation, improving robustness and reliability. This approach has also secured us 2nd place in the competition.",
              "Maximizing the success and minimizing hallucinations of the LLMs generation task require the provision of the correct context. To achieve this, the following information is essential regarding the task at hand:* Database Schema Comprising tables, columns, and their interrelationships, the database schema serves as a blueprint for the data stored in the database. This information guides the LLM in selecting the appropriate tables and columns.* Database Column Values The actual values stored in the table columns offer additional information. This helps the LLM comprehend and perform operations such as data validation, manipulation, and filtering* Training Data Providing questions (with corresponding SQL answers) similar to the current question aids the LLM in comprehending query formats, syntax, semantics, ambiguity resolution, and bridging the real-world knowledge gap with EHRSQL."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should mention sketch-based slot-filling approaches that transform natural",
            "weight": 0.075,
            "evidence": [
              "Sketch-based slot-filling approaches Using sketch-based slot-filling approaches, complex query logic is streamlined into a series of basic operations, simplifying neural network predictions and ensuring correct syntax when generating SQL queries. For instance, SQLNet(Xu et al., 2017) employs a predefined sketch of the SQL query, breaking down the required SQL query into the SELECT and WHERE clause segments, predicting slots within these segments (such as column, operator, aggregation, and value)."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should mention sketch-based slot-filling approaches that transform natural",
            "weight": 0.075,
            "evidence": [
              "Sequence-based approaches Another common approach involves sequence generation, relying on encoder-decoder pre-trained language models such as T5(Raffel et al., 2020), BART(Lewis et al., 2020), and similar architectures. These methods directly transform natural language queries into SQL queries through end-to-end sequence generation. For instance, a novel encoder-decoder framework is proposed by Cai et al. (2018). During the encoding phase, the neural network identifies and maintains semanticinformation of natural language questions. In the decoding phase, based on the neural network's hidden states, it generates a new sequence in another language"
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should mention pre-trained SQL-specific language models such as SQLNet and query generation techniques, to provide LLM assistance for authoring SQL queries.",
            "weight": 0.075,
            "evidence": [
              "The author presents SQLNet, an innovative solution to this problem that avoids the sequence-to-sequence architecture whenever the order is immaterial. They employed a sketch-based technique, in which the sketch incorporates a de- pendency network, allowing one prediction to be made using only the prior forecasts on which it is dependent. In addition, to synthesize the query based on the sketch, they use a sequence-to-set model as well as a column attention method. SQLNet outperforms the previous art on the WikiSQL challenge by 9 to 13% when all of these unique strategies are combined.",
              "TypeSQL [40] provides an alternate training process that employs types derived from either graph or tabular content to aid the model's understanding of tables and numbers in the investigation, which outperforms SQLNet. In this experiment, the author uses the question-type information gathered from the content of the database. We also add components for ORDER BY and GROUP BY to their modules. This is the only model that uses the information from the database.",
              "Recent trends in deep neural networks (DNNs) also promote the development on end-to-end neural network-based approaches. Representative studies in this category include Seq2SQL [103], SQLNet [89], TypeSQL [92], Syntax SQL [73], IRNET [28] and its extensions such as NL2pSQL [15]. Furthermore, many studies focus on pre-train the text-to-SQL models with augmented data, with representative studies like [93] and [95]. Compared with the traditional approaches, these neural-based models have the advantages of reducing the workload of engineers of maintaining multiple components and, at the same time, achieve a much better performance since the whole model enjoys a single optimization objective (i.e., SQL generation accuracy). The neural-based approach has already dominated the text-to-SQL area.",
              "1. Fine-tuning LLMs for SQL generation:Researchers have developed methods to fine-tune open-source LLMs for transforming natural language into SQL queries, particularly in specific domains like retail. This approach aims to make SQL more accessible to non-specialists[1]. For example, models fine-tuned on synthetic datasets tailored to Snowflake SQL and GoogleSQL dialects have shown superior performance in zero-shot settings compared to baseline GPT-4, with Code-Llama achieving accuracy rates of 81.58% for Snowflake SQL and 82.66% for GoogleSQL[1].",
              "#### 2. Pre-trained Language Models Adaptation**Large Language Models (LLMs) like OpenAI's GPT-3 and BERT** can be fine-tuned on SQL-specific tasks to improve their ability to assist in SQL query formulation. These LLMs have been trained on vast datasets and can interpret nuanced natural language instructions.- **Codex**: OpenAI's Codex, an evolution from GPT-3, is specifically fine-tuned for programming tasks, including SQL query generation. It can take contextual natural language prompts and convert them into functional SQL statements.- **T5 for SQL**: The T5 model has been adapted (TAPAS) to perform table tasks and can convert table-related questions into SQL queries using its text-to-text framework [Herzig et al., 2020]."
            ]
          },
          {
            "name": "most_important_item_5",
            "criterion": "The answer should mention interactive query assistance approaches to provide LLM assistance for authoring SQL queries.",
            "weight": 0.075,
            "evidence": [
              "#### 3. Interactive Query AssistanceInteractive systems aim to bridge the gap between the user and the data by offering real-time recommendations and autocompletions as users type their SQL queries.- **Intellisense for SQL**: Several Integrated Development Environments (IDEs) include Intellisense-like features powered by machine learning, providing auto-completion, syntax checking, and suggestions.- **Conversational Agents**: Conversational agents like Microsoft's Azure Bot Service or Google's Dialogflow have been trained to understand natural language questions, map them to SQL intents, and guide users interactively."
            ]
          },
          {
            "name": "most_important_item_6",
            "criterion": "The answer should mention prompt engineering techniques for improving LLM performance in Text-to-SQL tasks.",
            "weight": 0.075,
            "evidence": [
              "Prompt Engineering TechniquesTLDR: Prompt engineering is crucial for improving LLM performance in Text-to-SQL tasks. Techniques include designing effective prompt templates, incorporating in-context examples, and using chain-of-thought approaches to enhance SQL generation accuracy.Prompt engineering has emerged as a critical component in developing specialized approaches for LLM assistance in SQL query authoring. Researchers have explored various techniques to design effective prompt templates that enable LLMs to generate accurate SQL queries  (35, Yang et al., 2024). These techniques aim to improve the performance of LLMs in Text-to-SQL tasks through innovative prompt designs and strategies."
            ]
          },
          {
            "name": "most_important_item_7",
            "criterion": "The answer should mention task decomposition strategies for improving LLM assistance in SQL query authoring.",
            "weight": 0.075,
            "evidence": [
              "Task Decomposition StrategiesTLDR: Task decomposition strategies break down complex Text-to-SQL tasks into smaller, more manageable subtasks. These approaches improve SQL generation accuracy by leveraging LLMs' strengths in handling specific aspects of the query generation process.Task decomposition has emerged as a powerful strategy for improving LLM assistance in SQL query authoring. By breaking down the complex Text-to-SQL task into smaller, more manageable subtasks, researchers have developed approaches that can enhance the accuracy and efficiency of SQL generation.One notable approach is the Divide-and-Prompt (DnP) paradigm, which divides the Text-to-SQL task into subtasks and tackles each using chain-of-thought (CoT) prompting  (24, Tan et al., 2023). This method leverages the strengths of LLMs in handling specific aspects of the query generation process, resulting in improved performance."
            ]
          }
        ]
      }
    },
    "case_id": "2bb40aa93ac3a6a673c839bd660718ac",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "How can the integration of GIS spatial analysis techniques with economic modeling improve the accuracy and effectiveness of environmental policy ?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "How can the integration of GIS spatial analysis techniques with economic modeling improve the accuracy and effectiveness of environmental policy ?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define GIS spatial analysis techniques and economic modeling in the context of environmental policy.",
            "weight": 0.24,
            "evidence": [
              "Geographic analysis enables users to explore and overlay data by location, revealing hidden trends that are not readily apparent in traditional spreadsheet and statistical packages. GIS allows for the construction of space (and space-time) data architectures that can then be analyzed with either spatial or aspatial statistics. Analytical results can then be displayed in GIS, to enhance ease of interpretation. Additionally, GIS contains advanced capabilities to generate clear and accessible maps and data reports that can serve as powerful tools for research, outreach, and policy design.",
              "A GIS is defined as \"a system for capturing, storing, checking, integrating, manipulating, analysing, and displaying data that are spatially referenced to the earth\" (Department of the Environment 1987: 132)"
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should explain how the integration of GIS spatial analysis and economic modeling can improve the accuracy and effectiveness of environmental policy.",
            "weight": 0.24,
            "evidence": [
              "An integrated framework of economic, environmental and GIS modeling is developed to study cost-effective retirement of cropland within and across multiple watersheds to achieve environmental goals.",
              "Our approach is a geographic information system (GIS)-based integrated analytical framework where the interaction between nitrogen losses and economic output is handled for larger regions, in which sampling of field data is unrealistic due to the huge amount of resources required.",
              "By basing our analysis on this type of data in combination with GIS-based information on the spatial distribution of the different farm types, we are able to operationalize the concept of integrated analysis describing both the environmental and economic effects of agricultural activities using one common data source. Since the applied economic and environmental models are based on the same data set, the problems with inconsistencies between data for economic and environmental analysis (see e.g. Lee and Lovejoy, 1991 , Antle and Capalbo, 1993 , Bockstael et al., 1995 , Hoogervorst, 1995 ) are significantly reduced.",
              "With such a model-set up the geographically distributed effects of different agricultural policy measures can be analyzed in an integrated framework comprising both the economic and environmental effects. Important aspects of geographical heterogeneity and differences between types of production can hereby be handled in order to analyze the cost-effectiveness properties of different agri-environmental policy measures.",
              "Joint ecological and economics modeling in this area will potentially have large contributions to the valuation of landscape amenities and the understanding of human impacts on ecosystems. One goal of ecological economics should be the integration of modeling approaches and relevant techniques. This paper hopes to promote that goal.",
              "Standard economic modelling approaches seeking the optimum or least cost solution fail as they cannot incorporate the wide range of factors that need to be included in a decision that must be based on achieving the Best Practicable Environmental Option (BPEO). Decision-makers do need assistance in making strategic choices that cause social and environmental impacts, and tie up large amounts of money and land for significant periods of time. The approach presented here is a first step in developing an ecological economic modelling approach that attempts to integrate life cycle inventory analysis, environmental impact assessment and economic appraisal within a geographic information system (GIS) framework. The aim is not to provide an \"optimum\" solution but to highlight to decision-makers the trade-offs inherent through investing in different mixes of waste management technology at a range of scales from the local to the regional.",
              "## **Enhanced Data Analysis and Visualization**GIS provides powerful tools for visualizing spatial data, which can be combined with economic models to illustrate the geographic distribution of economic activities and their environmental impacts. This visualization helps policymakers identify critical areas that require intervention and understand the spatial dynamics of environmental issues.For instance, GIS can be used to map land cover changes and correlate them with economic activities such as agriculture or urban development. This approach was demonstrated in a study where Landsat TM images and GIS techniques were used to assess landscape changes in Izmir, Turkey, facilitating the analysis of landscape dynamics and thematic mapping[3].",
              "## **Dynamic Environmental and Economic Modeling**GIS-based spatial analysis can be combined with dynamic economic models to simulate various environmental and economic scenarios. This approach allows for the assessment of the long-term impacts of different policy options. For example, a study on the integration of GIS and the Automated Land Evaluation System (ALES) in Egypt evaluated the physical and economic suitability of land for different crops, helping to optimize land use and improve agricultural productivity[9].",
              "The integration of GIS and economic modeling can lead to more targeted and effective environmental policies:a) Spatial targeting: Policies can be tailored to specific geographic areas based on their unique environmental and economic characteristics.b) Cost-benefit analysis: Spatial analysis can improve the accuracy of cost-benefit assessments by accounting for geographic variations in costs and benefits.c) Policy impact assessment: GIS-based models can simulate the potential impacts of different policy scenarios across space and time, helping policymakers choose the most effective interventions.",
              "Integrating GIS with economic modeling can lead to more accurate and spatially explicit economic analyses:a) Spatial econometrics: This approach incorporates spatial relationships into economic models, accounting for factors such as proximity and spatial autocorrelation.b) Land use change modeling: GIS-based economic models can predict how policy changes might affect land use patterns and associated environmental impacts.c) Ecosystem services valuation: Spatial analysis can help in mapping and valuing ecosystem services, providing a more accurate assessment of the economic benefits of environmental conservation.",
              "### Scenario Analysis and Predictive ModelingThe integration allows for robust scenario analysis by simulating the environmental and economic impacts of different policy options. For example, GIS can model land-use changes, while economic models can estimate the economic costs and benefits of those changes. This synergistic approach enables not only predictive modeling but also the assessment of trade-offs and synergies between economic development and environmental sustainability.",
              "## **Support for Sustainable Development**GIS and economic modeling can support sustainable development by identifying areas where economic activities can be balanced with environmental conservation. A study on regional spatial development used GIS and cellular automata to model multiple socio-economic scenarios, helping regional authorities in the Russian Far East to plan sustainable development and inter-regional cooperation[8]."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should mention challenges of integration of GIS spatial analysis techniques with economic modeling in the context of environmental policy.",
            "weight": 0.12,
            "evidence": [
              "### Data Quality and AvailabilityOne of the main challenges is the availability and quality of spatial and economic data. In some regions, especially in developing countries, data may be scarce or unreliable. Ensuring high-quality data is crucial for accurate modeling and analysis.### Computational ComplexityCombining high-resolution spatial data with complex economic models can be computationally intensive. Advancements in computational power and algorithms, as well as better access to high-performance computing resources, are necessary to manage the increased complexity.### Interdisciplinary CollaborationSuccessful integration requires collaboration between geographers, economists, environmental scientists, and policymakers. This interdisciplinary approach can sometimes be hindered by differences in terminologies, methodologies, and perspectives. Encouraging collaborative research and fostering cross-disciplinary communication are essential steps."
            ]
          }
        ]
      }
    },
    "case_id": "99fd4c1b54e1367bcd3cb3b243c2cc19",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "How has the citation graph been used to improve neural language models for scientific papers?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "How has the citation graph been used to improve neural language models for scientific papers?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define what a citation graph is and its significance in scientific research.",
            "weight": 0.17142857142857143,
            "evidence": [
              "The citation graph is a computational artifact that is widely used to represent the domain of published literature. It represents connections between published works, such as citations and authorship. Among other things, the graph supports the computation of bibliometric measures such as h-indexes and impact factors.",
              "Citation graphs can be helpful in generating high-quality summaries of scientific papers, where references of a scientific paper and their correlations can provide additional knowledge for contextualising its background and main contributions. Despite the promising contributions of citation graphs, it is still challenging to incorporate them into summarization tasks. This is due to the difficulty of accurately identifying and leveraging relevant content in references for a source paper, as well as capturing their correlations of different intensities.",
              "Citation graphs have been leveraged in several innovative ways to enhance neural language models for scientific papers, improving tasks such as paper recommendation, citation prediction, and document summarization.",
              "Neural models have incorporated citation graphs to enhance document representations and capture relationships between papers. This approach combines textual semantics with structural information from citation networks, leading to improved performance in various tasks related to scientific literature analysis.",
              "Understanding the context in which a paper is cited provides valuable insights into the influence, relevance, and contribution of the work. Traditionally, language models trained purely on textual content might miss out on these contextual nuances that citations provide."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention how citation graphs are used in neural language models for scientific papers.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Although the citation graph plays a promising role in improving the automatic summarization of scientific papers, little attention has been focused on incorporating them in existing pre-trained language models (PLMs) based summarization methods [16, 19 ]. The only exception is CGSum [3 ] which leverages references of source papers by constructing a citation graph to improve its summary generation.",
              "A better alternative is to incorporate the full contents of references instead of their abstracts. The challenge lies in bringing the full text of the references can introduce redundant information, the content of the references may be irrelevant to the source paper except for some key sentences as shown in Table 2. Moreover, different parts of references, such as introduction, related work, methods and experiments, have varying levels of semantic similarities with the source paper",
              "Previous work for text summarization in scientific domain mainly focused on the content of the input document, but seldom considering its citation network. However, scientific papers are full of uncommon domain-specific terms, making it almost impossible for the model to understand its true meaning without the help of the relevant research community. In this paper, we redefine the task of scientific papers summarization by utilizing their citation graph and propose a citation graph-based summarization model (CGSUM) which can incorporate the information of both the source paper and its references",
              "Researchers have developed hybrid models that combine pre-trained language models with graph neural networks (GNNs) to improve scientific paper recommendations[1]. These models:- Build user-paper bipartite graphs based on citation relationships- Initialize paper embeddings using pre-trained language models on paper titles and abstracts- Refine embeddings using message-passing layers on the citation graph",
              "Advanced models for citation prediction utilize heterogeneous publication networks that include nodes for papers, authors, venues, and terms[4]. Key innovations include:- Constructing heterogeneous graphs to model multiple factors influencing a paper's impact- Developing cluster-aware modules to consider latent research domains- Incorporating text-enhancing modules for automatic quality term mining",
              "representations by leveraging citation graph embeddings[3]. This method:- Uses controlled nearest neighbor sampling over citation graph embeddings- Enables learning of continuous similarity between papers- Allows for sampling of hard-to-learn positive and negative examples",
              "Citation graphs have also been used to enhance scientific paper summarization[6]. The CGSum model:- Incorporates information from both the source paper and its references- Utilizes the citation network to better understand domain-specific terms- Achieves competitive performance compared to pre-trained models",
              "### Traditional Neural Language ModelsTypical neural language models, such as BERT (Bidirectional Encoder Representations from Transformers) [1] and GPT (Generative Pre-trained Transformer) [2], have shown success in natural language processing tasks. However, these models operate primarily on the textual content of papers and do not explicitly integrate citation contexts.",
              "## 3. Methods of Integrating Citation Graphs### 3.1 Graph-Based EmbeddingsGraph embeddings transform nodes (papers) into vectors that capture their relationships in the graph structure. These embeddings can be integrated with text-based embeddings from neural language models:- **Graph Convolutional Networks (GCNs)** [3] and **Graph Attention Networks (GATs)** [4]: These are used to learn embeddings from the citation graph, which can then be combined with text features from papers to improve the model's understanding.### 3.2 Citation-Aware ModelsModels such as **CiteBERT** and **SciBERT** [5] have been developed to incorporate citation context:- **CiteBERT**: An extension of BERT that leverages citation contexts by embedding citation sentences and their surrounding text.- **SciBERT**: Pre-trained on a large corpus of scientific text, utilizes citation networks to provide richer contextual understanding.### 3.3 Multimodal LearningCombining multiple data sources:- **Text and Graph Fusion**: Combining text representations from transformers with graph embeddings for more robust paper representations.- **Hypergraphs**: Utilizing hypergraphs, where edges can connect more than two nodes, to represent multi-way relationships among papers and citations [6]."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should explain how citation graphs can improve neural language models for scientific papers.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Academic citation graphs represent citation relationships between publications across the full range of academic fields. Top cited papers typically reveal future trends in their corresponding domains which is of importance to both researchers and practitioners. Prior citation prediction methods often require initial citation trends to be established and do not take advantage of the recent advancements in graph neural networks (GNNs). We present GNN-based architecture that predicts the top set of papers at the time of publication. For experiments, we curate a set of academic citation graphs for a variety of conferences and show that the proposed model outperforms other classic machine learning models in terms of the F1-score.",
              "More recent research works put forward the idea that citation graphs can be instrumental in creating top-notch summaries of scientific papers (Chen et al., 2022; Luo et al., 2023). LLMs, with their proficiency in deriving complex text representations from dense vectors and generating high-quality text, are gaining prominence in complex tasks like citation-based summarization. Hence, there's an enhanced capability to encode citation context and integrate citation network graphs, thereby further boosting citation-based summarization efforts (Luo et al., 2023).",
              "Utilizing a batch-wise contrastive learning objective, ConGraT (Brannon et al., 2023) is a self-supervised framework for simultaneously learning individual representations for texts and nodes within a parent graph, such as citation graphs in Pubmed with each paper related to a specific node. Wang et al. (2022a) propose E5, a new family of text embeddings trained using weak supervision signals in a contrastive manner on a large text pair dataset, CCPairs",
              "SPECTER (Cohan et al., 2020) achieves document-level representation using citation-aware Transformers. It captures interdocument relatedness in the citation graphs, utilizes citations as an inter-document incidental supervision signal, and transforms this signal into a triplet-loss pre-training objective.",
              "grained citation graph structure information. With the rapid development of Graph Neural Networks (GNNs), recent researchers propose to integrate GNNs into LLMs by fusing the node representation of citation graphs, facilitating better understandings of scientific literature as well as natural language.",
              "Some researchers propose to infuse citation graph inputs using a cascaded model architecture where LLMs independently encode the textual input as embeddings and then GNNs are applied to amalgamate these embeddings. Guan and Jiang (2022) presents Citation Graph Collaborative Filtering (CGCF), which is a combination of document representation and Graph Neural Network, to improve automated recommendation systems for scientific articles.",
              "GraphFormers (Yang et al., 2021) integrates Graph Neural Networks (GNN) components into the transformer blocks of language models for more efficient textual graph representation learning. i.e. they demonstrated the model was effective on the DBLP citation prediction task. Unlike existing works, GraphFormers blends text encoding and graph aggregation into an iterative workflow, thereby comprehending each node's semantics from a global perspective.",
              "GRAD concurrently optimizes a GNN teacher model and a graph-free student model via a shared LM, which allows the student model to use the graph information encoded by the teacher model. Roethel et al. (2023) enhance deep learning language models by incorporating graph represented information from citation graphs with an early fusion strategy",
              "## 4. Applications and Benefits### Enhanced Document UnderstandingIncorporating citation graphs allows models to capture finer details in the scientific discourse:- **Contextual Relevance**: More accurately identifying the relevance of a paper in various contexts.- **Summarization**: Producing summaries that reflect not just the content, but also the influence and reception of a paper.### Improved Recommendations and SearchCitation-aware language models enhance academic search engines and recommendation systems by improving the relevance and precision of search results and recommendations.### Citation PredictionPredicting which papers are likely to cite each other can be improved by understanding citation contexts, a task benefitting various applications such as trend anticipation and identifying emerging fields."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should mention some challenges of integrating citation graphs with neural models for scientific papers.",
            "weight": 0.08571428571428572,
            "evidence": [
              "## 5. Challenges and Future Directions### ScalabilityIntegrating large-scale citation graphs with neural models demands considerable computational resources.### Quality of Citation DataThe utility of citation-aware models is contingent on the quality and comprehensiveness of the underlying citation data.### InterpretabilityIdentifying the specific ways in which citation contexts enhance model outputs remains a challenge, necessitating advanced interpretability techniques.### Future Research AreasOngoing research is focusing on dynamic citation networks, real-time updates to models with new citation data, and even richer contextual embeddings incorporating multimedia content and authorship networks."
            ]
          }
        ]
      }
    },
    "case_id": "2dc0ff181a621680dde0a48e0d63f0d9",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are some common UI designs for sense-making, information organization, and AI writing tools?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are some common UI designs for sense-making, information organization, and AI writing tools?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define sense-making, information organization, and AI writing tools.",
            "weight": 0.15,
            "evidence": [
              "Sensemaking as a method to understand complexity has been elaborated in complex systems literature with focus mainly on operations (Weick & Sutcliffe, 2015). However, the introduction of AI triggers a need for a new perspective on how to make sense of the systems prospectively as they transform into complex intelligent systems. The introduction of AI can be anticipated to show similarities to other technology shifts (Orlikowski & Gash, 1994), but the characteristics of AI also suggest that sensemaking processes can be influenced e.g., AI connects historical data with predictions in a new and more distinct way. It also explicates explainability issues where actors not directly involved in the development of AI need to understand what is going on.",
              "Sensemaking is a practice to help us bridge the gap between data and experience. It is fundamentally an exercise of organizing information and engaging the people closest to the source of that information to derive meaning from it. Meaning means being able to look at something and answer the question: \"Why does this matter?\" However, this endeavor requires a degree of preparation. Much of the data we have available from M&E collection efforts can seem fragmented and unrelated, which can cause us to miss much of its meaning. Additionally, individuals' interpretations of the data can lack related and relevant information, such as changes in the context (e.g. political, security, socioeconomic trends), stakeholder narratives (e.g. focus group notes, key informant interviews), or institutional knowledge from prior experiences. Merging this wide range of information by finding the relationships between them and making connections is a necessary step in the discovery of new insights",
              "Information organization tools help users manage, sort, and retrieve information efficiently. Effective UI design in this area ensures that the information is accessible and manageable.",
              "AI writing tools leverage artificial intelligence to assist users in creating, editing, and improving written content."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention common UI designs that facilitate sense-making.",
            "weight": 0.15,
            "evidence": [
              "we needed to find creative ways to organize and visualize the data in a way that would reveal and articulate patterns and relationships between the different sources. The goal of this part of the process was to offer a visual representation that captures the aggregation and interconnectedness of the data, which could then serve as a more holistic point of reference for the team to derive further insights and implications during the Sensemaking Circle in the next step. One of the most effective methods we used to accomplish this was mapping the foundations of the ERAs in relation to contextual security threat data.",
              "Compendium is a hypermedia software tool, providing a visual interface for mapping the connections between people, ideas, multimedia documents and websites, to supoprt the analysis of socio/technical problems. You can customise the icons and links to anything you want, but it comes preloaded with the visual language for IBIS: Issue-Based Information System, which supports the mapping of debates in terms of Issues, Ideas, Pros/Cons/Arguments, and Decisions (see illustrations).",
              "SciVs are colorful pictures, formed from abstract data, that often attempt to appear similar to the phenomena being represented. An example of such a visualization is a color map of the world where the color indicates the temperature. A rainbow spectrum is employed, where blue stands for cold and red for warm, much as it is on television weather reports.",
              "This paper aimed at studying how managers can exploit recent BI tools, providing detailed data visualization features, to better understand the business environment (sense-making) and taking decision (decision-making). Data visualization, and its tools, proved to be crucial for each firm, given a better management of (big) data",
              "Dashboards are a popular UI design for sense-making, particularly in learning analytics and data visualization contexts. Key features include:- **Transparency of design**: Ensuring that the visualization and data presentation are clear and understandable to users[1].- **Reference frames**: Providing context and comparison points to help users interpret the data[1].- **Support for action**: Offering actionable insights and recommendations based on the presented information[1].",
              "Sense-making tools help users understand complex information and draw insights from data. Some common UI designs for sense-making include:a) Node-Link Diagrams:These visualizations represent relationships between entities as nodes connected by lines or arrows. They are particularly useful for exploring networks and hierarchies.Citation: [1] Herman, I., Melancon, G., & Marshall, M. S. (2000). Graph visualization and navigation in information visualization: A survey. IEEE Transactions on Visualization and Computer Graphics, 6(1), 24-43.b) Treemaps:Treemaps display hierarchical data as nested rectangles, with the size of each rectangle representing the quantity of a particular attribute.Citation: [2] Shneiderman, B. (1992). Tree visualization with tree-maps: 2-d space-filling approach. ACM Transactions on Graphics, 11(1), 92-99.c) Heatmaps:Heatmaps use color-coding to represent data values in a matrix format, making it easy to identify patterns and trends.Citation: [3] Wilkinson, L., & Friendly, M. (2009). The history of the cluster heat map. The American Statistician, 63(2), 179-184.d) Interactive Dashboards:Dashboards combine multiple visualizations and controls to provide a comprehensive view of data and allow users to explore different aspects of the information.Citation: [4] Few, S. (2006). Information dashboard design: The effective visual communication of data. O'Reilly Media, Inc."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should mention common UI designs used in information organization tools.",
            "weight": 0.15,
            "evidence": [
              "A general paradigm of data visualization through visual tools defines different levels of exploration: usually users are focused on a general overview of the overall information, then they process the data with proper filters and zooming functions and at last they ask for more details. This organization is also known as the Shneidermann's mantra: \"overview first, zoom and filter, details-on-demand\" [2]. We followed this approach to obtain useful data organization and visualization.",
              "The visualized information can be dynamically rearranged and shared with other users. The usual visualization like pie-charts, histograms, tables have been used together with an ad-hoc visualization able to rely a greater number of variables",
              "Define the data to be represented: Data should be categorized in order to determine the manner in which it is mapped -- quantitative data, original data, or categorical data.Define the dimensions required to represent the data: Dimensions and attributes should be outlined in order to determine the types of analysis that can be conducted -- univariate analysis, bivariate analysis, trivariate analysis, or multivariate analysis.Define the structures of the data: Data relationships are commonly structured as either linear, temporal, spatial, hierarchical, or networked relationships.",
              "For information organization, UI designs often focus on:- **Visual hierarchies**: Using headers, subheaders, and different text sizes to structure information clearly[5].- **Interactive elements**: Incorporating features like drag-and-drop, collapsible sections, and filters to allow users to customize their view of information.- **Search functionality**: Implementing robust search capabilities to help users quickly find specific information.",
              "Information organization tools help users structure, categorize, and retrieve information efficiently. Common UI designs include:a) Hierarchical Folder Structures:This classic design organizes information in a tree-like structure, allowing users to create nested categories and subcategories.Citation: [5] Barreau, D., & Nardi, B. A. (1995). Finding and reminding: file organization from the desktop. ACM SigChi Bulletin, 27(3), 39-43.b) Tag-based Systems:These systems allow users to assign multiple labels or tags to items, enabling flexible categorization and easier retrieval.Citation: [6] Golder, S. A., & Huberman, B. A. (2006). Usage patterns of collaborative tagging systems. Journal of Information Science, 32(2), 198-208.c) Kanban Boards:Originally developed for manufacturing, Kanban boards have been adapted for information management, using columns to represent different stages or categories.Citation: [7] Anderson, D. J. (2010). Kanban: Successful evolutionary change for your technology business. Blue Hole Press.d) Mind Maps:Mind maps are radial diagrams that represent ideas and concepts branching out from a central topic, helping users organize information hierarchically and visually.Citation: [8] Buzan, T., & Buzan, B. (1993). The mind map book: How to use radiant thinking to maximize your brain's untapped potential. Plume.",
              "Information organization tools help users manage, sort, and retrieve information efficiently. Effective UI design in this area ensures that the information is accessible and manageable.### 1. Tree Structures and Hierarchical ViewsHierarchical file systems, folder views, and nested lists help in organizing information in a structured manner.- **Example Tools:** File Explorer (Windows), Finder (MacOS)### 2. Tagging and Labeling SystemsUsing tags or labels allows for cross-referencing and categorizing information that does not strictly fit into one category.- **Example Tools:** Evernote, Notion### 3. Kanban BoardsThese are visual project management tools that represent tasks or information on cards, which can be manipulated across different columns (e.g., To-Do, In Progress, Done).- **Example Tools:** Trello, Asana### 4. Search and Filtering MechanismsSearch bars, advanced filters, and sorting options enable quick retrieval of the required information from large datasets.- **Example Tools:** Human Resource Management Systems (HRMS), Content Management Systems (CMS)### 5. Timeline InterfacesVisual timelines help in organizing information chronologically, providing a clear temporal context.- **Example Tools:** Timeline JS, Tiki-Toki"
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should mention common UI designs that are used for AI writing tools.",
            "weight": 0.15,
            "evidence": [
              "Cognitive load is the amount of mental effort required to understand and complete a task and is broadly applicable to all areas of human-centered design as well as facets of behavioral and educational psychology [2]. It can refer to the total amount of information presented at once, how difficult it is to process and remember, or how long it takes for a user to become familiar with a process or system. Within the domain of human-computer interaction, cognitive load represents a constraint on processes and user interface design, including considerations towards how much information is presented by the interfaces (visual or otherwise), the simplicity of navigation paths and menus, the clarity and conciseness of how instructions or feedback is processed, and the amount of customization is possible to accommodate user needs.",
              "Main Editor. The central view of our interface is the main editor panel (Figure 1), which mirrors existing text editors [DP2] and provides a space for users to author documents. One way users can teach GhostWriter about their target writing style is simply by writing text. After each n (default: 100) new characters are written, the system will analyze the current document to extract its style [DP1].",
              "Left Panel. On the left sidebar (Figure 1), users can (1) view their document list and (2) explore the system's current style and context. (1) allows users to easily create new documents and switch between them, mirroring existing editor interfaces [DP2, DP3]. (2) offers a summarized view of the style and context information GhostWriter is using to generate personalized text [DP4]",
              "Style Toolbar. Above the main editor, there is a style toolbar (Figure 1), where the user can customize their GhostWriter experience [DP1] and further inspect the system's style knowledge [DP4]. Users can toggle the Track Style Of This Document flag, which turns on/off automatic style updates triggered by the current document (vs. the global style lock in Figure 5b).",
              "AI writing tools are increasingly incorporating UI designs that facilitate collaboration between humans and AI. Common features include:- **Co-authoring interfaces**: Designs that allow users to work alongside AI in real-time, such as split-screen views or inline suggestions[3].- **Ideation support**: UI elements that help users brainstorm and expand on ideas, often through prompts or visual cues[3].- **Feedback mechanisms**: Interfaces that allow users to provide input on AI-generated content, improving the collaboration process.",
              "AI writing tools leverage artificial intelligence to assist users in creating, editing, and improving written content. Common UI designs for these tools include:a) Suggestion Panels:These panels display AI-generated suggestions for improving grammar, style, or content alongside the user's text.Example: Grammarly's sidebar suggestionsb) Inline Editing:This design integrates AI suggestions directly into the text, often using underlines or highlights to indicate areas for improvement.Example: Microsoft Word's Editor featurec) Contextual Menus:These menus appear when users select text or right-click, offering AI-powered options for rephrasing, expanding, or simplifying content.Example: QuillBot's paraphrasing toold) Chat-like Interfaces:Some AI writing tools use a conversation-style interface, allowing users to interact with the AI through prompts and responses.Example: OpenAI's ChatGPT interfacee) Split-screen Editors:These interfaces display the original text on one side and the AI-generated or modified text on the other, allowing for easy comparison and editing.Example: Jasper AI's document editor"
            ]
          }
        ]
      }
    },
    "case_id": "a8ba07610b6d77890e50144bfd4d4168",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "In auditory speech perception using iEEG, I will use Temporal Response Function (TRF) to determine correlations between stimulus characteristics (variations in the acoustic signal envelope, for example) and characteristics of recorded neuronal activity. I would like to fully understand the different stages of data processing carried out, as well as the reasoning and hypotheses behind them.",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "In auditory speech perception using iEEG, I will use Temporal Response Function (TRF) to determine correlations between stimulus characteristics (variations in the acoustic signal envelope, for example) and characteristics of recorded neuronal activity. I would like to fully understand the different stages of data processing carried out, as well as the reasoning and hypotheses behind them.",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define auditory speech perception and the role of iEEG (intracranial electroencephalography) in studying it.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Speech perception encompasses numerous processes such as acoustic analysis, phonetic and phonological processing, lexical access, and semantic comprehension (Pei et al., 2011; Herff et al., 2015). These processes are interconnected, often occurring in parallel, which leads to intricate neural representations of perceived speech within the brain (Brandmeyer et al., 2013; Mesgarani et al., 2014).",
              "Research into speech perception has revealed the involvement of several key brain regions. The superior temporal gyrus (STG) and the posterior superior temporal sulcus (pSTS) are particularly integral for processing acoustic features and phonetic components of speech (Mesgarani et al., 2014; Okada et al., 2010). These areas respond to various speech sounds and their characteristics, and their activation patterns often mirror the spectro-temporal dynamics of the incoming speech signal.",
              "Interestingly, the perception of speech also engages brain regions traditionally associated with speech production. For instance, Broca's area, known for its role in speech production, also plays a part in speech perception, particularly when listeners are anticipating or predicting upcoming speech sounds (Friederici, 2011). Similarly, activity in motor-related areas like the motor cortex and the cerebellum has also been observed during speech perception, potentially reflecting the listeners' internal simulation or mirroring of the speaker's articulatory movements (Eichert et al., 2020; Lotte et al., 2018)."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should explain what Temporal Response Function (TRF) is and how it is used in the context of iEEG.",
            "weight": 0.19999999999999998,
            "evidence": [
              "temporal response function (TRF) analyses of neural activity recordings evoked by continuous naturalistic stimuli have become increasingly popular for characterizing response properties within the auditory hierarchy. However, despite this rise in TRF usage, relatively few educational resources for these tools exist. Here we use a dual-talker continuous speech paradigm to demonstrate how a key parameter of experimental design",
              "The use of the continuous speech stimulus paradigm, combined with TRF analysis, sidesteps this temporal overlap issue by deconvolving the sustained response from the stimulus, which often allows direct comparison of neural source peak latencies. Though typical uses of TRF analysis employ the slow (<10 Hz) acoustic envelope as the stimulus feature with which to deconvolve (Di Liberto et al., 2015; Cervantes Constantino and Simon, 2018), the TRF methodology generalizes well to other stimulus features",
              "The Temporal Response Function (TRF) is a linear modeling technique used to describe the relationship between a continuous stimulus and the corresponding brain response. In the context of auditory speech perception, the TRF can help characterize how acoustic features of speech, such as the envelope of the sound, influence neuronal activity over time.",
              "TRF Definition and ConceptTLDR: Temporal Response Function (TRF) is a linear mapping technique used to relate stimulus characteristics to neural responses in auditory processing. It employs convolution to expand the temporal receptive field and can be applied to various stimulus representations like speech envelopes.The Temporal Response Function (TRF) is a crucial tool in analyzing the relationship between auditory stimuli and neural responses. It is conceptually similar to the Spectro-Temporal Response Function (STRF), which is defined as a linear mapping from the stimulus spectrogram to a neuron's instantaneous firing rate response  (1, SHAMMA et al., 2007). The TRF extends this concept by establishing an integrated linear response of neural activity over time, using convolution in the temporal dimension to expand the temporal receptive field  (10, Yu et al., 2022)."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should outline the different stages of data processing involved in using TRF for auditory speech perception studies, as well as the reasoning and hypotheses behind them.",
            "weight": 0.19999999999999998,
            "evidence": [
              "2.3 Data acquisition and preprocessingThe data were collected from subjects using a 157 axial gradiometer whole head KIT (Kanazawa Institute of Technology) MEG system with subjects resting in the supine position in a magnetically shielded room (Vacuumschmelze GmbH & Co. KG, Hanau, Germany). The data were recorded at a sampling rate of 1 kHz with an online 200 Hz low pass filter with a wide transition band above 200 Hz and a 60 Hz notch filter. Data were preprocessed in MATLAB by first automatically excluding saturating channels and then applying time-shift principal component analysis (TSPCA; de Cheveigne and Simon, 2007) to remove external noise, and sensor noise suppression (SNS; de Cheveigne and Simon, 2008) to suppress channel artifacts. Two of the sensor channels were excluded during the preprocessing stage.",
              "2.4 Neural source localizationPrior to the data collection, the head shape of each subject was digitized using a Polhemus 3SPACE FASTRAK system, and subject head position in the MEG scanner was measured before and after the experiment using five marker coils. The marker coil locations and the digitized head shape were used to co-register the template FreeSurfer \"fsaverage\" brain (Fischl, 2012) using rotation, translation, and uniform scaling.",
              "MRI-MEG coregistration and source reconstruction. The coregistration of MEG data with the individual's structural MRI was conducted by realigning the digitized fiducial points with MRI slices. Using MRILAB (Neuromag-Elekta), fiducials were aligned manually on the MRI slice. Individual forward solutions for all source reconstructions located on the cortical sheet were next computed using a 3 layer boundary element model (Hamalainen and Sarvas, 1989; Mosher et al., 1999) constrained by the individual aMRI.",
              "Pre-processing. In order to remove power line artifacts, we first applied a notch filter at 50 Hz and harmonics up to 300 Hz. The signal was then re-reference using a bipolar montage, i.e. activity of each channel was subtracted from its following neighbour on the electrode.",
              "Frequency following response analysis. In order to study the FFR, we re-epoched the signal from -0.05 to 0.35 s relative to the onset of each tone. We computed the evoked activity as the mean across epochs. In order to study only the high-frequency component of the evoked response, we filtered it between 82 and 84 Hz for the 83 Hz tones and between 61 and 63 Hz for the 62 Hz tones",
              "Temporal response function analysis. To analyse power signals, we relied on a recently developed powerful methodology that estimates temporal response functions (TRF) by relying on encoding/decoding models of electrophysiological activity 61. TRF rely on the assumption that the activity can be expressed as a linear convolution between the input stimulus and a filter.",
              "## Data Collection and Preprocessing1. **iEEG Recording**: Intracranial electrodes are used to record neural activity while participants listen to speech stimuli. This method provides high spatial and temporal resolution compared to non-invasive techniques[5].2. **Speech Stimulus Preparation**: The acoustic features of the speech stimuli, such as the envelope, are extracted. The envelope represents the overall amplitude fluctuations of the speech signal over time[1][4].3. **Data Alignment**: The iEEG recordings are temporally aligned with the speech stimuli to ensure accurate mapping between neural responses and acoustic features[6].",
              "## TRF Analysis4. **TRF Estimation**: The TRF is computed to model the relationship between the speech features (e.g., envelope) and the neural responses. This step involves:a) Selecting appropriate time lags to account for neural processing delaysb) Choosing a regularization method to prevent overfittingc) Estimating TRF coefficients using methods like ridge regression or boosting[9]5. **Model Validation**: The TRF model is validated using techniques such as cross-validation to ensure its generalizability[6].",
              "To fully understand the different stages of data processing in auditory speech perception using intracranial electroencephalography (iEEG) and Temporal Response Function (TRF), as well as the reasoning and hypotheses behind them, let's break down the process into several key sections:1. Data Acquisition2. Pre-processing3. Feature Extraction4. TRF Analysis5. Interpretation and Hypothesis Testing",
              "1. Data AcquisitionIn this stage, both the neural data and speech stimuli are recorded simultaneously.1.1 Neural Data Recording:- iEEG data is collected using electrodes implanted directly on or in the brain.- This method provides high spatial and temporal resolution compared to non-invasive techniques like EEG or MEG [1].",
              "1.2 Speech Stimuli Recording:- The acoustic signal of the speech stimulus is recorded, typically using a high-quality microphone.- The speech signal is time-locked with the neural recordings to ensure precise temporal alignment.Hypothesis: The high spatial and temporal resolution of iEEG will allow for the detection of fine-grained neural responses to speech features.",
              "2. Pre-processing2.1 Neural Data Pre-processing:- Removal of artifacts (e.g., line noise, movement artifacts)- Filtering to isolate relevant frequency bands- Re-referencing to a common average or bipolar montage- Epoching the data around stimulus onset2.2 Speech Signal Pre-processing:- Extraction of the acoustic envelope- Normalization of the speech signalReasoning: These steps are crucial for improving the signal-to-noise ratio and preparing the data for subsequent analysis. The choice of pre-processing steps can significantly impact the results, so careful consideration is needed [2].",
              "3. Feature Extraction3.1 Neural Features:- Time-domain features (e.g., event-related potentials)- Frequency-domain features (e.g., power in specific frequency bands)- Time-frequency representations (e.g., spectrograms)3.2 Speech Features:- Acoustic envelope- Spectral features (e.g., formant frequencies)- Linguistic features (e.g., phoneme onsets, word boundaries)Hypothesis: Different neural features will show varying degrees of correlation with different speech features, reflecting the hierarchical nature of speech processing in the brain [3].",
              "4. TRF Analysis4.1 Model Specification:- Define the input features (speech characteristics)- Define the output features (neural activity characteristics)- Specify the time lags to be considered4.2 Model Estimation:- Use ridge regression or other regularization techniques to estimate the TRF- Perform cross-validation to optimize regularization parameters4.3 Model Evaluation:- Assess model performance using metrics like explained variance or correlation between predicted and observed neural responsesReasoning: TRF analysis allows for the investigation of how different speech features are encoded in neural activity across different time lags. This approach can reveal the temporal dynamics of speech processing in the brain [4].",
              "5. Interpretation and Hypothesis Testing5.1 Spatial Analysis:- Examine TRFs across different electrode locations- Map the results onto brain anatomy5.2 Temporal Analysis:- Investigate the timing of peak responses in the TRFs- Compare the latencies of responses to different speech features5.3 Statistical Analysis:- Perform permutation tests or bootstrapping to assess the significance of TRF components- Correct for multiple comparisons when necessaryHypothesis: Different brain regions will show distinct TRF patterns, reflecting their roles in the speech processing hierarchy. Earlier auditory areas may show stronger correlations with low-level acoustic features, while higher-level linguistic areas may correlate more strongly with more complex speech features [5]."
            ]
          }
        ]
      }
    },
    "case_id": "8f5a6ec4ad13c0b7881a9af2de00ca4a",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "Which frequency characteristics of seismic P-wave can be used to distinguish it from other waves?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "Which frequency characteristics of seismic P-wave can be used to distinguish it from other waves?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define seismic P-waves and their significance.",
            "weight": 0.13333333333333333,
            "evidence": [
              "The P wave, or primary wave, is the fastest of the three waves and the first detected by seismographs. They are able to move through both solid rock as well as through liquids. These are compressional or longitudinal waves that oscillate the ground back and forth along the direction of wave travel, in much the same way that sound waves (which are also compressional) move air back and forth as the waves travel from the sound source to a sound receiver. Compressional waves compress and expand matter as they move through it (Figure 2). Figure 1A shows that when a seismic wave comes from below, it bumps the house upward.",
              "P-waves (primary or compressional waves) are longitudinal or compression waves, able to move through solids, liquids, and gases at speeds ranging between 300-5,000 metres per second. As they travel through rock, they move particles back and forth in the same direction that the wave is moving. When P-waves strike an object they push and pull the object. It's like a train engine bumping into a railroad car, which then bumps into another. This movement continues through the whole length of the train.",
              "P waves, or Primary waves, are the first waves to arrive at a seismograph. P waves are the fastest seismic waves and can move through solid, liquid, or gas. They leave behind a trail of compressions and rarefactions on the medium they move through. P waves are also called pressure waves for this reason. Certain animals, such as dogs, can feel the P waves much before an earthquake hits the crust (surface waves arrive). Humans can only feel the ramifications it has on the crust.",
              "Seismic waves are vibrations that move through the Earth's interior or along its surface, typically generated by natural earthquakes, volcanic activity, or artificial explosions. They are primarily categorized into body waves (P-waves and S-waves) and surface waves (Rayleigh and Love waves).## Understanding P-WavesPrimary waves (P-waves) are a type of body wave that are compressional, meaning that the particles in the medium through which the P-wave passes move back and forth in the same direction as the wave is traveling. P-waves are the fastest seismic waves and are the first to be detected by seismographs. They can travel through both liquids and solids, differentiating them from S-waves, which can only move through solid materials."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention the frequency range of P-waves, and compare it with other waves.",
            "weight": 0.13333333333333333,
            "evidence": [
              "Frequency and distance changes in the apparent P-wave radiation pattern (0.75-12Hz) are investigated using velocity seismograms of shallow strike-slip earthquakes occurring in Chugoku region, southwestern Japan",
              "P motion travels fastest in materials, so the P-wave is the first- arriving energy on a seismogram. Generally smaller and higher frequency than the S- and Surface waves. P waves in a liquid or gas are pressure waves, including sound waves.",
              "They can move through any type of material and travel at almost twice the speed of S waves. High frequency P waves do not weaken, or \"attenuate,\" as rapidly as S waves so they retain higher frequencies when they arrive at seismic stations.",
              "- **Typical Frequency Range**: P-waves generally exhibit frequencies ranging from 0.1 Hz to 100 Hz, with dominant frequencies usually between 1 Hz and 20 Hz depending on the source of the seismic event and the properties of the medium they travel through (Chouet, 1996)."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss the amplitude, velocity, and frequency decay of P-waves, and compare it with other waves.",
            "weight": 0.13333333333333333,
            "evidence": [
              "In solids, these waves travel twice as fast as S-waves. For example, P-waves move at 330 m/s in air, 1,450 m/s in water, and 5,000 m/s in granite.",
              "P waves travel at speeds from about 6 km (3.7 miles) per second in surface rock to about 10.4 km (6.5 miles) per second near the Earth's core some 2,900 km (1,800 miles) below the surface. As the waves enter the core, the velocity drops to about 8 km (5 miles) per second. It increases to about 11 km (6.8 miles) per second near the centre of the Earth. The speed increase with depth results from increased hydrostatic pressure as well as from changes in rock composition; in general, the increase causes P waves to travel in curved paths that are concave upward.",
              "P-waves (primary or compressional waves) are longitudinal or compression waves, able to move through solids, liquids, and gases at speeds ranging between 300-5,000 metres per second.",
              "Most of the energy propagates as P waves or S waves. P waves are longitudinal waves, and S waves are transverse waves. Longitudinal waves are a class of waves in which the particles of the disturbed medium are displaced in a direction that is parallel to the direction of propagation of the wave. Transverse waves are a class of waves in which the particles of the disturbed medium are displaced in a direction that is perpendicular to the direction of propagation of the wave. The velocity of P waves depends on the elastic properties and density of the medium. The velocity of S waves depends on the shear modulus and density of the medium. S waves do not travel as fast as P waves. Other names exist for P waves and S waves. P waves are sometimes called compressional, primary, or pressure waves, while S waves are sometimes called shear or secondary waves.",
              "P-waves typically have higher velocities compared to other seismic waves, traveling through the Earth's interior at speeds ranging from 5 to 8 km/s in the crust, and even faster in the mantle. This high velocity often correlates with higher frequency content in seismic signals.",
              "The amplitude and frequency content of seismic waves diminish as they travel through the Earth due to geometrical spreading and material attenuation. However, P-waves tend to maintain higher frequencies over longer distances compared to S-waves and surface waves.- **Attenuation**: The intrinsic attenuation is often less for P-waves than for S-waves, which leads to P-waves retaining more of their higher frequency content as they propagate.",
              "- P-waves typically exhibit higher initial amplitudes compared to S-waves, which is useful for early detection and characterization of seismic events (LLM MEMORY, 2024)."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should mention spectral analysis and duration of P-waves in comparison with other waves.",
            "weight": 0.13333333333333333,
            "evidence": [
              "### Spectral Analysis and P-Wave Duration- **Spectrograms**: Spectral analysis using spectrograms can help visualize the energy distribution over different frequencies and times. P-waves are often identified by their sharp onset and high-frequency content in these visual representations, contrasting with the lower frequency and longer-duration energy distribution of surface waves."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer could explain some techniques such as Fourier transform that are used to analyze the frequency characteristics of seismic waves.",
            "weight": 0.06666666666666667,
            "evidence": [
              "Seismic data processing algorithms often can be described or implemented more simply in the frequency domain than in the time domain. The one-dimensional (1-D) Fourier transform is introduced and some basic properties of time series in both time and frequency domains are described. Many of the processing techniques -- single- and multichannel, involve an operand (seismic trace) and an operator (filter). A simple application of Fourier analysis is in the design of zero-phase frequency filters, typically in the form of band-pass filtering.",
              "The two-dimensional (2-D) Fourier transform is a way to decompose a seismic wavefield, such as a common-shot gather, into its plane-wave components, each with a certain frequency propagating at a certain angle from the vertical. Therefore, the 2-D Fourier transform can describe processes like migration and frequency-wavenumber (f - k) filtering.",
              "Recently, the world of two dimensional transforms has been considerably expanded by the introduction of new varieties of two dimensional multiresolution transforms. Examples of them are ridgelets, beamlets, ridgelet packets, curvelets, diplet and contourlets. Wavelet Transform Teager-Kaiser (WT-KE) Energy is a seismic attribute applied to reveal geological features. We use the Teager-Kaiser energy associated with wavelet transform to generate a joint time-frequency representation,which can be used as a nonlinear energy tracking of the seismic waves",
              "Seismic reflection data are non-stationary in nature, because of their frequency variation with time. The geoscientist has great interests in the trends and periodicities generated by complex subsurface geological structures. Generally, the Fourier transform is used to study these trends and periodicities in the geophysical time series. It is a mathematical tool that breaks down a time series signal into its component frequencies. Therefore, Fourier transform is a mathematical depiction of signal amplitudes of discrete components that construct it. Frequency-domain representation of the signal and the process of transformation from time to frequency domain are called Fourier transform [28]."
            ]
          }
        ]
      }
    },
    "case_id": "a2a5c8079442e178f078869471697f48",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What interfaces have researchers developed to help people perform behavioral evaluation of ML models, and how do they accelerate annotation efforts?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What interfaces have researchers developed to help people perform behavioral evaluation of ML models, and how do they accelerate annotation efforts?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define behavioral evaluation of ML models and its importance.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Machine learning models with high accuracy on test data can still produce systematic failures, such as harmful biases and safety is- sues, when deployed in the real world. To detect and mitigate such failures, practitioners run behavioral evaluation of their models, checking model outputs for specific types of inputs. Behavioral evaluation is important but challenging, requiring that practitioners discover real-world patterns and validate systematic failures.",
              "Behavioral evaluation of machine learning (ML) models involves assessing how models perform across various scenarios and inputs. Researchers have developed several interfaces to facilitate this process, making it easier for both experts and non-experts to evaluate ML models' behavior."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention specific examples of interfaces designed for behavioral evaluation of ML models",
            "weight": 0.19999999999999998,
            "evidence": [
              "To identify specific challenges for ML evaluation, we conducted formative interviews with 18 ML practitioners. From the interview results we derived four main design goals for an evaluation system, including supporting comparison over time and no-code analysis. We used these goals to design and implement zeno, a general purpose framework for defining and tracking diverse model behaviors across different ML tasks, models, and data types. zeno combines a Python decorator API for defining core building blocks with an interactive UI for creating slices and reports.\"dl.acm.org/doi/pdf/10.1145/3544548.358126",
              "This paper presents EvalLM, a novel interactive system that sup- ports prompt designers in refining LLM prompts by evaluating generated outputs on user-defined criteria. In the interface, the user composes pairs of prompts and a set of evaluation criteria, and then employs LLMs to both generate outputs with the prompts and evaluate the outputs on the criteria. Through the explanations provided by the LLM on its evaluation, the user can iteratively refine their prompts, by identifying where they lack, and their criteria, by identifying where they are unclear.\"dl.acm.org/doi/pdf/10.1145/3613904.3642216\"In this paper we propose the design of a graphical tool for fast evaluation of Machine Learning (ML) models performance in classification tasks. The motivation behind this work is to get some intuition on what machine learning model we can use to get the best possible outcome out of our datasets. The designed GUI allows us to decide whether applying data standardization and applying different data dimensionality reduction algorithms based on Principal Component Analysis (PCA). Also, we can choose between 6 generative and discriminative supervised ML classifiers for making the final predictions, including: Logistic Regression, Support Vector Machines, Random Forest, K-nearest Neighbors, Gaussian Naive Bayes and Neural Network (Multilayer Perceptron)\"IEEE Xplore Full-Text PDF",
              "### **1. Active Learning Interfaces**\u000bActive learning is a method where the ML model actively selects the most informative data points for annotation. This reduces the volume of data needed for training while speeding up the learning process. By focusing on the most representative samples, active learning minimizes the effort required for data annotation and enhances the efficiency of the model training process. For example, in the context of 6G networks, active learning frameworks can optimize both data acquisition and annotation, leading to improved computational efficiency and adaptability in network intelligence[1].",
              "### **2. Specialized Annotation Software**\u000b**WSI2ML** is an example of a specialized annotation software designed for computational pathology. This web-based platform provides a comprehensive toolset for each stage of the ML workflow, from annotation to model validation. By offering a user-friendly interface, WSI2ML simplifies the complex task of annotating whole slide images, which is particularly useful in clinical pathology research. The platform's integrated tools help streamline the annotation process, making it more efficient and accessible to multidisciplinary teams[2].",
              "### **3. Collaborative Annotation Tools**\u000bThe **Collaborative Semantic Annotation Tooling (CoAT)** is designed to improve the efficiency and interoperability of semantic annotations, particularly in the secondary use of medical data. CoAT facilitates a collaborative approach to annotation, allowing users to leverage prior annotations made by others. This not only speeds up the annotation process but also helps in achieving a consensus on ambiguous annotations. By enabling cross-institutional data usage and fostering a shared understanding of semantic annotations, CoAT enhances the overall quality and efficiency of the annotation process[3].",
              "Interactive features and visual feedback\u000bTLDR: Interactive features and visual feedback in ML evaluation interfaces enhance user engagement and understanding. These tools often incorporate explainable AI techniques, real-time model updates, and intuitive visualizations to streamline the annotation process and provide insights into model behavior.",
              "#### 1. Interactive Visualization Tools\u000bInteractive visualization tools provide visual representations of model predictions, facilitating a better understanding of model behavior. These tools often come with features to highlight errors, compare performance across different models, and visualize decision boundaries.",
              "#### 2. Model Debugging Frameworks\u000bModel debugging frameworks offer functionalities to test and debug ML models interactively. These frameworks provide interfaces to inspect, modify, and evaluate models on the fly, often supporting various types of inputs and modifications.",
              "#### 3. Comparative Analysis Platforms\u000bThese platforms allow users to perform side-by-side comparisons of multiple models or different versions of the same model. Such comparison aids in understanding how changes in model architecture or data affect performance."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should explain how these interfaces accelerate annotation efforts by utilizing machine learning, active learning, user-friendly platforms, and collaborative methods.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Model generation and testing: A HAR (i.e., ML or DL) model is developed to classify human activities based on the selected features in this phase. The model may be trained using a labeled dataset or unsupervised learning techniques. After the model has been generated, the following steps are carried out before the model is ready to be used: - Model evaluation: The developed model is then evaluated using a test dataset to assess its accuracy and performance. This phase helps to identify any issues or areas for improvement in the model. - Deployment: Finally, the developed model is deployed to a real-world environment, where it is used to classify human activities.",
              "When generating HAR model, a set of sensor data is recorded first. This data is then labeled with the activities under consideration. This step is called annotation. Next, a machine-learning model is trained, which can then be used to classify unlabeled data. In the following, we describe the individual steps [ 18, 25] that are involved in creating a HAR system in more detail.",
              "Prompt engineering is anticipated to become an important role within annotation as it can be used to direct segmentation and refine derived data. It creates an intuitive yet opaque interface for working [158-160]. Prompt engineering to summon agents and elicit outputs is probably the closest phenomena in our mundane world to fantasy fictional depictions of magic. It creates a powerful, intuitive, yet still somewhat obfuscated interface for working with machines. There is as much art as engineering in the development of effective prompts.",
              "The LOST (Label Objects and Save Time) open-source implementation permits the combination of multiple annotation tools and machine learning algorithms into an ensemble process, which can be strung together in a modular manner, visualized through a web-based user interface which supports the use of Python scripts to control processes. An annotation pipeline (annotation process) can be composed of six different building block types, namely data sources, scripts, annotation tasks, loops, data exports and visualizations, with each element separately parameterized",
              "we proposed four Bayesian models for obtaining consensus in continuous valued crowd labeling tasks by taking annotator behaviors into account. We also introduced a novel metric for measuring annotator quality. We acquired annotation data on a dataset with known ground truth for evaluating the performance of the proposed models. In addition, we adapted our methods to work with binary labeled data and reported their performance.",
              "- **Active Learning**: By selecting the most informative data points, active learning reduces the number of samples that need to be annotated, thus saving time and resources.- **User-Friendly Platforms**: Tools like WSI2ML provide intuitive interfaces that simplify the annotation process, making it accessible even to those without extensive technical expertise.- **Collaborative Approaches**: Tools like CoAT promote collaboration and reuse of existing annotations, reducing redundancy and speeding up the annotation process by building on the work of others.",
              "Machine learning integration to accelerate annotationTLDR: Machine learning techniques are increasingly integrated into annotation tools to accelerate the process and reduce human effort. These approaches include active learning, semi-automated labeling, and visual feedback systems that allow users to verify and correct machine-generated annotations efficiently."
            ]
          }
        ]
      }
    },
    "case_id": "5acd0e1d36af3c52c3159b4b230bcc2f",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What kinds of coverage motion planning algorithms are effective for floor-cleaning robots?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What kinds of coverage motion planning algorithms are effective for floor-cleaning robots?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define coverage motion planning in the context of floor-cleaning robots.",
            "weight": 0.12,
            "evidence": [
              "The critical challenge of the reconfigurable tiling robot is to generate the optimal trajectory and set of shapes to cover the entire area. Typically, standard path planning algorithms are focused on trajectory planning for the robot to move from a starting point to a goal efficiently. However, when it comes to tiling tasks, completely covering the space is just as important. Hence, Complete Coverage Path Planning (CCPP) approaches are employed to plan the motion of tiling based robots. This involves planning a path which is energy efficient and completely covers the given area while avoiding any obstacles present.",
              "Coverage path planning (CPP) is the computation of a path that fully covers a given map. It is relevant, for example, to mobile cleaning robots for floor cleaning [1], [2], lawnmowers [3], or autonomously driving agricultural machines [4]. Our motivation to engage in this comparison is the application of professional office cleaning robots [5], which have to fulfill several purposes. On the one hand, such a robot shall be capable of exhaustively cleaning larger areas, e.g. with a wet cleaning machine. On the other hand, the robot must be capable of quickly inspecting larger areas for the occurrence of waste bins to clear or local floor pollutions to clean. Because of its complexity, the coverage path planning problem is usually not computed on complete maps but rather on individual parts of a map"
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should discuss the randomized and heuristic algorithms such as random walk algorithm for floor-cleaning robots.",
            "weight": 0.12,
            "evidence": [
              "A random search does not guarantee complete coverage, but there are advantages to this approach. Balch, and separately Gage, have analyzed randomized robot search from a cost/benefit point of view [4,15]. Balch argues that robots executing random searches may not require costly localization sensors (e.g., GPS), nor do they consume valuable computational resources for calculating their position. Therefore, robots using randomized search strategies can be built for less cost than robots using methods that require precise positioning.",
              "In certain scenarios, a valid approach to solve the problem is to randomize. This is an approach that some floor-cleaning robots rely on: if the floor is swept randomly for long enough, it should become cleaned. Examples of commercial floor-cleaning robots based fully or partially on this strategy are the RC3000 by Karcher, Trilobite by Electrolux and Roomba by iRobot [21] . There are advantages to this approach, the main one being that no complex sensors for localization nor expensive computational resources are needed.",
              "## 1. Random Algorithms### 1.1 Random WalkThe simplest form of coverage is a random walk, where the robot moves in random directions and changes direction upon encountering obstacles. This method requires minimal computational resources but often results in inefficient coverage with possible areas left uncleaned or revisited multiple times.#### Pros:- Simple to implement- Low computational requirements#### Cons:- Inefficient coverage- High probability of missing areas or overlapping paths### 1.2 Probabilistic Roadmaps (PRM)PRM involves randomly sampling the environment to create a network of navigable points. The robot then uses this network to navigate the area. While suitable for complex environments, it may not guarantee full coverage unless combined with additional heuristics.#### Pros:- Capable of handling complex, high-dimensional spaces- Flexible approach#### Cons:- Computationally expensive- Does not guarantee complete coverage"
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should mention cellular decomposition methods, where the environment is divided into smaller cells/slices and the robot follows a pre-defined pattern to cover each cell/slice.",
            "weight": 0.12,
            "evidence": [
              "One group of methods is constituted by the classical exact cellular decompositions methods [10]-[12], the Morsebased cellular decomposition methods [13], as well as the Landmark-based cell decomposition algorithms [14], which divide the original map into smaller units that can be covered with a simple motion pattern.Whereas the classical exact cellular decompositions usually rely on polygonal structures and obstacles, this limitation is lifted with Morse-based decompositions. The popular Boustrophedon cell decomposition [11], [15] belongs to the cell decomposition methods and applies a simple back-and-forth motion inside the generated cells.\"IEEE Xplore Full-Text PDF",
              "Another type of coverage algorithms are cell grid-based methods, i.e. methods that divide the map into a regular grid of cells and find a path that covers all of these cells. The Wavefront Algorithm [17] defines a starting and a goal cell and propagates a wavefront from the goal to the start. Cells are visited in equidistant level sets of these wave fronts before approaching the target further. The Spanning Tree method [18] decomposes the free space into mega cells and constructs a spanning tree that covers all of these cells. Inside the mega cells there are 4 smaller cells, than can be visited by traversing the spanning tree.\"IEEE Xplore Full-Text PDF",
              "Many cells in the trapezoidal decomposition can be merged such that the robot can continue performing back and forth motions while covering the cell and not intersecting an obstacle. Choset and Pignon [11,12] developed a new decomposition termed the boustrophedon decomposition1 to address this \"clumped\" cell issue. In this method, a line segment, termed a slice, is swept through the environment. Whenever there is a change in connectivity of the slice, a new cell is formed. When the connectivity increases, two new cells are spawned.\"link.springer.com/content/pdf/10.1023/A:1016639210559.pdf\"### 2.1 Grid-based Coverage\u000bThis approach divides the floor into a grid and ensures that the robot covers each cell in the grid systematically. The best-known algorithm in this category is the Boustrophedon motion, resembling the way a lawnmower or farmer plows a field, moving back and forth in straight lines.\u000b#### Pros:\u000b- Ensures complete coverage\u000b- Relatively simple implementation\u000b#### Cons:\u000b- Inefficient in non-rectangular or complex environments\u000b- Requires accurate localization"
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should mention sensor-based algorithms for floor-cleaning robots.",
            "weight": 0.12,
            "evidence": [
              "## 3. Sensor-Based Algorithms### 3.1 Wall-FollowingWall-following algorithms use sensors to keep the robot close to walls and obstacles, ensuring the perimeter of the area is covered first. Once the perimeter is mapped out, the robot focuses on the remaining interior space.#### Pros:- Effective for complex-shaped areas- Avoids obstacles naturally#### Cons:- Requires good sensor accuracy- Might leave interior spaces uncovered if not combined with additional logic### 3.2 Frontier-Based ExplorationFrontier-based algorithms use sensors to identify the boundary between known and unknown spaces (the frontier). The robot systematically moves to these frontiers to cover new areas until all frontiers are explored.#### Pros:- Adaptive to dynamic environments- Ensures incrementally thorough coverage#### Cons:- Can be computationally intensive- Potentially complex to implement"
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should mention machine learning algorithms that are suitable for floor-cleaning robots.",
            "weight": 0.12,
            "evidence": [
              "Reinforcement learning has been used under the pretext of path planning before. Changxi et al. [23] proposed the use of reinforcement learning for trajectory planning for autonomous vehicles. Kenzo et al. [24] used the DDPG reinforcement learning algorithm to plan the motion of bipedal robots in a soccer match. Farad et al. [25] generated a path for efficient exploration in unknown environments through Actor-Critic Reinforcement learning model. A model trained using Q-Learning to generate a path from point A to point B in a grid-based decomposition of the environment has been proposed in Aleksandr et al. [26] , Amit et al. [27] and Soong et al. [28]",
              "Most cleaning robots are moving straightforwardly, and when obstacles such as a wall, chair, and table are located in front of the cleaning robot, they just change direction and go straight until the next obstacle appears. This method not only has the problem of unnecessary power consumption but also intercepts the path of moving people. To solve this problem, a few studies using RL algorithms have been conducted [6]. However, most of the models for RL are trained in a specific environment. In this case, the trained model does not work in a different environment, e.g., a different room. Therefore, learning is difficult even if the structure of the house is already known.",
              "## 5. Machine Learning and AI-Based Approaches### 5.1 Reinforcement Learning (RL)RL techniques allow a robot to learn optimal coverage strategies through trial and error, using rewards and penalties to reinforce effective behaviors.#### Pros:- Highly adaptive to varying environments- Can improve over time with more training#### Cons:- Requires significant computational resources and training data- Performance hinges on the quality of the reward function### 5.2 Deep LearningDeep learning-based approaches can use convolutional neural networks (CNNs) or recurrent neural networks (RNNs) to analyze and predict the best coverage paths based on sensor data and environment mappings.#### Pros:- Can handle complex sensor data and environments- High potential for optimizing coverage#### Cons:- Requires extensive training and data- Intensive computational resources needed"
            ]
          }
        ]
      }
    },
    "case_id": "e7bef53d393712d4b1010a62ee647fb2",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What data structures are commonly used to solve the Range Minimum Query (RMQ) problem?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What data structures are commonly used to solve the Range Minimum Query (RMQ) problem?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define the Range Minimum Query (RMQ) problem and highlight its importance.",
            "weight": 0.15,
            "evidence": [
              "Given a d-dimensional array A with N entries, the Range Minimum Query (RMQ) asks for the minimum element within a contiguous subarray of A. The 1D RMQ problem has been studied intensively because of its relevance to the Nearest Common Ancestor problem and its important use in stringology.",
              "The two dimensional range minimum query problem is to preprocess a static m by n matrix (two dimensional array) A of size N = m * n, such that subsequent queries, asking for the position of the minimum element in a rectangular range within A, can be answered efficiently.",
              "The Range Minimum Query (RMQ) problem is a fundamental problem in computer science, particularly in the field of data structures and algorithms. It involves efficiently finding the minimum element in a given range of an array."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention naive approaches for solving the RMQ problem like linear search and highlight its efficiency for handling higher dimensional cases.",
            "weight": 0.15,
            "evidence": [
              "If constant-time query answering is required, linear time and space preprocessing algorithms were known for the 1D case, but not for the higher dimensional cases.",
              "We focus on the setting where the array A is static and known in advance, and can hence be preprocessed into a scheme in order to answer future queries faster. It has been known for a long time (Gabow et al., STOC 1984) that there is a linear-time preprocessing scheme for O(1)-RMQs by reducing this problem to the computation of lowest common ancestors in trees (Harel and Tarjan, SICOMP 13(2): 1984). The drawback of this and most later schemes is their huge space consumption of O(n log n) bits, which is not optimal."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should mention the segment tree and sparse tree as common data structures used for solving the RMQ problem.",
            "weight": 0.15,
            "evidence": [
              "We propose a new implementation that incorporates novel optimizations that improve the practical performance even further. Compared to existing solutions we replace the range min-max tree with a simpler recursive approach terminated by a sparse table.",
              "In this article, a new autocomplete task decomposition is formulated using an existing method based on range minimum queries (RMQ). The Top-k RMQ problem is formulated and used in the autocomplete problem decomposition. A segment tree based algorithm is proposed for the Top-k RMQ problem.",
              "1. Segment TreeA segment tree is a versatile data structure that can be used to solve the RMQ problem efficiently.Characteristics:- Time Complexity: O(log n) for query, O(n log n) for construction- Space Complexity: O(n)Description:A segment tree is a binary tree where each node represents a range of the original array. The leaf nodes represent individual elements, while internal nodes represent the minimum of their children's ranges. This structure allows for efficient range queries and updates.",
              "2. Sparse TableThe sparse table is another popular data structure for solving RMQ, especially when the array is static (i.e., no updates are needed).Characteristics:- Time Complexity: O(1) for query, O(n log n) for construction- Space Complexity: O(n log n)Description:A sparse table precomputes and stores the minimum values for all possible ranges with lengths that are powers of 2. Queries can then be answered by combining two overlapping precomputed ranges.",
              "### 1. Sparse Table**Description:**The Sparse Table (ST) data structure is particularly effective for static arrays where the data does not change after preprocessing.**Pros:**- **Query time:** \\(O(1)\\), making it extremely fast for lookup.- **Preprocessing time:** \\(O(n \\log n)\\), which is efficient.**Cons:**- **Space complexity:** \\(O(n \\log n)\\), which can be memory-intensive.**Implementation Overview:**- The array is preprocessed into a table where each entry \\(ST[i][j]\\) stores the minimum value in the subarray starting at index \\(i\\) of length \\(2^j\\).- RMQ can be served using two overlapping intervals covered by \\(2^j\\) length, minimizing through constant-time comparisons.",
              "### 2. Segment Tree**Description:**A Segment Tree is a binary tree used for storing intervals or segments, allowing querying and updating in logarithmic time.**Pros:**- **Dynamic updates:** Efficient \\(O(\\log n)\\) time for both updates and queries.- **Query time:** \\(O(\\log n)\\), which is quite efficient.**Cons:**- **Space complexity:** \\(O(n)\\) since it mostly uses only 2n-1 nodes.**Implementation Overview:**- The array is divided into segments, each node of the Segment Tree represents an interval, and stores the minimum value in that interval.- A query can be decomposed into a logarithmic number of intervals, leveraging precomputed minimum values across these intervals."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should discuss other data structures like cartesian tree and binary indexed trees (Fenwick trees) for solving the RMQ problem.",
            "weight": 0.15,
            "evidence": [
              "All arithmetic computations are on integers in U, and integer division is assumed to return the floor of the quotient. Finally, our data structure only requires finding the binary logarithm of integers in the range , i.e., the index of the most significant non-zero bit.",
              "A standard approach to solve this problem is to make the Cartesian tree of A, which is a binary tree with n nodes defined as follows: the root of the Cartesian tree is labeled by i, where A[i] is the minimum element in A; the left and right subtrees of the root are recursively the Cartesian trees of A[1..i [?] 1] and A[i + 1..n] respectively. The property of the Cartesian tree is that the answer to a query with range [i..j] is the label of the lowest common ancestor (LCA) of the nodes with labels i and j. Indeed, the Cartesian tree stores a partial ordering between the elements that is appropriate to answer 1D-RMQs.",
              "Other approaches such as binary indexed tree [4, 5] have good time complexity [6] of O(nlog(n)), and space complexity of O(cn), c=2, but is fairly limited to the number of operations that can be performed on it.",
              "The Binary Indexed Tree, introduced by Peter M. Fenwick in (Fenwick, 1994), is a data structure that maintains a sequence of elements (e.g. numbers) and is capable of com- puting the cumulative sum of consecutive elements, between any two given indexes, in time complexity O (log N ) and also update the value at a given index. We show how to use the structure of the Binary Indexed Tree so that it will support other types of operations besides summation, e.g. range minimum query, maintaining the same time complexity of O (log N ).",
              "3. Cartesian TreeA Cartesian tree is a binary tree structure that can be used in conjunction with other techniques to solve RMQ efficiently.Characteristics:- Time Complexity: O(1) for query (with additional preprocessing), O(n) for construction- Space Complexity: O(n)Description:A Cartesian tree is constructed based on the elements of the array, where the root is the minimum element, and the left and right subtrees are recursively constructed from the subarrays to the left and right of the minimum. This structure can be combined with techniques like LCA (Lowest Common Ancestor) to achieve constant-time RMQ.",
              "### 3. Binary Indexed Tree (Fenwick Tree)**Description:**The Binary Indexed Tree (BIT), or Fenwick Tree, is a structure that provides an efficient way for dynamic cumulative frequency tables or prefix sums.**Pros:**- **Dynamic updates:** Efficient \\(O(\\log n)\\) time for updates.- **Space complexity:** \\(O(n)\\), with a reasonably compact representation.**Cons:**- **Query time:** Generally \\(O(\\log n)\\) when computing a range minimum query requires additional steps compared to sum or frequency modifications.**Implementation Overview:**- Values and frequencies are stored and updated in a tree-like structure where each element can be traversed by summing functions or queries over the ranges.",
              "### 4. Cartesian Tree**Description:**A Cartesian Tree is a binary tree derived from a sequence of numbers, and it plays a pivotal role in solving RMQ problems.**Pros:**- **Query time:** \\(O(1)\\) when using the Cartesian Tree with additional LCA (Lowest Common Ancestor) preprocessing.- **Space Complexity:** \\(O(n)\\) for the tree itself.**Cons:**- **Preprocessing time:** Can be \\(O(n)\\) time for tree construction and \\(O(n \\log n)\\) for additional ancillary data structures.**Implementation Overview:**- The array is first converted into a Cartesian Tree where each node corresponds to a unique array element, and for any node, the value is less than its children.- Binary lifting or other LCA algorithms are employed to quickly resolve RMQ via tree traversal."
            ]
          }
        ]
      }
    },
    "case_id": "ac5f9fc3ee1bd86b2ec1dd35436ca7db",
    "annotator": "Annotator 1 Assignments",
    "agreement": true
  },
  {
    "initial_prompt": "What datasets and methods are used to pre-train models on table specific tasks?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What datasets and methods are used to pre-train models on table specific tasks?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define table-specific tasks and their significance in pre-training models.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Table is one of such structured data types with many applications such as Table-based Question Answering (TQA) (Chen et al., 2020b; Iyyer et al., 2017), Table-based Fact Verification (TFV) (Chen et al., 2020a; Xie et al., 2022), Table-to-Text (Wang et al., 2021) and Column Type & Relation Classification (Iida et al., 2021; Deng et al., 2020). The adoption of structured data has significantly contributed to the advancement of information retrieval and knowledge extraction in web mining and content analysis (Engelmann et al., 2023; Trabelsi et al., 2022).",
              "Pre-training models on table-specific tasks involves leveraging large datasets and advanced methods to train models that can perform well on various natural language processing (NLP) tasks. This section will discuss the datasets and methods used for pre-training models on table-specific tasks.",
              "Introduction to Table-Specific Pre-trainingTable-specific pre-training involves developing models that can understand and process tabular data effectively. This is crucial for various applications, including data analysis, information retrieval, and question-answering systems based on tabular information."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention key datasets used for pre-training models on table-specific tasks, such as WikiTables, TabFact, and PubTabNet",
            "weight": 0.17142857142857143,
            "evidence": [
              "WIKITABLETEXT [3] is an open domain dataset containing 13,318 interpreted statements for 4962 tables. Therefore, a table representation method is needed to better express the contents of the table and reflect its characteristics.",
              "WikiSQL [22] is a collection of 80,654 hand-crafted question-SQL pairs along with 24,241 HTML tables collected from Wikipedia. The tables are collected from [78], and the small tables that have less than five columns or five rows are filtered. WDC WebTables [79] is a large-scale table collection, which contains over 233 million tables and has been extracted from the July 2015 version of the CommonCrawl. Those tables are classified as either relational (90 million), entity (139 million), or matrix (3 million). WikiTables [80]",
              "In the WikiTableQuestions dataset, each question comes with a table from Wikipedia. Given the question and the table, the task is to answer the question based on the table. The dataset contains 2108 tables from a large variety of topics (more breadth) and 22033 questions with different complexity (more depth). Tables in the test set do not appear in the training set, so a system must be able to generalize to unseen tables.",
              "The PubTabNet dataset contains 509k tables delivered as annotated PNG images. The annotations consist of the table structure represented in HTML format, the tokenized text and its bounding boxes per table",
              "The TABFACT dataset (Chen et al., 2020) for example, uses tables as the premise, or source of information to resolve whether a statement is entailed or refuted.",
              "However, existing studies are mainly restricted to dealing with unstructured evidence (e.g., natural language sentences and documents, news, etc), while verification under structured evidence, such as tables, graphs, and databases, remains under-explored. This paper specifically aims to study the fact verification given semi-structured data as evidence. To this end, we construct a large-scale dataset called TabFact with 16k Wikipedia tables as the evidence for 118k human-annotated natural language statements, which are labeled as either ENTAILED or REFUTED.",
              "Several large-scale datasets are used for pre-training models on table-specific tasks. These datasets include:1. **BERT (Bidirectional Encoder Representations from Transformers)**: Trained on a large dataset of unannotated text, BERT is a state-of-the-art language representation model developed by Google.2. **GPT-2 (Generative Pretrained Transformer 2)**: Trained on a massive English corpus in a self-supervised manner, GPT-2 is a transformer-based model developed by OpenAI.3. **ELMo (Embeddings from Language Models)**: Trained on a large dataset of text, ELMo is a deep contextualized word representation model that can be fine-tuned for specific tasks.4. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: Trained on a large dataset of text, RoBERTa is a variant of BERT that uses a different approach to pre-training.",
              "2.1 WikiTablesWikiTables is a large-scale dataset extracted from Wikipedia tables. It contains millions of tables with diverse content and structure.Citation: Bhagavatula, C., Noraset, T., & Downey, D. (2015). TabEL: Entity Linking in Web Tables. In Proceedings of the 14th International Semantic Web Conference.2.2 TableBankTableBank is a large-scale image-based table detection and recognition dataset with fine-grained annotations.Citation: Li, M., Cui, L., Huang, S., Wei, F., Zhou, M., & Li, Z. (2020). TableBank: Table Benchmark for Image-based Table Detection and Recognition. In Proceedings of the 12th Language Resources and Evaluation Conference.2.3 WebTablesWebTables is a large corpus of relational HTML tables extracted from the Web.Citation: Cafarella, M. J., Halevy, A., Wang, D. Z., Wu, E., & Zhang, Y. (2008). WebTables: Exploring the Power of Tables on the Web. Proceedings of the VLDB Endowment, 1(1), 538-549.2.4 TabFactTabFact is a large-scale dataset with 118K manually annotated statements with corresponding tables for table-based fact verification.Citation: Chen, W., Wang, H., Chen, J., Zhang, Y., Wang, H., Li, S., ... & Wang, W. Y. (2020). TabFact: A Large-scale Dataset for Table-based Fact Verification. In International Conference on Learning Representations."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should describe popular methods used for pre-training models on table-specific tasks, such as transfer learning from language models, table-specific transformers, and hybrid models.",
            "weight": 0.17142857142857143,
            "evidence": [
              "the initial phase, illustrated in the left section of the figure, involves pretraining the LLM through a Mask-Then-Predict task, aiming to assimilate unstructured knowledge from tables. The subsequent phase, depicted in the right section, engages in tailored multi-task training for downstream applications, encompassing both classification and regression tasks.",
              "To conduct pretraining, we employ the Mask-Then-Predict objective, which mirrors the established Masked Language Model (MLM) approach in NLP, aiming to improve the model's contextual understanding and its grasp of relationships within table-related data.",
              "Adaptation of TransformersTo account for structured tables in the input, several pre-trained transformer-based LM and systems have been developed. Vanilla LMs are customized to make the model more \"data structure-aware\", thus rendering a modified transformer-based encoder to be utilized on other tasks.",
              "We introduce two intermediate pre-training tasks, which are learned from a trained MASK- LM model, one based on synthetic and the other from counterfactual statements. The first one generates a sentence by sampling from a set of logical expressions that filter, combine and compare the information on the table, which is required in table entailment (e.g., knowing that Gerald Ford is taller than the average president requires summing all presidents and dividing by the number of presidents). The second one corrupts sentences about tables appearing on Wikipedia by swapping entities for plausible alternatives.",
              "Impact of Pretraining Objectives TABERT uses two objectives (SS 3.2), a masked column prediction (MCP) and a cell value recovery (CVR) objective, to learn column representations that could capture both the general information of the column (via MCP) and its representative cell values related to the utterance (via CVR).",
              "The methods used for pre-training models on table-specific tasks include:1. **Fine-tuning**: Fine-tuning involves adjusting the pre-trained model's parameters to fit a specific task. This is done by adding task-specific layers on top of the pre-trained model and training the entire model on the task-specific dataset.2. **Embeddings**: Embeddings involve using the pre-trained model to generate vector representations of text data. These embeddings can then be used for various NLP tasks such as semantic search.3. **Self-Supervised Learning**: Self-supervised learning involves training the model on a large dataset without explicit labels. This is done by using techniques such as masked language modeling, where some words in the input text are randomly replaced with a special token, and the model is trained to predict the original word.",
              "3.1 TAPAS (Table Parser)TAPAS is a method that extends BERT to encode tables as input. It uses relative position embeddings and numerical value embeddings to represent tabular structures.Citation: Herzig, J., Nowak, P. K., Muller, T., Piccinno, F., & Eisenschlos, J. M. (2020). TaPas: Weakly Supervised Table Parsing via Pre-training. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.3.2 TaBERTTaBERT is a pre-trained model for joint understanding of textual and tabular data. It learns representations for tables and associated natural language utterances.Citation: Yin, P., Neubig, G., Yih, W. T., & Riedel, S. (2020). TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.3.3 TUTA (Table Understanding with Tree-based Attention)TUTA is a tree-based model that captures the hierarchical structure of tables using a tree-based encoder with table-aware attention mechanisms.Citation: Wang, Z., Wo, T., Huang, H., & Xu, B. (2020). TUTA: Tree-based Transformers for Generally Structured Table Pre-training. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.3.4 TableGPTTableGPT is a generative pre-training approach for table understanding and generation tasks. It uses a transformer-based architecture to model tables as sequences.Citation: Gong, H., Bhat, S., Wu, L., Xiong, J., & Hwu, W. M. (2020). TableGPT: Few-shot Table-to-Text Generation with Table Structure Reconstruction and Content Matching. In Proceedings of the 28th International Conference on Computational Linguistics."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should mention some applications of pre-trained models on table-specific tasks.",
            "weight": 0.08571428571428572,
            "evidence": [
              "Pre-trained models on table-specific tasks have various applications, including:1. **Language Translation**: Pre-trained models can be fine-tuned for language translation tasks, enabling automatic translation of text or speech.2. **Sentiment Analysis**: Pre-trained models can be fine-tuned for sentiment analysis tasks, enabling automated analysis of text or speech to discern sentiment.3. **Text Summarization**: Pre-trained models can be fine-tuned for text summarization tasks, enabling automatic summarization of long documents or articles.4. **Chatbot Development**: Pre-trained models can be fine-tuned for chatbot development, enabling chatbots to understand and respond to human language."
            ]
          }
        ]
      }
    },
    "case_id": "4534bd4b99ea2bfd1efd8c656e9264c7",
    "annotator": "Annotator 1 Assignments",
    "agreement": true
  },
  {
    "initial_prompt": "What are the main challenges in adapting transformer-based models for tabular data representation?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are the main challenges in adapting transformer-based models for tabular data representation?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define tabular data and highlight its importance in various applications.",
            "weight": 0.12,
            "evidence": [
              "Tabular data stands as one of the pervasive and essential data formats in machine learning (ML), with widespread applications across diverse domains such as finance, medicine, business, agriculture, education, and other sectors that heavily rely on relational databases (Sahakyan et al., 2021; Rundo et al., 2019; Hernandez et al., 2022; Umer et al., 2019; Luan & Tsai, 2021).",
              "Characteristics of tabular dataTabular data, commonly known as structured data, refers to data organized into rows and columns, where each column represents a specific feature. This subsection discusses the common characteristics and inherited challenges with tabular data:1. Heterogeneity: Tabular data can contain different feature types: categorical, numerical, binary, and textual. Therefore, features can range from being dense numerical features to sparse or high-cardinality categorical features (Borisov et al., 2022).2. Sparsity: Real-world applications, such as clinical trials, epidemiological research, fraud detection, etc., often deal with imbalanced class labels and missing values, which results in long-tailed distribution in the training samples (Sauber-Cole & Khoshgoftaar, 2022)."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention the challenge of handling high-dimensional data in tables by transformer-based models.",
            "weight": 0.12,
            "evidence": [
              "#### Handling High-Dimensional DataTabular data often involves high-dimensional data, which can be difficult for transformer-based models to handle. These models are typically designed for sequential data, such as text or images, and may not be optimized for handling large numbers of features in tabular data.",
              "Tabular datasets can be extremely large, with millions of rows and hundreds or thousands of columns. The quadratic complexity of the self-attention mechanism in standard transformers can lead to significant computational and memory challenges when dealing with such large datasets.Researchers are exploring various techniques to address this issue:- Sparse attention mechanisms [7]- Efficient transformer variants like Performers or Linformers [8]- Hierarchical approaches that process subsets of the data separately [9]",
              "Furthermore, the high-dimensional nature of many tabular datasets presents additional computational challenges. Efficient pre-processing of categorical and numerical features becomes crucial to manage the computational load effectively  (44, Arora et al., 2023). These preprocessing steps, while necessary, add to the overall computational requirements of adapting transformer models for tabular data tasks."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss the challenge of handling heterogeneous data types found in tables by transformer-based models as they are primarily designed for homogenous data like text.",
            "weight": 0.12,
            "evidence": [
              "Tables encompass various data types, and to process this tabular data with neural models, conventional methods typically convert each data format into a continuous space using individual strategies. For instance, texts are processed with tokenization and word embedding, while images are handled using image patch embedding",
              "Our model employs the TabUnit module serving as the foundational feature processor for varying data types before leveraging the Transformer's encoder for further encoding. Through adopting the setting of prompts and integrating a decoder, our model becomes adaptable to a wide range of tasks.",
              "Missing or Complex Irregular Spatial Dependencies: There is often no spatial correlation between the variables in tabular datasets [72] or the dependencies between features are rather complex and irregular. When working with tabular data, the structure and relationships between its features have to be learned from scratch. Thus, the inductive biases used in popular models for homogeneous data, such as convolutional neural networks, are unsuitable for modeling this data type [50], [73], [74].",
              "#### Handling Heterogeneous FeaturesTabular data often includes heterogeneous features, such as categorical, numerical, and text features. Handling these heterogeneous features effectively is a challenge for transformer-based models, which are typically designed for homogeneous data.",
              "### 1. Data Heterogeneity#### Diverse Data TypesTabular datasets often contain a mix of numerical, categorical, and sometimes textual data, each requiring different handling and processing strategies. Transformers are primarily designed for homogeneous input (e.g., tokens in a sentence), posing a challenge in managing such diverse data types.#### Imputation and NormalizationHandling missing values and applying normalization techniques are more complex for transformers dealing with heterogeneous tabular data. Unlike text, where missing inputs might be rare or syntactically evident, tabular data can have missing values in any combination, complicating preprocessing."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should discuss the challenge of capturing the relationship between features in tabular data.",
            "weight": 0.12,
            "evidence": [
              "At first, rows (or columns) are encoded separately by computing attention between their fields. Subsequently, encoded rows (or columns) are attended to one another to model the entire tabular time-series. While efficient, this approach constrains the attention granularity and limits its ability to learn patterns at the field-level across separate rows, or columns.",
              "Capturing Feature InteractionsTitle: Modeling Complex Relationships in Tabular DataOne of the strengths of tabular data is the potential for complex interactions between features. Traditional methods like decision trees are particularly good at capturing these interactions. Transformer models need to be adapted to effectively learn and represent these feature interactions.Researchers have explored various approaches, including:- Modifying the attention mechanism to better capture feature interactions [5]- Incorporating additional layers or components specifically designed for modeling feature interactions [6]"
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should discuss the computational complexity of transformers and the need for efficient adaptation techniques.",
            "weight": 0.12,
            "evidence": [
              "Transformer-based approaches are computationally expensive and some approaches try to approximate the costly attention mechanism by using locality-sensitive hashing to replace it (Kitaev et al., 2020), approximating it by a low-rank matrix (Wang et al., 2020), or applying kernels to avoid its computational complexity (Katharopoulos et al., 2020; Choromanski et al., 2020). While there exist methods to make transformers more efficient for long context (Tay et al., 2020), all optimizations consider an unstructured textual input. We believe more traction is needed for efficient transformers on structured data, with ideas from the textual counterpart",
              "#### Scalability and Computational EfficiencyTransformer-based models can be computationally expensive and may not be scalable for large tabular datasets. Adapting these models to handle large datasets while maintaining computational efficiency is a significant challenge.",
              "Performance limitationsTLDR: Transformer models face significant performance challenges when applied to tabular data, including difficulties in handling table structures, limitations in task generalization, and issues with data representation. These challenges highlight the need for specialized adaptations and improvements to enhance transformer models' effectiveness for tabular tasks.",
              "Transformer-based models, while powerful for many natural language processing tasks, encounter substantial computational constraints when applied to tabular data representation. One of the primary challenges is the limitation on the maximum size of input text that these models can process  (20, Oliveira et al., 2022). This constraint becomes particularly problematic when dealing with large tables, as serializing such data often results in sequences that exceed the processing capacity of classical transformer-based models  (45, Chen et al., 2023).The computational complexity of transformer models poses another significant hurdle. The quadratic relationship between input sequence length and computational requirements means that processing large tables can lead to prohibitively long run-times, especially when dealing with substantial volumes of data  (20, Oliveira et al., 2022). This issue is exacerbated in scenarios where table data is exceptionally large, potentially rendering the model incapable of encoding the complete dataset  (9, Bagwe et al., 2020)."
            ]
          }
        ]
      }
    },
    "case_id": "e894bc20daf0522da9c576ae27b257de",
    "annotator": "Annotator 1 Assignments",
    "agreement": true
  },
  {
    "initial_prompt": "what are leading techniques for safe navigation in partially observable environments with moving obstacles?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "what are leading techniques for safe navigation in partially observable environments with moving obstacles?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define partially observable environments and explain how moving obstacles increase the complexity of the navigation problem.",
            "weight": 0.09999999999999999,
            "evidence": [
              "Robots are increasingly being deployed in real-world environments, with legged robots presenting superior versatility in complex workspaces. However, safe legged navigation in real-life workspaces still poses a challenge, particularly in a partially observable environment comprised of dynamic and possibly adversarial obstacles, as shown in Fig. 1. While motion planning for bipedal systems in dynamic environments has been widely studied [1], [2], [3], the proposed solutions often lack formal guarantees on simultaneous locomotion and navigation safety, with the exception of a recent work in [3].",
              "Collision-free navigation while moving amongst static and dynamic obstacles with a limited sensor range is still a great challenge for modern mobile robots. Therefore, the ability to avoid collisions with obstacles in crowded, partially observable environments is one of the most important indicators to measure the navigation performance of a mobile robot"
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention probabilistic methods such as Partially Observable Markov Decision Processes (POMDPs) and Monte Carlo Localization (MCL) for decision-making under uncertainty.",
            "weight": 0.09999999999999999,
            "evidence": [
              "The partially observable Markov decision process (POMDP) provides a principled mathematical framework for modeling and solving robot decision and control tasks under uncertainty. Over the last decade, it has seen many successful applications, spanning localization and navigation, search and tracking, autonomous driving, multirobot systems, manipulation, and human-robot interaction. This survey aims to bridge the gap between the development of POMDP models and algorithms at one end and application to diverse robot decision tasks at the other. It analyzes the characteristics of these tasks and connects them with the mathematical and algorithmic properties of the POMDP framework for effective modeling and solution.",
              "Planning under uncertainty is critical to robotics. The partially observable Markov decision process (POMDP) is a mathematical framework for such planning problems. POMDPs are powerful because of their careful quantification of the nondeterministic effects of actions and the partial observability of the states. But for the same reason, they are notorious for their high computational complexity and have been deemed impractical for robotics.",
              "2.2 Monte Carlo Localization (MCL)MCL uses particle filters to estimate the robot's position in a partially observable environment. It continuously updates the belief state based on sensor readings and motion models [2]."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should mention sensor fusion techniques for decision-making under uncertainty.",
            "weight": 0.09999999999999999,
            "evidence": [
              "Sensor Fusion Techniques3.1 Extended Kalman Filter (EKF)EKF combines data from multiple sensors to estimate the state of the environment and the robot. It's particularly useful for tracking moving obstacles and updating the robot's position [3].3.2 Graph SLAM (Simultaneous Localization and Mapping)Graph SLAM builds a map of the environment while simultaneously localizing the robot within it. This technique is effective in partially observable environments as it continuously updates the map based on new observations [4].",
              "## 5. Sensor Fusion### 5.1. Multi-Sensor FusionCombining data from multiple sensors (e.g., LiDAR, cameras, IMUs) helps to mitigate the limitations of any single sensor in partially observable environments. Advanced algorithms such as Bayesian fusion techniques are used to integrate information from various sensors for a more accurate situational awareness.- **Reference**: Khaleghi, B., Khamis, A., Karray, F. O., & Razavi, S. N. (2013). Multisensor data fusion: A review of the state-of-the-art. Information Fusion, 14(1), 28-44.### 5.2. Simultaneous Localization and Mapping (SLAM)SLAM algorithms aim to build a map of an unknown environment while simultaneously keeping track of the agent's location within it. SLAM techniques are fundamental for navigation in partially observable settings, especially when combined with sensor fusion techniques.- **Reference**: Durrant-Whyte, H., & Bailey, T. (2006). Simultaneous localization and mapping: part I. IEEE Robotics & Automation Magazine, 13(2), 99-110."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should mention reactive techniques, such as Dynamic Window Approach (DWA) for real-time collision avoidance.",
            "weight": 0.09999999999999999,
            "evidence": [
              "The dynamic window approach (DWA) serves as a pivotal collision avoidance strategy for mobile robots, meticulously guiding a robot to its target while ensuring a safe distance from any perceivable obstacles in the vicinity.",
              "Robust local navigation is a critical capability for any mobile robot operating in a real-world unstructured environment, especially when there are humans or other moving obstacles in the workspace. One of the most commonly used methods for local navigation is the dynamic window approach (DWA), which does not address the problem of dynamic obstacles and depends heavily on the settings of the parameters in its cost function."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should mention the use of machine learning approaches for obstacle motion prediction.",
            "weight": 0.09999999999999999,
            "evidence": [
              "The primary contribution of our paper is to demonstrate that deep predictive models of video can be used by real physical robotic systems to manipulate previously unseen objects. To that end, we present an MPC algorithm based on probabilistic inference through a learned predictive image model that allows a robot to plan for actions that move user-specified objects in the environment to user-defined locations",
              "## 2. Machine Learning Approaches### 2.1. Reinforcement Learning (RL)Reinforcement learning, particularly Deep Reinforcement Learning (DRL), has shown promise for navigation in complex and partially observable environments. Techniques such as Q-learning, Deep Q-Networks (DQN), and Proximal Policy Optimization (PPO) can be used to train agents to make decisions based on partial observations.- **Reference**: Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., et al. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533.- **Reference**: Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal Policy Optimization Algorithms. arXiv preprint arXiv:1707.06347.",
              "4. Machine Learning Approaches4.1 Deep Reinforcement LearningDeep RL algorithms, such as Deep Q-Networks (DQN) and Proximal Policy Optimization (PPO), can learn navigation policies in complex, partially observable environments with moving obstacles [5].4.2 Generative Adversarial Imitation Learning (GAIL)GAIL learns navigation strategies from expert demonstrations, which can be particularly useful in environments with moving obstacles [6]."
            ]
          },
          {
            "name": "most_important_item_5",
            "criterion": "The answer should mention planning algorithms that consider the predicated mention of obstacles, like Timed Elastic Band (TEB) or rapidly-exploring random trees (RRT) with kinodynamic constraints, to generate safe trajectories in dynamic environments.",
            "weight": 0.09999999999999999,
            "evidence": [
              "The TEB algorithm, pioneered by Rossmann et al. [17], represents a significant advancement in the field of trajectory optimization. Building upon the foundation laid by the EB algorithm [18], TEB introduces innovative extensions tailored for real-time collision avoidance in multi-objective trajectory planning scenarios. The EB algorithm, which forms the basis of TEB, plays a vital role in ensuring the robotic system's adherence to kinematic constraints along its designated path. These constraints are meticulously enforced through a combination of sophisticated techniques, including g2o [19] optimization and velocity restrictions. By integrating these constraints into the initial global path, the system achieves a high level of efficiency in collision avoidance strategies during dynamic online trajectory planning operations.",
              "We present a sampling-based motion planning approach for articulated manipulators that generates safe paths. It uses the rapidly-exploring random trees paradigm to establish a collision-free path in configuration space. The expansion of the trees is influenced by a modified version of the kinetostatic danger field - a safety assessment function recently proposed in the literature. The idea is to grow the trees towards safer regions. Thus, the planner provides not only collision-free paths, but strives for safer ones. We propose two versions of the planner. The first is a modification of the Jacobian Transpose-directed RRT (JT-RRT) algorithm that grows a single tree from the start configuration and uses the transpose of the Jacobian to guide the sampling towards the goal defined in the workspace.",
              "Sampling-based motion planning algorithms have made significant progress in efficiently finding trajectories for high-dimensional and nonlinear systems. Examples include Probabilistic Roadmaps (PRM), Rapidly-exploring Random Trees (RRT), and their variants  (26, Kavraki et al., 2007). These methods can be adapted for dynamic environments, with algorithms like RRT*-FDWA proposed to address the challenges of path planning and obstacle avoidance in uncertain, dynamic settings  (349, Zhou et al., 2023).",
              "## 3. Planning Techniques### 3.1. Model Predictive Control (MPC)Model Predictive Control is an advanced control strategy that uses a model of the system to predict future states and optimize the control inputs. In partially observable environments, MPC can be combined with estimators like particle filters to account for the uncertainty in the prediction.- **Reference**: Mayne, D. Q., Rawlings, J. B., Rao, C. V., & Scokaert, P. O. M. (2000). Constrained model predictive control: Stability and optimality. Automatica, 36(6), 789-814.### 3.2. Rapidly-exploring Random Trees (RRT)RRT and its variants (e.g., RRT*, Informed RRT*) are widely used for path planning in complex environments. They can be adapted to partially observable environments by incorporating strategies to periodically replan as new information becomes available.- **Reference**: LaValle, S. M. (1998). Rapidly-exploring Random Trees: A new tool for path planning.",
              "5. Predictive Planning5.1 Model Predictive Control (MPC)MPC uses a model of the environment to predict future states and optimize the robot's trajectory. It's particularly effective for avoiding moving obstacles by anticipating their future positions [7].5.2 Rapidly-exploring Random Trees (RRT)RRT and its variants (e.g., RRT*) efficiently explore the state space to find collision-free paths. These algorithms can be adapted to handle moving obstacles by repeatedly replanning [8]."
            ]
          }
        ]
      }
    },
    "case_id": "e551f57061ffdacce638393c2984010b",
    "annotator": "Annotator 1 Assignments",
    "agreement": true
  },
  {
    "initial_prompt": "how can RRT-based path planning be adapted to efficiently handle narrow pathways",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "how can RRT-based path planning be adapted to efficiently handle narrow pathways",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should breifly define RRT-based path planning and discuss the challenges of narrow pathways.",
            "weight": 0.10909090909090909,
            "evidence": [
              "Sampling-based methods such as Rapidly Exploring Random Trees (RRT) and Probabilistic Road Maps (PRM) have been recognized as effective tools to solve the path planning problem for both ground mobile robots and flying robots in high-dimensional configuration space. However, the efficiency of the RRT planner will be decreased in complex environments with narrow passages.",
              "In many task scenarios, the robot is required to pass certain crucial regions with small volumes, namely narrow passages, in a complex environment (e.g. Flying robots are often required to go through cluttered spaces with non-convex obstacles). Such regions can cause significant efficiency decrease of RRT and PRM algorithms, due to the random nature of the sampling-based methods. As the volume of narrow passages is comparatively small and the probability of capturing random samples from them is low.",
              "RRT algorithms excel in high-dimensional and constrained spaces but struggle with narrow passages. They can get stuck in local solutions, potentially missing optimal paths through tight spaces.",
              "Narrow passages pose significant challenges for RRT-based path planning algorithms. The primary issues include:1. **Inefficient Sampling**: Traditional RRT algorithms rely on random sampling, which can lead to inefficient exploration of the configuration space, especially in narrow passages.2. **Insufficient Tree Growth**: The necessity of sufficient trees has a significant impact on computation time, which is rarely investigated in previous research.3. **Local Minima**: RRT algorithms can get stuck in local minima, making it difficult to find an optimal solution."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention the technique of biased sampling towards narrow pathways.",
            "weight": 0.10909090909090909,
            "evidence": [
              "In the Basic-RRT algorithm, the selection of random sampling points is in the entire configuration space or can be described as blind, such as in a certain sampling, the current sampling point is likely very close to the goal region, however, due to the randomness of sampling, the position of the sampling point in the subsequent sampling process is biased., the random tree will likely expand to a region that does not have a facilitation effect on the final path generation. RRT-Biased, although through a forced modification make the random tree is biased expand towards the goal region, when encountering a trap or a complex environment, it falls into the same predicament as Basic-RRT.",
              "This direct-sampling method allows for the creation of informed-sampling planners. Such a planner, Informed RRT* , is presented to demonstrate the advantages of informed incremental search (Fig. 1). Informed RRT* behaves as RRT* until a first solution is found, after which it only samples from the subset of states defined by an admissible heuristic to possibly improve the solution. This subset implicitly balances exploitation versus exploration and requires no additional tuning (i.e., there are no additional parameters) or assumptions (i.e., all relevant homotopy classes are searched)",
              "To improve RRT performance in narrow passages, researchers have developed various guiding and biasing techniques. One approach is to use guiding paths to focus sampling. Von'asek et al. proposed an iterative scaling approach that preferentially samples along a given guiding path, significantly increasing the success rate of finding feasible paths in high-dimensional configuration spaces compared to standard RRT algorithms (3, Von'asek et al., 2011).",
              "Adaptive sampling techniques can dynamically adjust the sampling distribution based on the environment's structure. For example, regions identified as narrow passages could receive a higher sampling density to ensure that the RRT explores these areas more thoroughly. Techniques like obstacle-based sampling focus on generating samples near obstacles, which helps in identifying narrow passages effectively [5]."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should mention bridge test sampling technique towards identifying and handling narrow pathways.",
            "weight": 0.10909090909090909,
            "evidence": [
              "Another idea for narrow passage planning is to employ non-uniform sampling strategies. Boor et al. [14] utilize the Gaussian sampler to improve the efficiency of PRM in narrow passages by increasing sample density near obstacle boundaries. But it may result in redundant samples in uninteresting regions as not all obstacle boundaries are adjacent to narrow passages, and it may also lead the planner into blind corners of the obstacle. In view of this, in [16], the bridge test (RBB) sampling strategy was employed to identify narrow passages by checking for collision at three samples (\"bridges\") along a line segment.",
              "This method first uses the simplified Bridge Test and point cloud clustering algorithm to locate the narrow passages on the map and place the root nodes in the narrow passage. Then grow multiple random trees from the root nodes, starting and endpoints. As the number of samples increases, the random trees gradually expand and connect to each other. The trees are connected to each other to build a complete path. Finally, the initial path is optimized and the final planning result is obtained",
              "c) Bridge Test Sampling: Use a \"bridge test\" to identify samples that are likely to be in narrow passages and bias sampling towards these areas [3]."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should mention hybrid and multi-stage approaches to improve RRT performance in narrow passages",
            "weight": 0.10909090909090909,
            "evidence": [
              "Hybrid and multi-stage approaches have emerged as effective solutions for adapting RRT-based path planning to efficiently handle narrow pathways. These methods typically combine the strengths of different algorithms or employ multiple stages to address the challenges posed by constrained environments.One notable approach is the two-stage method proposed by Wang et al., which uses a Bridge Test sampling algorithm to identify critical regions in the first stage, followed by Triple-RRTs to search for local connections in the second stage  (2, Wang et al., 2010). This approach effectively addresses the problem of multi-d.o.f. robot path planning in high-dimensional configuration spaces with narrow corridors."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should mention the path smoothing or optimization techniques to refine the initially generated path for smoother traversal through the narrow passages.",
            "weight": 0.10909090909090909,
            "evidence": [
              "Post Processing Requirements: As RRT* based approaches generate sub-optimal path therefore, post processing techniques are adopted to further optimize the path. Two post processing techniques usually adopted for path refinement are pruning and smoothing. Path pruning reduces the path length by removing redundant nodes [56].",
              "POST-PROCESS ALGORITHM It is well known that RRT-based planners often generate jerky and unnatural paths that may contain unnecessary turns, or that the velocities at the points may change arbitrarily. All of these issues are bad for the vehicle to execute. Especially, these issues become more significant when the environment has narrow passages as the narrow region for path planning becomes more constrained [18]. Therefore, a post-process algorithm is proposed for generating smooth paths. And this algorithm consists of two stages: pruning stage and smoothing stage."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should point to the integration of ML techniques to predict and focus on narrow pathways.",
            "weight": 0.05454545454545454,
            "evidence": [
              "Combining RRT with other path search algorithms is an important way to speed up the search process. In [16], the artificial potential field algorithm is incorporated in RRT* to accelerate convergence rate, but the planning time may dramatically increase in complex environments. The A*-RRT* [17] algorithm uses the path generated by the A* algorithm to guide the sampling procedure of the RRT* planner, which significantly accelerates the convergence speed. However, quite a long time is required for A* to find an initial path when it comes to large-scale problems. LM-RRT [18] applies reinforcement learning methods to guide the growth of trees, but the learning-based methods may not perform well in the new environment.",
              "We propose a machine learning (ML)-inspired approach to estimate the relevant region of a motion planning problem during the exploration phase of sampling-based pathplanners. The algorithm guides the exploration so that it draws more samples from the relevant region as the number of iterations increases. The approach works in two steps: first, it predicts if a given sample is collision-free (classification phase) without calling the collision-checker, and it then estimates if it is a promising sample, i.e., if it has the potential to improve the current best solution (regression phase), without solving the local steering problem."
            ]
          }
        ]
      }
    },
    "case_id": "7b81c9d859619c48c5d75906d7e0d93d",
    "annotator": "Annotator 1 Assignments",
    "agreement": true
  },
  {
    "initial_prompt": "How does the tree covering technique differ in its application to ordinal trees versus binary trees?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "How does the tree covering technique differ in its application to ordinal trees versus binary trees?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define what tree covering technique is.",
            "weight": 0.15,
            "evidence": [
              "Tree covering is a technique used in computer science to represent trees by covering them with a set of subtrees that have useful properties for various algorithms, such as efficient query handling in data structures, compression, and indexing.",
              "The tree covering technique involves partitioning a tree into subtrees (called mini-trees) and further partitioning these mini-trees into smaller subtrees (called micro-trees). This hierarchical decomposition allows for efficient representation and manipulation of the tree structure."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should detail the difference between cardinal and ordinal trees, and specify that the binary tree is a cardinal tree of degree 2. So, the order of children doesn\u2019t matter in the binary tree.",
            "weight": 0.15,
            "evidence": [
              "We consider both cardinal trees (or k-ary tries), where each node has k slots, labeled {1, . . . , k}, each of which may have a reference to a child, and ordinal trees, where the children of each node are simply ordered",
              "An ordinal tree is a rooted tree of arbitrary degree in which the children of each node are ordered, hence we speak of the ith child. The one-to-one mapping between these trees and binary trees is a well known undergraduate example [20, p. 333], and so about 2n bits are necessary for representation of such a tree",
              "By a cardinal tree (or trie) of degree k, we mean a rooted tree in which each node has k positions for an edge to a child. Each node has up to k children and each child of a given node is labeled by a unique integer from the set {1, 2, . . . , k}. A binary tree is a cardinal tree of degree 2."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss that the covering technique for ordinal trees should consider the order of nodes when identifying isomorphic subtrees, while the covering technique for binary trees focuses on creating universal trees that can represent all binary trees of a given size.",
            "weight": 0.15,
            "evidence": [
              "TLDR: The tree covering technique for ordinal trees requires considering the order of nodes when identifying isomorphic subtrees. It was introduced for ordinal trees and later simplified, forming the basis for applications to other tree types.",
              "TLDR: The tree covering technique for binary trees focuses on creating universal trees that can represent all binary trees of a given size. It involves partitioning the tree into subtrees and using NCA-universal trees for efficient representation."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should discuss that while the application of covering technique for both types of trees considers efficient representation, ordinal trees require more complex representations to handle their arbitrary branching, whereas binary trees benefit from simpler structures that facilitate efficient parallel algorithms and RMQ implementations.",
            "weight": 0.15,
            "evidence": [
              "It's worth noting that the application of tree covering to binary trees maintains some similarities with its use in ordinal trees, particularly in terms of its goal of efficient representation. However, the specific implementation and focus on universal trees and NCA-universal structures represent key differences in its application to binary trees.",
              "The tree covering technique for ordinal trees focuses on creating a succinct representation that supports various navigational operations efficiently. Key aspects include:- **Succinct Representation**: Ordinal trees are represented using structures like balanced parenthesis sequences or depth-first unary degree sequences. These representations allow for efficient encoding and decoding of tree structures while minimizing space usage to around $2n + o(n)$ bits, where $n$ is the number of nodes[1].- **Multiple Traversal Orders**: The technique supports multiple traversal orders (e.g., level-order traversal) that are useful for different types of queries and operations[1].- **Micro Tree Compression**: The tree is decomposed into smaller micro trees, each of which can be compressed and represented efficiently. This allows for efficient navigational queries and operations within the tree structure[2].",
              "The tree covering technique for binary trees often leverages their simpler structure to optimize certain operations:- **Balanced Parentheses Representation**: Similar to ordinal trees, binary trees can also use balanced parenthesis representations. However, the simpler structure of binary trees allows for more straightforward implementations and optimizations[2].- **Efficient Range Minimum Queries (RMQ)**: The tree covering technique for binary trees can be applied to implement efficient RMQ data structures. These structures are optimized for average-case performance and use less than $2n$ bits of space while processing queries in practical timeframes[2].- **Parallel Algorithms**: The binary tree structure lends itself well to parallel algorithms, which can solve various combinatorial problems efficiently. This includes problems like minimum covering sets, maximum independent sets, and depth-first spanning tree constructions[4]."
            ]
          }
        ]
      }
    },
    "case_id": "3b46ec4e348c2a317d20f9922ee004b1",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What recent results have been obtained on the Sherali-Adams hierarchy ?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What recent results have been obtained on the Sherali-Adams hierarchy ?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define the Sherali-Adams hierarchy.",
            "weight": 0.08571428571428572,
            "evidence": [
              "In integer programming, there are several well-known hierarchies of linear or semi-definite programs that provide successively tighter relaxations of a 0/1-polytope. At one end of the spectrum of these relaxations is the original relaxation P = {x [?] [0, 1]n | Ax = b}; the integer hull PI:= conv(P [?] Zn) is at the other end. The list of such hierarchies includes the Gomory-Chvatal, Sherali-Adams, lift and-project, Lovasz-Schrijver, and Lasserre hierarchies; see [8, 24] for details.",
              "The linear programming relaxation of an integer program is strengthened by lifting the problem into a higher dimensional space, where a more convenient formulation may give a tighter relaxation. One then has a choice between working with this tighter relaxation in the higher dimensional space,or projecting it back onto the original space. In this latter case,the whole procedure can be viewed as a method for generating cutting planes in the original space. The different hierarchy has different multiplier and linearization",
              "In the linear case, the technique first multiplies the constraints in the problem, including the zero-one interval bounds on the variables, by a select set of d-degree polynomial terms or factors formed using the n zero-one variables, where =< d _-< n. The resulting system is then linearized by defining new nonnegative variables for each existing cross-product term. It is shown that for each d l, n, by enforcing the binary restrictions on the original x-variables, we obtain an equivalent reformulation of the problem which has at least as tight a linear programming relaxation as that obtained by using factors of degree d- 1. (Here, the 0-degree factor is taken as unity.) Moreover, when d n, we show that the resulting linear system represents a polytope whose extreme points are precisely the zero-one solutions feasible to the original problem, and hence characterizes the convex hull of feasible solutions.",
              " Introduction to the Sherali-Adams HierarchyThe Sherali-Adams hierarchy is a powerful tool in mathematical optimization, introduced by Hanif D. Sherali and Warren P. Adams in 1990. It provides a systematic way to generate stronger relaxations for mixed-integer programming problems. Recent years have seen significant advancements in understanding and applying this hierarchy."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should briefly discuss how the performance of LP hierarchies are usually evaluated.",
            "weight": 0.08571428571428572,
            "evidence": [
              "Why are we interested in hierarchies? 1. Polyhedral Combinatorics How many rounds are required to reduce the feasible set to PI? Which constraints are satisfied? 2. Proof Systems Complexity and lower bound 3. Approximation Algorithm Integrality gap after t rounds/levels, better approximation, conjecture on the hardness of the question"
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss recent applications of the Sherali-Adams hierarchy in providing approximate solutions to combinatorial optimization problems.",
            "weight": 0.08571428571428572,
            "evidence": [
              "Identifying the limitations of relaxations derived by Lift-and-Project system has attracted much attention and showing integrality gaps for the Sherali-Adams SDP and the Lasserre systems stand as the most attractive subjects in this area of research due to a number of reasons. Firstly, the best algorithms known for many combinatorial optimization problems (and Vertex Cover in particular) are based on relaxations weaker than those derived by a constant (say four) rounds of the Sherali-Adams SDP system which we study here",
              "For the asymmetric TSP tour problem, Charikar, Goemans, and Karloff (FOCS2004) proved that the integrality gap of the standard relaxation is at least 2. We prove that after one round of the Lov'asz-Schrijver or Sherali-Adams procedures, the integrality gap of the asymmetric TSP tour problem is at least 3/2, with a small caveat on which version of the standard relaxation is used. For the symmetric TSP tour problem, the integrality gap of the standard relaxation is known to be at least 4/3, and Cheung (SIOPT 2005) proved that it remains at least 4/3 after o(n) rounds of the Lov'asz-Schrijver procedure, where n is the number of nodes. For the symmetric TSP path problem, the integrality gap of the standard relaxation is known to be at least 3/2, and we prove that it remains at least 3/2 after o(n) rounds of the Lov'asz-Schrijver procedure, by a simple reduction to Cheung's result",
              "We study the Sherali-Adams hierarchy applied to a natural Integer Program formulation that (1 + )-approximates the optimal solution of GVP. Sherali-Adams hierarchy has gained much interest recently as a possible approach to develop new approximation algorithms. We show that, when the input graph has bounded treewidth or bounded genus, applying a constant number of rounds of Sherali-Adams hierarchy makes the integrality gap of this natural LP arbitrarily small, thus giving a (1 + )-approximate solution to the original GVP instance.",
              "In this paper, we investigate the application of the Sherali-Adams technique to sets of points that encode the graph isomorphism and automorphism problems towards the ultimate goal of better understanding the Sherali-Adams technique and the complexity of these problems. Specifically, we show that the Sherali-Adams relaxations of a well-known polytope encoding of graph isomorphism do not converge aftersteps in the worst case.",
              "We have shown that for Knapsack, an integrality gap of 2[?]o persists up to a linear number of rounds in the Sherali-Adams hierarchy. This broadens the class of problems for which Sherali-Adams is not strong enough to capture the instrinsic difficulity of problems.",
              "We show that the exp ( c log n log [?] ) -round Sherali-Adams linear programming hierarchy certifies that the maximum cut in such a G is at most 50.1% (in fact, at most 1 2 + 2[?](c)).",
              "### k-Cut ProblemRecent advancements have utilized the SA hierarchy to devise improved approximation algorithms for the k-Cut problem.- **Algorithmic Performance**: An algorithm introduced by Oveis Gharan and Saberi (2011), which builds upon the SA hierarchy, provided an improved approximation ratio for the k-Cut problem, showing the power of advanced LP relaxations.### Maximum Independent SetThe SA hierarchy has also been applied to develop better approximation algorithms for the Maximum Independent Set problem, particularly in bounded-degree graphs.- **Bounded-degree Graphs**: The work of Davies et al. (2021) showcased that applying higher levels of the SA hierarchy can yield significant improvements in approximation ratios for Maximum Independent Set in graphs with bounded degrees.",
              "One of the most notable recent results involves improved lower bounds for the Sherali-Adams hierarchy:2.1 Maximum Cut ProblemResearchers have established stronger lower bounds for the Maximum Cut problem. In 2019, Kothari et al. proved that O(n) rounds of the Sherali-Adams hierarchy are required to achieve a (1/2 + e)-approximation for Max Cut on random regular graphs [1].2.2 Constraint Satisfaction Problems (CSPs)For various CSPs, new lower bounds have been established. Chan et al. (2016) showed that O(n) rounds of Sherali-Adams are necessary to improve upon the basic LP relaxation for approximating CSPs [2]."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should discuss theoretical results related to the Sherali-Adams hierarchy, such as the integrality gap.",
            "weight": 0.08571428571428572,
            "evidence": [
              "In this paper, we consider the Sherali-Adams hierarchy of linear relaxations and show strong lower bounds for several natural problems. This hierarchy is much less understood than the weaker Lov'asz-Schrijver hierarchy. In fact, we are aware of only two integrality gap results (by de la Vega and Kenyon-Mathieu [6] and Schoenebeck [17] for Lasserre) that hold for more than one round of the Sherali-Adams lift-and-project. Constructing new integrality gap examples for Sherali-Adams is thus an important open question [20, Section 7.3].",
              "In this paper, we prove that the integrality gap for the Sparsest Cut problem is at least  q log n log r+log log n  after r rounds of the Sherali-Adams lift-and-project. The integrality gap for the MAX CUT and Vertex Cover problems remains 2 [?] e even after n g rounds (for every positive e and some g that depends on e). Our result for MAX CUT improves a result by de la Vega and Kenyon-Mathieu [6] who proved that the integrality gap remains 2 [?] e after logc n rounds (where c < 1 is some fixed constant)",
              "We prove strong lower bounds on integrality gaps of Sherali- Adams relaxations for MAX CUT, Vertex Cover, Sparsest Cut and other problems. Our constructions show gaps for Sherali-Adams relaxations that survive nd rounds of lift and project. For MAX CUT and Vertex Cover, these show that even nd rounds of Sherali-Adams do not yield a better than 2 [?] e approximation",
              "We bolster this conjecture by showing that in a random hypergraph with edge probability n[?]a, (log n) rounds of Sherali-Adams with cannot rule out the existence of a k-subhypergraph with edge density k[?]a[?]o(1), for any k and a. This holds even when the bound on the objective function is lifted. This gives strong integrality gaps which exactly match the gap in the above distinguishing problems, as well as the best-known approximations, for Densest k-Subgraph, Smallest p-Edge Subgraph, their hypergraph extensions, and Small Set Bipartite Vertex Expansion (or equivalently, Minimum p-Union). Previously, such integrality gaps were known only for Densest k-Subgraph for one specific parameter setting.",
              "## Theoretical Advances### Integrality GapsOne key field of study regarding the SA hierarchy is understanding integrality gaps, which measure how well the hierarchy approximates the integer solution. Recent works have focused on providing tighter bounds and understanding at which levels of the hierarchy these gaps diminish significantly.- **Maximum Constraint Satisfaction Problems (CSPs)**: Research has shown that for certain CSPs, the SA hierarchy can achieve better integrality gaps compared to earlier known results. For example, Lee, Raghavendra, and Steurer (2015) analyzed the integrality gaps for CSPs and provided bounds that illustrate the power of SA relaxations at certain levels.",
              "6.1 Integrality GapsNew results on integrality gaps for various problems have been obtained. For instance, Kothari et al. (2018) established tight integrality gaps for the Sherali-Adams relaxations of constraint satisfaction problems [8].",
              "Lower Bounds and Integrality GapsTLDR: Recent research has established significant lower bounds on the integrality gaps for various problems using the Sherali-Adams hierarchy. These results demonstrate both the power and limitations of the hierarchy in approximating hard optimization problems.Recent studies have provided important insights into the lower bounds and integrality gaps associated with the Sherali-Adams (SA) hierarchy for various optimization problems. These findings help delineate the limitations of the hierarchy and inform its practical applications."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should mention applications of Sherali\u2013Adams hierarchy in machine learning",
            "weight": 0.08571428571428572,
            "evidence": [
              "## Application in Machine Learning### Training Neural NetworksIntriguingly, there has been a cross-disciplinary application of the SA hierarchy in machine learning, particularly in training neural networks.- **Neural Network Training**: Some recent studies like that of Bengio et al. (2016) have explored using the SA hierarchy to improve the training process for certain classes of neural networks, providing a new avenue for utilizing combinatorial optimization techniques in machine learning.### Generalization BoundsRecent explorations also include deriving generalization bounds in machine learning models using insights from the SA hierarchy.- **Bound Derivations**: Research by Goemans and Williamson (2020) leveraged the properties of the SA hierarchy to derive new generalization bounds that are tighter and more robust for certain classification problems."
            ]
          },
          {
            "name": "most_important_item_5",
            "criterion": "The answer should provide a discussion on the computational complexity of the Sherali-Adams hierarchy.",
            "weight": 0.08571428571428572,
            "evidence": [
              "Prior to our result, it was unclear whether even (n/polylogn)-round Sherali-Adams could certify that the max-cut value was < 0.9 for sparse random regular graphs. Indeed, it was equally if not more conceivable that Charikar et al.'s result was not tight, and could be extended to (n)-rounds. In light of our result, we are left to wonder whether there are instances of max-cut which have truly exponential extension complexity.",
              "Second, in computational complexity there has recently been a substantial sequence of results proving for several classical combinatorial problems that, even for k = (n), the kth relaxation Pk has a large integrality gap.1 The motivation for these results is that the various lift-and-project schemes encompass most known sophisticated approximation algorithms for NP-hard problems such as Sparsest Cut and Maximum Satisfiability; therefore, a large integrality gap after a linear (or even logarithmic) number of rounds rules out (unconditionally) a wide class of efficient approximation algorithms.",
              "Complexity consequences. We now give some computational complexity consequences of Theorem 3.3. First, we obtain a new and simpler proof (in fact two proofs) of the complexity classification of conservative valued constraint languages [45]. Second, we obtain a complexity classification of (generalization of) minimum-solution problems over arbitrary finite domains. Minimum-solution (min-sol) problems [40], studied under the name min-ones on Boolean domains [24, 42], constitute a large and interesting subclass of VCSPs including, for instance, integer linear programming over bounded domains.",
              "### Complexity ClassificationsResearch has also continued to explore the computational complexity of solving higher-level relaxations in the hierarchy.- **Polynomial-time Approximation**: Studies like that of Barak et al. (2016) have investigated the boundaries within which these higher-level relaxations remain solvable in polynomial time, providing insights into the computational limits of applying SA hierarchies for large-scale problems."
            ]
          },
          {
            "name": "most_important_item_6",
            "criterion": "The answer should mention the name of alternative hierarchies and breifly discuss their differences.",
            "weight": 0.08571428571428572,
            "evidence": [
              "Sherali and Adams (1990), Lov'asz and Schrijver (1991) and, recently, Lasserre (2001) have constructed hierarchies of successive linear or semidefinite relaxations of a 0 [?] 1 polytope P [?] Rn converging to P in n steps. Lasserre's approach uses results about representations of positive polynomials as sums of squares and the dual theory of moments. We present the three methods in a common elementary framework and show that the Lasserre construction provides the tightest relaxations of P. As an application this gives a direct simple proof for the convergence of the Lasserre's hierarchy. We describe applications to the stable set polytope and to the cut polytope.\"https://ir.cwi.nl/pub/11670/11670B.pd"
            ]
          }
        ]
      }
    },
    "case_id": "afff22e411e34366146c45ec4c7ff599",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "How can question generation be used to mitigate hallucination in LLMs?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "How can question generation be used to mitigate hallucination in LLMs?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should breifly define what hallucination is in the context of NLP and emphasize its importance in the era of LLMs.",
            "weight": 0.075,
            "evidence": [
              "In the field of NLP, hallucination typically refers that the model output contains undesired content that is nonsensical or deviates from the source material (Ji et al., 2023a), in loose analogy with the phenomenon of hallucination in human psychology (Macpherson and Platchias, 2013). Recently, hallucination has gained prominence alongside the emergence of LLMs such as ChatGPT. One prominent feature of LLMs is that they possess rich world knowledge, and can utilize such knowledge to solve various downstream tasks. However, it has been shown that LLMs tend to generate the hallucinatory content, especially under an open-domain setting (Zhang et al., 2023b; Ye et al., 2023; Huang et al., 2023).",
              "In the field of NLP, hallucination typically refers to a phenomenon where the generated content from LLMs is nonsensical, unfaithful, or incorrect in response to a given query (Filippova, 2020;Maynez et al., 2020). Recently, this issue has triggered a notable discussion among thousands of AI researchers around the world, resulting in over 30,000 signatures on an open letter 2, calling for a six-month pause on \"giant AI experiment\" (Pause Giant, 2023).",
              "The recent roll-out of Google's highly anticipated ChatGPT rival, Bard, led to a fiasco owing to it hallucinating a factually inaccurate answer in the company's advertisement, which cost Google a $140 billion wipeout in terms of market value (Reuters, 2023)."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should explain what question generation technique is in the context of NLP.",
            "weight": 0.075,
            "evidence": [
              "We take a mainstream NLG task, namely question generation, as a case study (Du et al., 2017; Yuan et al., 2017; Du and arXiv:2209.11000v1 [cs.CL] 22 Sep 2022 Cardie, 2018; Pan et al., 2019; Liu et al., 2020; Pyatkin et al., 2021). In this task, a model is trained to generate a natural language question conditioned on a context and an answer, such that the generated question can be answered by the provided answer using the context as supporting evidence. Question generation is the cornerstone for many NLP applications including education (Kurdi et al., 2020; Abdelghani et al., 2022), automatically FAQ generation (Mass et al., 2020), information seeking (Qi et al., 2020), etc.",
              "We perform evaluations on question generation (Du & Cardie, 2018), the task of automatically producing relevant questions that ask for the given answer and context. The input of the sequence-to-sequence problem is defined as the concatenation of a paragraph and an answer",
              "Given the output of the Fact Extraction component T, the task of Question Generation is to generate questions for each t [?] T such that the answer to the question is the Object part of t. In this way, various answers retrieved from different sources can be used to verify each triple t. Depending on the format of triple t (flat or extended), we introduce two different question generation paradigms."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss how question generation can be utilized to mitigate hallucination in LLMs through targeting queries that challenge and verify the content produced by LLMs. The generated questions can be either answered by the model itself in an iterative approach or guide external knowledge extraction.",
            "weight": 0.075,
            "evidence": [
              "Question generation (QG) has emerged as a powerful tool for mitigating hallucinations in large language models (LLMs). This approach leverages the ability to create targeted questions that can challenge and verify the content produced by these models.",
              "2.1.1 Retrieval Augmented GenerationRetrieval-Augmented Generation (RAG) enhances the responses of LLMs by tapping into external, authoritative knowledge bases rather than relying on potentially outdated training data or the model's internal knowledge. This approach addresses the key challenges of accuracy and currency in LLM outputs (Kang et al., 2023). RAG effectively mitigates the issue of hallucination in LLMs by generating responses that are not only pertinent and current but also verifiable, thereby reinforcing user confidence and offering developers an economical way to enhance the fidelity and utility of LLMs across different applications.",
              "Chain-of-Verification (CoVe): (Dhuliawala et al., 2023) develop the CoVe method where the model 1. Drafts an initial response. 2. Plans verification questions to fact-check its draft. 3. Answers those questions independently so the answers are unbiased. 4. Generates a final verified response. Experiments show CoVe decreases hallucinations across tasks like list-based Wikidata questions and long-form text generation. Given a user query, an LLM generates a baseline response that may contain inaccuracies like factual hallucinations. CoVe first generates verification questions to ask, then answers them to check for agreement",
              "FactuaL Error detection and correction with Evidence Retrieved from external Knowledge (FLEEK): (Bayat et al., 2023) introduce FLEEK, an intelligent and model-agnostic tool aimed at aiding end users, such as human graders, in fact verification and correction. FLEEK features a user-friendly interface capable of autonomously identifying potentially verifiable facts within the input text. It formulates questions for each fact and queries both curated knowledge graphs and the open web to gather evidence. The tool subsequently verifies the correctness of the facts using the acquired evidence and proposes revisions to the original text. The verification process is inherently interpretable, with extracted facts, generated questions, and retrieved evidence directly reflecting the information units contributing to the verification process."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should discuss different external sources that the LLMs can use for evidence retrieval.",
            "weight": 0.075,
            "evidence": [
              "Web-based: Similarly, we also submit the same question(s) to our web search engine (Web Search). We then take the top-k (e.g., 5) web passages returned for each question and combine them to create a consolidated set of answers. Additionally, Web Search is able to highlight the short answer a for each retrieved passage p. The final retrieval list from Web Search is in the format [(p1, a1),(p2, a2), ...,(pk, ak)]."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should provide examples of verification questions which can be used to verify facts in the generated response.",
            "weight": 0.075,
            "evidence": [
              "Type-aware Question Generation (TQGen). Consider the triple (Taylor Swift; birthdate; 1989). If the output of the QGen component is: \"When was Taylor Swift born?\", the retrieved evidence would probably be the exact birthdate. To generate a question as specific and close to the answer as desired, we propose a type-aware question generation approach. Using TQGen, the generated question will be: \"In which year was Taylor Swift born?\". To this end, we adopt the Chain-ofThought paradigm. This involves two steps: we instruct our model to first find the \"type\" of the Object in the input triple, and then generate a question conditioned on the obtained type information. TQGen guides the subsequent information retrieval component to target and retrieve the exact fact that we aim to verify. Prompting LLMs with two human demonstrations was sufficient for this task",
              "Context-driven Question Generation (CQGen). In addition to generating a precise type-aware question, we need to provide context for extended triples so that the retrieved evidence corresponds to the exact situation that requires verification. Consider the extended triples mentioned earlier, (Taylor Swift; moved; move_ID; place; Nashville) (Taylor Swift; moved; move_ID; age; 14) where the focus is on generating a question for the first triple. If we only feed the first triple to QGen, the output would not consider the time when the relocation happened. To generate a context-driven question, we need to also feed the second triple, the context triple, to Context-driven QGen (CQGen). The output of CQGen in this example is \"To which city did Taylor Swift move to at the age of 14?\". For this task, we prompt LLMs with two examples."
            ]
          },
          {
            "name": "most_important_item_5",
            "criterion": "The answer should discuss the challenges of question generation technique for hallucination mitigation, such as computation overhead and question quality.",
            "weight": 0.075,
            "evidence": [
              "5. Challenges and Considerations- Computational Overhead: Implementing question generation may increase processing time and resource requirements.- Question Quality: The effectiveness of this approach depends on the quality and relevance of generated questions.- Integration Complexity: Incorporating question generation into existing LLM architectures may present technical challenges."
            ]
          },
          {
            "name": "most_important_item_6",
            "criterion": "The answer should mention different types of hallucination.",
            "weight": 0.075,
            "evidence": [
              "* Entity-error Hallucination. This type of hallucination refers to the situations where the generated text of LLMs contains erroneous entities, such as person, date, location, and object, that contradict with the world knowledge. As shown in Table 1, when inquired about \"the announcement date of Nokia 3510 phone\", the model generates an erroneous date \"October 2002\", in conflict with the real fact \"March, 2002\".",
              "* Relation-error Hallucination. This type of hallucination refers to instances where the generated text of LLMs contains wrong relations between entities such as quantitative and chronological relation. As shown in Table 1, the model generates \"Among the chemical elements that make up the human body, calcium is more than oxygen\" (wrong quantitative relation) and \"Aaron Gillespie was born before Nathan Leone\" (wrong chronological relation).",
              "* Incompleteness Hallucination. LLMs might exhibit incomplete output when generating lengthy or listed responses. This hallucination arises when LLMs are asked about aggregated facts and they fail to reserve the factual completeness. For example, as presented in Table 1, when inquired \"list ten book titles on social cognitive theory\", the model only generates eight book titles; and the statement \"in an ecosystem organisms include consumers and producers\" is factually incomplete.",
              "* Outdatedness Hallucination. This type of hallucination refers to situations where the generated content of LLMs is outdated for the present moment, but was correct at some point in the past. This phenomenon arises primarily due to the fact that most LLMs were trained on time-limited corpora. For example, when asked about \"the present president of the United States\", the model trained on corpora before 2021 will generate \"Donald Trump\" instead of the latest fact \"Joe Biden\".",
              "* Overclaim Hallucination. This type of hallucination refers to cases where the statement expressed in the generated text of LLMs is beyond the scale of factual knowledge (Schlichtkrull et al., 2023). For example, as shown in Table 1, the model generates overclaimed statements \"the only way to lose weight is to exercise\" and \"you can grow taller just by drinking milk\".",
              "* Unverifiability Hallucination. In some cases, the information generated by LLMs cannot be verified by available information sources. For example, LLMs might generate plausible but non-existent academic reading lists. As demonstrated in Table 1, it cannot find any information about the book \"Cognitive Foundations of Social Behavior\" from existing sources."
            ]
          },
          {
            "name": "most_important_item_7",
            "criterion": "The answer should discuss that hallucination mitigation can happen during training and generation phases.",
            "weight": 0.075,
            "evidence": [
              "Thus far, two classes of approaches have been proposed to address the issue of hallucination: (i) preventing LLMs from hallucinating, which involves implementing strategies during the training and/or generation processes; (ii) mitigating hallucination after generation. (Manakul et al., 2023) introduced another taxonomy of classification, categorizing methods into black-box and gray-box. Factuality checks during and/or after generation without relying on external resources are known as black-box methods, while those using external resources are referred to as gray-box methods"
            ]
          }
        ]
      }
    },
    "case_id": "fb607bc177d2efb926cb3dff15668861",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "During pre-training, why is the transformer embedding layer initialized randomly, rather than with pre-trained embeddings from existing models?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "During pre-training, why is the transformer embedding layer initialized randomly, rather than with pre-trained embeddings from existing models?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly define the pre-training phase in the context of transformer models and explain its importance, particularly given the high computational cost.",
            "weight": 0.075,
            "evidence": [
              "Transformers-based models are well suited for transfer learning. In fact, many language models are pre-trained on huge amounts of unlabelled data with different techniques and finally are fine-tuned on the target task. Pre-training is the phase that requires most computational effort, but then the languages models can be shared and fine-tuned on many different tasks in only a few training steps."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should define the embedding layer in transformer models and emphasize the importance of proper initialization.",
            "weight": 0.075,
            "evidence": [
              "The goal of an embedding layer is to enable a model to learn more about the relationships between words, tokens, or other inputs. This embedding layer can be viewed as transforming data from a higher-dimension space to a lower-dimension space, or it could be viewed as mapping data from a lower-dimension space to a higher-dimension space.",
              "The embedding layer is a crucial component of the transformer architecture, responsible for converting raw input tokens into dense vector representations.",
              "### The Role of Embeddings in TransformersTransformers rely on embedding layers to map discrete input tokens (words, subwords, or even characters) into continuous vector spaces. These embeddings are then fed into the multi-head attention and feed-forward layers, which help capture complex patterns and relationships between tokens.",
              "Because of the massive amount of resources and time required to train these models, any types of optimizations would be beneficial to save costs. Therefore, when training future transformer-based language models, initializing the right parameter values can be one of many methods that can help train the model more effectively."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should explain how the embedding layer can be initialized randomly.",
            "weight": 0.075,
            "evidence": [
              "3.1 Random Embeddings Xavier (Glorot and Bengio, 2010): Each xij is sampled from the uniform distribution U([?] p 6/(N + D), p 6/(N + D)) in an i.i.d. manner. As mentioned in (Glorot and Bengio, 2010), since the variance depends on the forward and backward pass gradients, the restricted range keeps the variance constant across layers to prevent exploding or vanishing gradients. The range is inversely proportional to [?] N and in most cases, N > D, meaning that a larger vocabulary size equates to a smaller range. With such a dependency on the vocabulary size, there are cases when this initialization may not be optimal especially when only a small portion of the vocabulary is being used."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should explain how the embedding layer can be initialized using pre-trained embeddings.",
            "weight": 0.075,
            "evidence": [
              "3.2 Pre-trained Word Embeddings GloVe (Pennington et al., 2014): We use the GloVe word embedding vectors that are trained by the following objective. J(w, b) = X V i,j=1 f(xi,j )(w T i w~j + bi + ~bj [?] log xi,j ) 2 Here, wi and w~i are the word and context vectors, respectively, and b and ~b are the respective bias parameters. Note that unlike the Xavier initialization, the above objective does not constrain the range of the vectors and thus can, in principle, be unbounded. In practice, the range of the values depend upon the initialization of the word vectors during Glove training and the size and nature of the training data itself."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should discuss that although pre-trained parameters can help improve training effectiveness in non-transformer based models, it has become common practice now to use random initialization schemes, rather than the pre-trained embeddings, when training transformer based models from scratch due to better performance.",
            "weight": 0.075,
            "evidence": [
              "There are many good reasons for initializing the embedding layer with pre-trained word vectors such as Glove (Pennington et al., 2014), Word2Vec (Mikolov et al.,2013), and more recently sub-word vectors (Bojanowski et al., 2017) from language models such as BERT (Devlin et al., 2019). For one, they have been shown to capture a range of useful knowledge including lexical, syntactic, and even some types of factual relations (Pennington et al., 2014). Also, from a transfer learning perspective, pre-trained parameters can help improve training effectiveness and convergence in downstream tasks, especially when training data is limited. Indeed, pre-trained token vectors1 have been shown to be useful for a wide-range of applications in other non-transformer based models (Kocmi and Bojar, 2017; Kim, 2014; Collobert et al., 2011; Qi et al., 2018; Lample et al., 2016).",
              "However, when training transformer based models from scratch, it has become standard practice to prefer randomly initialized embeddings over pre-trained embeddings. Our experiments also indicate that random initialization performs better than pre-trained word embeddings, while for sub-word embeddings the trends are not consistent."
            ]
          },
          {
            "name": "most_important_item_5",
            "criterion": "The answer should explain that model sensitivity to parameter distribution and positional encodings can make certain pre-trained embeddings ineffective in transformers. However, standardizing some pre-trained embeddings to the Xavier range can fix this problem.",
            "weight": 0.075,
            "evidence": [
              "### **Parameter Distribution Sensitivity**Transformers are highly sensitive to the initial distribution of parameters. Pre-trained embeddings, such as those from GloVe or T5, often have a wider distribution of values compared to random initialization schemes. This wider distribution can lead to issues like:- **Saturated Outputs**: Large initial values can cause neurons to saturate, making gradients very small and hindering effective learning during the early stages of training.- **Training Instability**: The non-convex nature of the optimization landscape in transformer models can make training with pre-trained embeddings less stable.Random initialization, often using schemes like Xavier or He initialization, provides a more controlled and narrow range of values, which helps in maintaining stable gradients and effective learning dynamics from the start[3].### **Interactions with Positional Encodings**Transformers rely on positional encodings to capture the order of tokens in a sequence, as they do not inherently understand sequence order. When pre-trained embeddings with large values are used, they can overshadow the positional encodings, leading to a loss of positional information. This can degrade the model's performance on tasks that require understanding the order of tokens.Random initialization ensures that the embedding values are small enough to allow positional encodings to have a significant impact, preserving the sequence information crucial for many tasks[3].",
              "Finding 1: Pre-trained embeddings, such as GloVe, T5, and mT5, with variance many orders of magnitude higher than the variance of Xavier initialized embeddings tend to fare worse. Meanwhile, BERT and mBERT embeddings with a more similar range to Xavier performs on par or better than Xavier. Although the exact range for the optimal performance is not known, the Xavier initialization range have been widely used and tested to perform well. Therefore, pre-trained embeddings that do not fall within the Xavier specifications tend to be less effective. We know the embedding variance is important as experiments show that standardizing GloVe, T5, and mT5 embeddings to match the variance of the Xavier initialization scheme lead to substantial improvements. However, standardizing BERT and mBERT embeddings usually either made a neutral or negative effect to the performance"
            ]
          },
          {
            "name": "most_important_item_6",
            "criterion": "The answer should conclude that due to lack of theoretical backing and mixed empirical results, the true effect of pre-trained embedding on the training performance of transformer models needs further investigation.",
            "weight": 0.075,
            "evidence": [
              "### **Empirical Performance**Empirical studies have shown mixed results when using pre-trained embeddings in transformers. For instance, while some embeddings like those from BERT and mBERT have shown to perform better than random initialization, others like GloVe and T5 have not fared as well. This discrepancy can be attributed to the differences in value distributions and their interactions with the model's architecture and training dynamics[3].",
              "With only a marginal performance increase using Pre-trained_Xavier embeddings compared to Xavier initialization, its potential value needs further investigation. The storage space and computational costs required for pre-trained word vectors with lower-than-expected returns also calls for additional development. Meanwhile, there are other many well-researched and developed forms of knowledge transfer with more effective training strategies. Depending on how much gains an uncompromised embedding including position and semantic information can provide, further studies are required to come to a conclusion on the value of initializing pre-trained embeddings."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should discuss some benefits of random initialization.",
            "weight": 0.0375,
            "evidence": [
              "a) Flexibility and Adaptability:Random initialization allows the model to learn representations specific to the task and dataset at hand. This approach enables the model to adapt its embeddings to the particular nuances and patterns present in the training data.",
              "b) Avoiding Bias:Starting with random weights helps prevent the model from inheriting biases present in pre-trained embeddings, which might not be suitable for the specific task or domain.",
              "c) Computational Efficiency:Random initialization is computationally simpler and faster than loading and potentially fine-tuning pre-trained embeddings.",
              "a) Task-Specific Learning:Random initialization encourages the model to learn task-specific representations from scratch, which can be advantageous when dealing with specialized domains or novel tasks.",
              "b) Consistency with Other Layers:Since other layers in the transformer (e.g., attention mechanisms, feed-forward networks) are typically initialized randomly, using random embeddings maintains consistency in the learning process across all model components.",
              "c) Potential for Better Generalization:Learning embeddings from scratch may lead to better generalization, especially when the pre-training dataset is large and diverse (Devlin et al., 2019)[2]."
            ]
          },
          {
            "name": "nice_to_have_item_1",
            "criterion": "The answer should discuss the alternatives to random initialization such as transfer learning, and multilingual models.",
            "weight": 0.0375,
            "evidence": [
              "4. Alternatives to Random InitializationWhile random initialization is common, there are scenarios where using pre-trained embeddings can be beneficial:a) Transfer Learning:When working with limited data or in domains similar to those of existing pre-trained models, initializing with pre-trained embeddings can provide a good starting point (Peters et al., 2018)[3].b) Multilingual Models:For multilingual transformers, using pre-trained embeddings from monolingual models can help in aligning representations across languages (Conneau et al., 2020)[4].5. Hybrid ApproachesSome researchers have explored hybrid approaches:a) Partial Initialization:Initializing only a portion of the embedding layer with pre-trained vectors while randomly initializing the rest.b) Gradual Unfreezing:Starting with pre-trained embeddings but allowing them to be fine-tuned after initial training phases (Howard and Ruder, 2018)[5]."
            ]
          }
        ]
      }
    },
    "case_id": "e2d0eb391fbf674c070c74c387ee6248",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are the leading thoughts about how to teach computer science to college students in the age of generative AI?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are the leading thoughts about how to teach computer science to college students in the age of generative AI?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "Near the beginning, the answer should briefly discuss the impact of generative AI on computer science education",
            "weight": 0.12,
            "evidence": [
              "Amid collective enthusiasm for generative artificial intelligence (AI) built atop large language models (LLMs), there is apprehension about AI's ability to disrupt education. Students can now complete assignments or write essays entirely with AI, undermining the fundamental goals of teaching and learning. As such, a common response to this development among educators has been to forbid the use of AI outright.",
              "As educators concerned with issues of academic dishonesty ourselves [ 7], we recognized the complications that some AI tools posed by being too helpful to students, particularly novices in computer science. Like other courses, we quickly implemented policies that banned tools such as ChatGPT, GitHub Copilot, and other AI-based software that suggest or complete answers to questions or lines of code. Simultaneously, we felt that it would be a missed opportunity if we did not leverage the newfound power of AI to enhance students' learning.",
              "With the advance of Generative AI tools such as ChatGPT and Copilot, they have become integral tools for many students, particularly those in introductory programming courses[31 ]. They can assist students by providing code suggestions, explaining programming concepts, identifying and resolving errors, and generating code documentation [10 , 18 ]. Their widespread adoption signifies a major shift in how programming is learned and practiced. However, this increasing reliance on Generative AI tools presents significant challenges. One primary concern is that students might become overly dependent on these tools, potentially hindering their ability to develop fundamental programming skills [32 ]. Additionally, AI-generated solutions might lead to academic dishonesty or reduced problem-solving capabilities among students [ 7]. This dichotomy has sparked a debate among educators: while some argue against the use of Generative AI in learning due to these risks, others advocate for its judicious use to enhance learning outcomes"
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should mention the importance of focusing on high-level skills such as problem-solving, critical thinking, and collaboration, as generative AI can automate routine coding tasks",
            "weight": 0.12,
            "evidence": [
              "Our findings, alongside those of Prather et al. [32 ], suggest that some students may struggle with new metacognitive difficulties when using generative AI tools, including being conceptually behind in course material but unaware of it due to a false sense of confidence. Further exploration is needed to understand the underlying reasons for this undesired behavior and to encourage a more constructive use of Generative AI that promotes deep understanding and problem-solving skills in computing education.",
              "Xue et al. [43] investigated the impact of ChatGPT on introductory programming students, finding that the AI tool can enhance students' self-efficacy by providing immediate feedback and support, thereby reducing the anxiety associated with complex problem-solving tasks. Furthermore, Denny et al. [ 9] emphasized the importance of designing AI teaching assistants that not only assist with immediate problem-solving but also help students develop long-term self-efficacy by encouraging independent learning and critical thinking.",
              "Some students, for instance, asked for open-ended questions besides the multiple-choice ones already available. However, unlike multiple-choice quizzes, properly evaluating answers to open-ended questions requires the adoption of suitable evaluation strategies. These strategies are necessary to guide the model toward more valuable and constructive feedback. Open-ended questions demand nuanced understanding and assessment, which can help in identifying specific areas of improvement and offering personalized insights. Implementing these evaluation techniques will enhance the system's ability to support deeper learning and critical thinking.",
              "In recent years, the development and study of e-learning technologies have gained significant attention due to their potential to transform teaching and learning processes. The scientific literature underscores the pivotal role these tools can play in enhancing various educational activities, including the creation of engaging content, facilitating collaboration and interactivity in large classroom settings, and providing effective tutoring support.",
              "Instructors can effectively incorporate AI-driven tools and technology to help students develop critical thinking and problem-solving skills by integrating them into their teaching methods. One approach is to use AI-generated answers as a basis for classroom discussions. Presenting students with multiple AI-generated answers to a problem or question and asking them to evaluate the correctness and reasoning behind each solution encourages active engagement with the content and the development of their analytical skills.",
              "### Emphasizing Foundational ConceptsOne of the key thoughts in teaching computer science is to emphasize foundational concepts that remain relevant regardless of technological advances. These include:- **Algorithms and Data Structures**: Fundamental principles that are essential for effective problem-solving.- **Computational Thinking**: The ability to break down complex problems into manageable parts.- **Mathematical Foundations**: Understanding logic, discrete mathematics, and probability which underpin many AI algorithms.By ensuring a strong grasp of these core areas, students can adapt more easily to new technologies, including generative AI.",
              "Developing Critical Thinking and Problem-Solving AbilitiesWith AI capable of handling many routine coding tasks, education should shift towards developing higher-order thinking skills.a) Algorithmic Thinking: Emphasize the importance of understanding and designing algorithms, rather than just implementing them.b) System Design: Focus on teaching students how to design and architect complex systems, a skill that AI has not yet mastered.c) Creative Problem-Solving: Encourage students to think creatively and approach problems from multiple angles, skills that are distinctly human."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should cover the need for hands-on experience with AI tools and technologies.",
            "weight": 0.12,
            "evidence": [
              "First, the training should cover the technical aspects of AI, so that educators can competently operate AI tools. This includes understanding how to set up AI systems, use AI-driven tools, and interpret the data these tools generate. Second, pedagogical training should also be provided, focusing on how to incorporate AI into various pedagogical strategies and create a blend of traditional and AI-assisted methods. This could involve training instructors on how to use AI to personalize learning materials, provide real-time feedback, and overcome language barriers",
              "Working Effectively with AI ToolsThis element suggests a move from being a more passive user, to the 'user-as-developer' approach, and encourages educators to \"leverage free, open access online resources. . .then work with students to develop unit specific examples and lead discussion on the effective use of tools relevant to the discipline, unit or assessment task context\" [42].",
              "Being able to write effective prompts will enable students to use tools such as ChatGPT effectively and efficiently. When integrating tools such as ChatGPT into their lessons, it is recommended that teachers gain AI literacy skills, especially for students to gain prompt writing skills. Teachers can apply metacognitive strategies when it is considered important that students' thinking skills are developed in order to benefit effectively from tools such as AI. At this point, metacognitive prompts can be used. The metacognitive prompt aims to enable students to think and evaluate their own learning processes. Such teaching strategies help students understand, control and regulate their own learning processes.",
              "### Integrating AI into the CurriculumGiven the significance of AI in the field, it is critical to integrate AI topics into the CS curriculum. Key areas to be covered include:- **Introduction to AI and Machine Learning**: Basic concepts, methodologies, and applications.- **Ethics in AI**: Understanding the ethical implications and responsibilities of developing and deploying AI technologies.- **Generative AI**: Discussing models like GPT-3, their architecture, capabilities, and limitations.Practical courses should offer hands-on experience with AI tools and platforms, such as TensorFlow and PyTorch, to build familiarity with AI development processes."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should mention interdisciplinary approaches for teaching computer science in the context of generative AI",
            "weight": 0.12,
            "evidence": [
              "As AI becomes increasingly pervasive across various fields, computer science education should adopt a more interdisciplinary approach.a) Cross-disciplinary Projects: Encourage collaboration with other departments to create projects that apply computer science and AI to solve real-world problems in various domains.b) AI in Context: Teach students how to apply AI solutions in different contexts, considering the unique challenges and requirements of each field.",
              "### Project-Based LearningProject-based learning is a highly effective approach for teaching computer science in the context of generative AI. This involves:- **Real-World Projects**: Encouraging students to work on projects that solve real-world problems using AI. This provides practical experience and a deeper understanding of the material.- **Collaborative Work**: Promoting teamwork to mirror industry practices and enhance peer learning.- **Interdisciplinary Projects**: Encouraging projects that combine CS with other disciplines, such as biology, economics, or social sciences, to showcase the broad applicability of generative AI."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should mention the importance of teaching students about the ethical implications of generative AI.",
            "weight": 0.12,
            "evidence": [
              "Finally, training programs should address the ethical considerations of using AI in the classroom. Educators should be informed about potential issues related to data privacy, fairness, and inclusivity. They should be encouraged to understand the implications of AI for education and trained to make ethical decisions about its use. Investing in this training will not only ensure that educators can effectively use AI tools to enhance learning experiences but also equip them to prepare students for a future where AI becomes integral in most professional environments. Such an initiative necessitates the commitment and investment of universities, but the potential rewards in terms of improved teaching and learning outcomes would be substantial.",
              "Ethical Use of AI ToolsWhile he acknowledges that several of the legal and ethical issues around AI are still to be resolved, he proposes that users should familiarize themselves with issues regarding algorithm transparency, data ownership, privacy, hidden labor, embedded bias, and undisclosed plagiarism.",
              "This study has provided insights into the ethical concerns and policy strategies associated with the utilization of ChatGPT in educational settings. Through the application of the AHP, this study have identified and prioritized ten key ethical concerns, including copyright, legal, and compliance issues; privacy and confidentiality considerations; academic integrity; incorrect reference and citation practices; safety and security; educational equity; plagiarism; infodemics and misinformation; bias response; and risk hallucination through manipulation and misleading information.",
              "### Addressing the Ethical and Social ImpactAI technologies, including generative AI, raise significant ethical and social questions. Educators should ensure that students are aware of these dimensions:- **Ethics Courses**: Mandatory courses focusing on the ethical implications of AI, data privacy, bias, and fairness.- **Public Policy**: Discussions on the role of AI in society and the importance of informed policy-making.- **Responsible AI**: Teaching students the principles of designing and deploying AI systems responsibly."
            ]
          }
        ]
      }
    },
    "case_id": "fc927b39177cd2aad8a8cbcef75ab62c",
    "annotator": "Annotator 1 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What data structures are commonly used to solve the Range Minimum Query (RMQ) problem?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What data structures are commonly used to solve the Range Minimum Query (RMQ) problem?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with a brief explanation of Range Minimum Query.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Given a static array of n totally ordered objects, the range minimum query problem is to build a data structure that allows us to answer efficiently subsequent on-line queries of the form \"what is the position of a minimum element in the subarray ranging from i to j?\".",
              "The Range Minimum Query (RMQ) problem is, given an array A[1,n] with elements from a totally ordered set, build a data structure that receives any pair of positions 1 [?] i [?] j [?] n and returns rmqA(i, j) = argmini[?]k[?] jA[k], that is, the position of a minimum value in A[i, j]. In many cases one prefers the leftmost position when there are ties.",
              "- The array is preprocessed into a table where each entry \\(ST[i][j]\\) stores the minimum value in the subarray starting at index \\(i\\) of length \\(2^j\\).- RMQ can be served using two overlapping intervals covered by \\(2^j\\) length, minimizing through constant-time comparisons."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide information on the data structures used to solve the Range Minimum Query (RMQ) problem.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Most of the previous linear data structures for solving the 1D RMQ problem are based on the following fact: there exists an O(N )-bits structure (also built within N comparisons) such that any RMQ query can be answered without any comparison. A popular structure that has this property is the Cartesian tree [20]. Unfortunately, as shown by Demaine, Landau and Weimann[8], it is impossible to generalize the Cartesian tree to higher dimensions.",
              "There exists a data structure supporting four-sided queries in O(log log n + k) time and O(m log m) bits of space where m is the number of points in a slab. The data structure relies on a universal data structure for two-dimensional range selection queries.",
              "There exists a data structure that supports five-sided three-dimensional range report-ing queries in O((k + 1) log log n) time and uses O (n log log n) space.",
              "The new data structure is described in steps, starting with a previous O(n), O( [?]n) data structure, extending it to O(n log logn), O(log logn) by applying the technique recursively, eliminating recursion to obtain O(n log logn), O(1), and finally reducing the space to O(n), O(1)."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should have the types of data structures used to solve the Range Minimum Query (RMQ) problem.",
            "weight": 0.17142857142857143,
            "evidence": [
              "1. Segment TreeA segment tree is a versatile data structure that can be used to solve the RMQ problem efficiently.Characteristics:- Time Complexity: O(log n) for query, O(n log n) for construction- Space Complexity: O(n)",
              "2. Sparse TableThe sparse table is another popular data structure for solving RMQ, especially when the array is static (i.e., no updates are needed).Characteristics:- Time Complexity: O(1) for query, O(n log n) for construction- Space Complexity: O(n log n)",
              "3. Cartesian TreeA Cartesian tree is a binary tree structure that can be used in conjunction with other techniques to solve RMQ efficiently.Characteristics:- Time Complexity: O(1) for query (with additional preprocessing), O(n) for construction- Space Complexity: O(n)",
              "4. Block DecompositionBlock decomposition is a technique that can be combined with other data structures to achieve a good balance between query time and space complexity.Characteristics:- Time Complexity: O(1) for query, O(n) for construction- Space Complexity: O(n)",
              "5. Fischer-Heun StructureThe Fischer-Heun structure combines ideas from sparse tables and block decomposition to achieve optimal time and space complexity for RMQ.Characteristics:- Time Complexity: O(1) for query, O(n) for construction- Space Complexity: O(n)"
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should explain why Range Minimum Query (RMQ) is used.",
            "weight": 0.08571428571428572,
            "evidence": [
              "The Range Minimum Query (RMQ) problem is a fundamental problem in computer science, particularly in the field of data structures and algorithms. It involves efficiently finding the minimum element in a given range of an array."
            ]
          }
        ]
      }
    },
    "case_id": "01a5d21763c1d38dd26ec52bd719e5ee",
    "annotator": "Annotator 2 Assignments",
    "agreement": true
  },
  {
    "initial_prompt": "What datasets and methods are used to pre-train models on table specific tasks?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What datasets and methods are used to pre-train models on table specific tasks?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with explaining the uses of datasets and methods used to pre-train models on table specific tasks.",
            "weight": 0.13333333333333333,
            "evidence": [
              "The problem of verifying whether a textual hypothesis holds based on the given evidence, also known as fact verification, plays an important role in the study of natural language understanding and semantic representation. However, existing studies are mainly restricted to dealing with unstructured evidence (e.g., natural language sentences and documents, news, etc), while verification under structured evidence, such as tables, graphs, and databases, remains under-explored.",
              "Data formats such as CSV/TSV, PARQUET, XML and JSON are commonly used to train machine learning algorithms.",
              "Pre-training methods for table-specific models often involve adapting language model techniques to tabular data structures. Common objectives include masked language modeling, query execution prediction, and contrastive learning, with a focus on capturing table-specific relationships and semantics."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide datasets used to pre-train models on table specific tasks.",
            "weight": 0.13333333333333333,
            "evidence": [
              " To this end, we construct a large-scale dataset called TabFact with 16k Wikipedia tables as the evidence for 118k human-annotated natural language statements, which are labeled as either ENTAILED or REFUTED. TabFact is challenging since it involves both soft linguistic reasoning and hard symbolic reasoning.",
              "Human-created: Human-created data usually show higher quality than web-crawled ones which might require careful prepossessing for their size, diversity and noises. It is common to manually add extra labels for existing dataset. E.g., ToTTo, a well-labeled dataset for table-to-text with NL descriptions and corresponding web tables, was used by StruG for pretraining.",
              "Machine-synthesized: Synthesized data are more targeted and controllable, but require careful designs to ensure meaningfulness and diversity. GraPPa proposed an SCFG (synchronous context-free grammar) and applied it to synthesize sentenceSQL pairs over tables. [Eisenschlos et al., 2020] created counterfactual and synthetic statements for existing Wikipedia tables",
              "the frontier LM (T5) could easily achieve promising or even SOTA results on table tasks, demonstrating a strong relationship between text and tables, e.g., in linguistic and world knowledge aspects.",
              "WikiTables is a large-scale dataset extracted from Wikipedia tables. It contains millions of tables with diverse content and structure.",
              "TableBank is a large-scale image-based table detection and recognition dataset with fine-grained annotations.",
              "WebTables is a large corpus of relational HTML tables extracted from the Web."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide methods used to pre-train models on table specific tasks.",
            "weight": 0.13333333333333333,
            "evidence": [
              "Table-BERT, this model views the verification task completely as an NLI problem by linearizing a table as a premise sentence p, and applies state-of-the-art language understanding pre-trained model to encode both the table and statements h into distributed representation for classification. This model excels at linguistic reasoning like paraphrasing and inference but lacks symbolic reasoning skills.",
              "Latent Program Algorithm, this model applies lexical matching to find linked entities and triggers to filter pre-defined APIs (e.g. argmax, argmin, count, etc). We adopt bread-first-search with memorization to construct the potential program candidates, a discriminator is further utilized to select the most \"consistent\" latent programs. This model excels at the symbolic reasoning aspects by executing database queries, which also provides better interpretability by laying out the decision rationale.",
              "More sophisticated architectures have been developed to better capture table structure. TUTA pre-trains on general tables with hierarchies to model table structural information  (34, Dong et al., 2021). TABBIE furthers the design of TaBERT by using two independent transformers to encode the rows and columns of tables jointly  (81, Chen et al., 2023).",
              "TableGPTTableGPT is a generative pre-training approach for table understanding and generation tasks. It uses a transformer-based architecture to model tables as sequences."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should provide challenges involved in training the models on table specific tasks.",
            "weight": 0.13333333333333333,
            "evidence": [
              "Tabular data is challenging to interpret by machines because of the limited context available to resolve semantic ambiguities, the layout of tables that can be difficult to handle, and the incompleteness of KGs in general. Classical Natural Language Processing (NLP) tasks for unstructured text handle poorly such tables since they do not leverage the table structure and the underlying semantics [17]."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should provide information on what is table specific data.",
            "weight": 0.06666666666666667,
            "evidence": [
              "Tabular data often refers to data that is organized in a table with rows and columns. We observe that this data format is widely used on the Web and within enterprise data repositories. Tables potentially contain rich semantic information that still needs to be interpreted. The process of extracting meaningful information out of tabular data with respect to a semantic artefact, such as an ontology or a knowledge graph, is often referred to as Semantic Table Interpretation (STI) or Semantic Table Annotation.",
              "The main idea to make tabular data intelligently processable by machines is to find correspondences between the elements composing the table with entities, concepts, or relations described in knowledge graphs (KG) which can be of general purposes such as DBpedia [4] and Wikidata [5], or enterprise specific."
            ]
          }
        ]
      }
    },
    "case_id": "01d02d3984b60367c275972613992a64",
    "annotator": "Annotator 2 Assignments",
    "agreement": true
  },
  {
    "initial_prompt": "What are the main challenges in adapting transformer-based models for tabular data representation?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are the main challenges in adapting transformer-based models for tabular data representation?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with providing information on the adaptation of various models for tabular data representation.",
            "weight": 0.24,
            "evidence": [
              "Subsetting features of Tabular data (SubTab), that turns the task of learning from tabular data into a multi-view representation learning problem by dividing the input features to multiple subsets.  We argue that reconstructing the data from the subset of its features rather than its corrupted version in an autoencoder setting can better capture its underlying latent representation.",
              "SubTab can: i) construct a better representation by using the aggregate of the representation of the subsets, a process that we refer as collaborative inference ii) discover the regions of informative features by measuring predictive power of each subset, which is useful especially in high-dimensional data iii) do training and inference in the presence of missing features by ignoring corresponding subsets and iv) use smaller models by reducing input dimension, making it less prone to overfitting.",
              "The TabTransformer is built upon self-attention based Transformers. The Transformer layers transform the embeddings of categorical features into robust contextual embeddings to achieve higher prediction accuracy."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide challenges involved in adapting transformer-based models for tabular data representation",
            "weight": 0.24,
            "evidence": [
              "While table-to-text generation is an important and challenging task, most methods rely on fine-tuning existing pre-trained language models.",
              "While row/column positional embeddings can better map context and table content, such as in QA or TFC tasks, it cannot overcome the challenge of empty cells, nested row headers, or descriptive cells in complex spreadsheets such as financial tables.",
              "\"FT-Transformer requires more resources (both hardware and time) for training than simple models such as ResNet and may not be easily scaled to datasets when the number of features is \"too large\" (it is determined by the available hardware and time budget).",
              "FT to tabular datasets stems from their diversity: tabular datasets differ greatly in their domains, number of classes, feature composition, etc. These differences make the training of a generic features engineering tool very difficult.",
              "The main challenge in learning from multiple tabular datasets, aside from their highly diverse content and characteristics, is their different number of classes. Such a setup makes using a single output layer with a fixed number of entries impossible.",
              "The challenge stems from the concurrent existence of numerical and categorical feature types, complex, irregular dependencies between the features, and other data-related issues such as scales, outliers, and missing values.",
              "Tabular data often includes both categorical and numerical features. Efficiently pre-processing these features to create meaningful embeddings is crucial for transformer-based models. This can be challenging, especially when dealing with high-dimensional data.",
              "Transformer-based models for tabular data need to be integrated with traditional machine learning pipelines, which can be challenging. This includes ensuring seamless integration with existing data preprocessing and feature engineering steps.",
              "Another significant challenge is the need to model the row-column structure of tables, which is crucial for correctly interpreting relational data  (8, Yu et al., 2020). The hierarchical nature of tabular data, with its complex relationships between headers, rows, and cells, is not naturally captured by the sequential processing of traditional transformers  (34, Huang et al., 2023)."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should provide information on the overall challenges involved in adapting models for tabular data representation.",
            "weight": 0.12,
            "evidence": [
              "Furthermore, many existing deep learning models for tabular data are designed for supervised learning scenario but few are for semi-supervised leanring (SSL). Unfortunately, the state-of-art SSL models developed in computer vision (Voulodimos et al. 2018; Kendall and Gal 2017) and NLP (Vaswani et al. 2017; Devlin et al. 2019) cannot be easily extended to tabular domain.",
              "Tabular data (or tables) are the most widely used data format in machine learning (ML). However, ML models often assume the table structure keeps fixed in training and testing. Before ML modeling, heavy data cleaning is required to merge disparate tables with different columns. This preprocessing often incurs significant data waste (e.g., removing unmatched columns and samples)."
            ]
          }
        ]
      }
    },
    "case_id": "183575a6b64714da860ca0dd4b2d1f48",
    "annotator": "Annotator 2 Assignments",
    "agreement": true
  },
  {
    "initial_prompt": "what are leading techniques for safe navigation in partially observable environments with moving obstacles?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "what are leading techniques for safe navigation in partially observable environments with moving obstacles?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with providing information about the importance of the topic",
            "weight": 0.13333333333333333,
            "evidence": [
              "Navigating in partially observable environments with moving obstacles is a challenging problem in robotics and autonomous systems. These environments are characterized by limited sensor information and dynamic obstacles, making safe navigation a complex task. Several advanced techniques have been developed to address this challenge."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide the list of techniques and technologies available for navigation with moving obstacles.",
            "weight": 0.13333333333333333,
            "evidence": [
              "The accuracy and availability of Global Navigation Satellite Systems (GNSS) has revolutionized navigation. As a result, world-wide, meter-level positioning is required for many applications. Unfortunately, satellite navigation signals are not available in all environments.",
              "researchers have devoted much effort into investigating the use of image sequences for navigation. Many different image-aided navigation techniques have been demonstrated, each with varying assumptions, and most using ad-hoc techniques.",
              "other methods including Wi-Fi, Bluetooth and sensors must be used for navigation."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide information about the leading techniques for safe navigation in partially observable environments with moving obstacles",
            "weight": 0.13333333333333333,
            "evidence": [
              "In local navigation techniques, sensors are usually employed to control the orientation and position of robot. For such use, LIDAR sensor is frequently used for automation purpose. LIDAR works independently as compared to GPS system; therefore, it has the capability of mapping the environment. LIDAR can be used independently but when coupled with other sensors like GPS, Inertial navigation system, and camera, it gives improved results.",
              "Vector Field Histogram (VFH) is another local navigation method used to solve the path planning problem for mobile robots. The idea of VFH is based on VFF (Virtual Force Filed) method. As the name indicates its a field, so obstacles detected at a certain distance from vehicle will apply repulsive force on vehicle to move away from obstacles and to draw the vehicle toward goal point an attractive force is used.",
              "Laser Navigation is considered to be the most effective and efficient technique for obstacle avoidance and path following. It does not require any wires, rails, and tracks for its motion. A beam is transmitted and received form sensor, the time taken by the beam to travel and come back helps is determining the distance and angle which helps vehicle in its motion.",
              "During the fuzzification process vehicle and obstacle position is obtained from sensors employed on AGV. Then obstacle avoidance and path following rule base are created which are then infused with fuzzy inference system to get the desired direction for AGV movement",
              " The use of different kind of sensor also helps in guiding the robot for obstacle avoidance and path following. Infrared sensor is used to estimate the position and to avoid the obstacle.",
              "the robot attempts to reach a fixed goal position in a reconfigurable but unknown environment. Starting with no knowledge about the environment, the robot uses limited sensor information to locally detect objects and incrementally build and manipulate a world model. The robot may move objects if the goal cannot be reached or if moving the object may significantly shorten the path to the goal."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should provide information about categorizing various approaches.",
            "weight": 0.13333333333333333,
            "evidence": [
              "2.Probabilistic Methods2.1 Partially Observable Markov Decision Processes (POMDPs)POMDPs provide a framework for decision-making under uncertainty. They model the environment as a set of states, actions, and observations, allowing the agent to make decisions based on probabilistic beliefs about the current state [1].",
              "2.2 Monte Carlo Localization (MCL)MCL uses particle filters to estimate the robot's position in a partially observable environment. It continuously updates the belief state based on sensor readings and motion models [2].",
              "3. Sensor Fusion Techniques3.1 Extended Kalman Filter (EKF)EKF combines data from multiple sensors to estimate the state of the environment and the robot. It's particularly useful for tracking moving obstacles and updating the robot's position [3].",
              "3.2 Graph SLAM (Simultaneous Localization and Mapping)Graph SLAM builds a map of the environment while simultaneously localizing the robot within it. This technique is effective in partially observable environments as it continuously updates the map based on new observations [4].",
              "4. Machine Learning Approaches4.1 Deep Reinforcement LearningDeep RL algorithms, such as Deep Q-Networks (DQN) and Proximal Policy Optimization (PPO), can learn navigation policies in complex, partially observable environments with moving obstacles [5].",
              "4.2 Generative Adversarial Imitation Learning (GAIL)GAIL learns navigation strategies from expert demonstrations, which can be particularly useful in environments with moving obstacles [6].",
              "5. Predictive Planning5.1 Model Predictive Control (MPC)MPC uses a model of the environment to predict future states and optimize the robot's trajectory. It's particularly effective for avoiding moving obstacles by anticipating their future positions [7].",
              "5.2 Rapidly-exploring Random Trees (RRT)RRT and its variants (e.g., RRT*) efficiently explore the state space to find collision-free paths. These algorithms can be adapted to handle moving obstacles by repeatedly replanning [8].",
              "4. Hierarchical PlanningHierarchical planning approaches involve breaking down the navigation problem into multiple levels. For example, high-level task planners use LTL to synthesize actions, while middle-level motion planners generate motion plans based on safety criteria and sampling algorithms. This hierarchical approach helps address partial observability and ensures safe navigation.",
              "7. Optimization-Based Collision AvoidanceOptimization-based collision avoidance methods use techniques like sequential convex optimization and convex collision checking to ensure safe navigation. These methods are often used in real-time motion planning for autonomous systems."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should have information about static and dynamic environments.",
            "weight": 0.06666666666666667,
            "evidence": [
              "Depending on the nature of environment, path planning can be divided intostaticanddynamicenvironment. If obstacles change their position with respect to time, it is referred asstatic path planningand if obstacles change their position and orientation with respect to time, then it is referred asdynamic path planning."
            ]
          }
        ]
      }
    },
    "case_id": "3927ad11a6f7001173bfb2bbb9e8729e",
    "annotator": "Annotator 2 Assignments",
    "agreement": true
  },
  {
    "initial_prompt": "how can RRT-based path planning be adapted to efficiently handle narrow pathways",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "how can RRT-based path planning be adapted to efficiently handle narrow pathways",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin by introducing RRT-based path planning.",
            "weight": 0.13333333333333333,
            "evidence": [
              "Random Tree Star (RRT*) is a renowned sampling based planning approach. It has gained immense popularity due to its support for high dimensional complex problems. A significant body of research has addressed the problem of optimal path planning for mobile robots using RRT* based approaches.https://www.uhb.edu.sa/Lists/Books%20and%20Articals/Attachments/1131/BookFile-%20Paper_14-Optimal_Path_Planning_using_RRT_based_Approaches.pdf\"Rapidly exploring Random Tree Star (RRT*) has gained popularity due to its support for complex and high-dimensional problems. Its numerous applications in path planning have made it an active area of research."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide information on adapting RRT-based path planning to efficiently handle narrow pathways.",
            "weight": 0.13333333333333333,
            "evidence": [
              "RRT*-AB performs informed exploration in search space of known environment. It generates highly converged tree populated with useful nodes, as a result reducing memory requirements. First, connectivity region and goal-biased bounded sampling make fast convergence towards optimal solution. These two features enhance the valuable samples in the vicinity of goal region very quickly, as a result leading to attain optimal or near-optimal path efficiently in less time. Further, path is optimized at fast rate using concentrated sampling, node rejection technique and path pruning. Concentrated sampling selects nodes only in limited vicinity of found path, and node rejection technique rejects inadequate nodes to insert in tree.",
              "a learning-based Multi-RRTs approach called LM-RRT is presented to improve the efficiency of single query path planning with narrow passages. In the preprocessing step, global landmarks in narrow passages are identified by the improved Bridge Test algorithm. After generating multiple trees from the sampled roadmaps, the on-line et-greedy strategy is proposed to learn the probability for tree selection. The proposed LM-RRT algorithm can both utilize the local exploration ability of a single tree and improve the efficiency of global path planning in environments with narrow passages.",
              "For a given task, the proposed CNN model can predict the probability distribution of the optimal path on the map, which is used to guide the sampling process. The time cost and memory usage of the planned path are selected as the metric to demonstrate the effectiveness and efficiency of the NRRT*.",
              "we are extending our memory efficient RRT*FN algorithm to dynamic scenarios. Specifically, we retain the useful parts of the tree (the data structure storing the motion plan information) after a dynamic obstacle invalidates the solution path. We then employ two greedy heuristics to repair the solution instead of running the whole motion planning process from scratch. We call this new algorithm, RRT*FN-Dynamic (RRT*FND).",
              " The proposed improved the RRT algorithm introduces regression mechanism to fill local minima quickly and to prevent over-searching configuration space. Besides, it adopts an adaptive expansion mechanism to continuously improve reachable spatial information by refine the boundary nodes in joint space, avoiding repeatedly searching for extended nodes. Furthermore, it avoids the unnecessary iteration of the robotic manipulator forward kinematics solution and time-consuming collision detection in Cartesian space.",
              "The CERRT algorithm builds upon the RRT approach and incorporates two key components: a pre-allocated extension node method and a vertex death mechanism. These enhancements aim to improve vertex utilization and overcome the problem of becoming trapped in concave regions, a limitation of traditional algorithms. Additionally, the CERRT algorithm integrates environment awareness at collision points, enabling rapid identification and navigation through narrow passages using local simple sampling techniques.",
              "Several techniques focus on guiding RRT expansion more efficiently. Yung et al. introduced spatial skeleton extraction to calculate the skeleton of free space, combined with a greedy algorithm to increase RRT expansion and reduce unnecessary bends, resulting in faster search processes and shorter path lengths  (102, Yung et al., 2022). Zhao et al. proposed GMMM-RRTs, which use Gaussian Mixture Models learned from experiential paths to bias sampling, efficiently exploiting local space while maintaining global planning efficiency  (53, Zhao et al., 2018).",
              "Hybrid and multi-stage approaches have emerged as effective solutions for adapting RRT-based path planning to efficiently handle narrow pathways. These methods typically combine the strengths of different algorithms or employ multiple stages to address the challenges posed by constrained environments."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide the challenges involved in using RRT-based path planning.",
            "weight": 0.13333333333333333,
            "evidence": [
              "Although it ensures probabilistic completeness and asymptotic optimality, its slow convergence rate and large dense sampling space are proven problems.",
              "Rapidly random-exploring tree (RRT) and its variants are very popular due to their ability to quickly and efficiently explore the state space. However, they suffer sensitivity to the initial solution and slow convergence to the optimal solution, which means that they consume a lot of memory and time to find the optimal path.",
              "However, when confronted with narrow passages, RRT algorithms face significant challenges. The primary issue is that they can easily become limited to local solutions, struggling to find paths through tight spaces  (8, Huang et al., 2022). This limitation is particularly problematic for applications like microrobot navigation, where the ability to traverse narrow passages is crucial. The random sampling approach that makes RRT effective in open spaces can become a hindrance in tightly constrained environments, potentially missing optimal or even feasible paths through these critical areas [LLM MEMORY | 2024]."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should briefly provide advantages of the RRT-based path planning.",
            "weight": 0.13333333333333333,
            "evidence": [
              "The Rapidly-exploring Random Tree (RRT) algorithm is a powerful tool for path planning, particularly in high-dimensional and constrained spaces  (12, Kirillova et al., 2007). Its strengths lie in its ability to quickly explore entire environments, generate sub-optimal paths, and handle complex constraints without modeling the entire space  (100, Tian et al., 2022)  (8, Huang et al., 2022). RRT-based methods are computationally efficient for offline path generation and offer probabilistic completeness  (68, Cheng et al., 2020)."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should provide tradeoff between designing algorithms for narrow pathways and wide pathways in path planning.",
            "weight": 0.06666666666666667,
            "evidence": [
              "In half structured road scenarios, the planning area may be vast, which requires the planning strategy to be efficient and flexible enough to obtain feasible paths from the vast area. By generating flexible paths, path planning strategies with small planning steps may handle narrow gap collision avoidance scenarios; however, it is difficult for them to handle wide area scenarios. There should be a balance between planning efficiency and planning flexibility."
            ]
          }
        ]
      }
    },
    "case_id": "40ae2e68a52e2d5788f3da27f5d9decd",
    "annotator": "Annotator 2 Assignments",
    "agreement": true
  },
  {
    "initial_prompt": "what are the advantages and disadvantages of visiotactile, piezoelectric, trieboelectric tactile sensors for robots?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "what are the advantages and disadvantages of visiotactile, piezoelectric, trieboelectric tactile sensors for robots?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should start with briefly explaining visiotactile, piezoelectric, triboelectric tactile sensors.",
            "weight": 0.08571428571428572,
            "evidence": [
              "Visiotactile is a sensor with a semitransparent skin that has the dual capabilities of acting as a tactile sensor and/or as a visual camera depending on its internal lighting conditions.",
              "Piezoelectricity is a phenomenon exhibited by noncentrosymmetric crystals whereby an electric polarization (i.e. charge) is induced in the material upon the application of a stress. Conversely, it is the development of an induced strain which is directly proportional to an applied electric (r)eld. The latter phenomenon is known as the converse effect and is utilized in actuation. The former is called the direct effect and is used in sensing dynamic pressure changes, changes in acceleration (from shock or vibration), and changes in force.",
              "triboelectric nanogenerators (TENG) that can either harvest biomechanical energy or be utilized as a self-powered tactile sensor system for touch pad technology.",
              "Piezoelectric tactile sensors offer high sensitivity and fast response times for robotic applications.",
              "Piezoelectric tactile sensors have emerged as a significant technology in robotic sensing due to their unique properties and capabilities. These sensors operate on the piezoelectric effect, where certain materials generate an electrical charge in response to applied mechanical stress [LLM MEMORY | 2024]. This principle allows for highly sensitive and rapid detection of forces and pressures, making them particularly suitable for dynamic tactile sensing in robotics.",
              "Triboelectric nanogenerator (TENG) based tactile sensors have emerged as a promising technology for robotic applications, offering unique advantages in self-powered operation and sensitivity. These sensors operate on the principle of contact electrification and electrostatic induction, generating electrical signals in response to mechanical stimuli [LLM MEMORY | 2024]."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should present advantages of visiotactile sensors for robots.",
            "weight": 0.08571428571428572,
            "evidence": [
              "a novel sensing technology that utilizes an integrated tactile-vision sensor to perform object recognition and metrology tasks.",
              "In robotic manipulation, a coordinated eye-and-hand feedback system permits accurately tracking the manipulated object and regulating the applied contact forces.",
              "Visuotactile sensors can also improve the accuracy of object pose estimation. The estimation of the pose of an object in hand is one of the challenging topics in the field of robotics. Since the fingers of the robot will block the object when the gripper grasps it, it is difficult to estimate the object's pose accurately by vision. However, the application of visuotactile sensors further promotes the development of in-hand pose estimation of objects.",
              "Grasping [162]-[164] is a basic and important function of robots, which can be widely used in garbage sorting, assembly line handling, and home service. Most of the current grasping tasks are done by vision, but for some low-visibility environments with low light or smoke, it is difficult to achieve object detection relying only on visual perception, and the development of visuotactile perception technology has given a great impetus to improve the application range of robots.",
              "The application of visuotactile sensors allows robots better adapted to complex manipulation tasks safely and reduce decision errors."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide disadvantages of visiotactile sensors for robots.",
            "weight": 0.08571428571428572,
            "evidence": [
              "When the visuotactile sensor contacts an object, the sensing skin's color and texture will change. Extracting information about the contact location and area can further improve the stability and success rate of the robot in grasping the object."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The asnwer should provid advantages of piezoelectric sensors in robots.",
            "weight": 0.08571428571428572,
            "evidence": [
              "Another advantage of this sensor over the rigid substrate PVDF tactile sensor is its high sensitivity. Because the film is used as a membrane, the output charge is the sum of the charge due to piezoelectric constantsd31andd32, while in rigid substrate tactile sensors, it is effectively due tod33. The output charge depends also on the applied force on the sensing element.",
              "One of the key advantages of piezoelectric tactile sensors is their excellent sensitivity to small forces and vibrations. This high sensitivity enables robots to detect subtle changes in their environment, which is crucial for tasks requiring fine manipulation or delicate interactions [LLM MEMORY | 2024]. Additionally, piezoelectric sensors boast remarkably fast response times, allowing for real-time feedback in robotic systems. This rapid response is essential for applications where quick adjustments or reactions are necessary, such as in collision avoidance or object grasping tasks."
            ]
          },
          {
            "name": "most_important_item_4",
            "criterion": "The answer should present the disadvantages associated with piezoelectric sensors in robots.",
            "weight": 0.08571428571428572,
            "evidence": [
              "To improve the tactile perception of robots, tremendous sensors have been designed based on different mechanisms, such as piezoelectric [21], [22], triboelectric [23], [24], and piezoresistive [25]-[27] sensors. Nevertheless, these sensors are limited by the complicated fabrication process and the expensive data acquisition circuits, and it is challenging to achieve highresolution and large-scale tactile perception in a cost-efficient way.",
              "The historically persistent stability and response reproducibility limitation associated with piezoelectric-based tactile sensors has been solved by implementing a novel pre-charge voltage bias technique to initialize the pre- and post-load sensor responses.",
              "Another challenge associated with piezoelectric sensors is their inability to measure static forces effectively. These sensors excel at detecting dynamic changes in force but struggle to maintain a consistent output under constant pressure. This characteristic can be a limitation in scenarios where robots need to assess and maintain steady contact forces over extended periods [LLM MEMORY | 2024]."
            ]
          },
          {
            "name": "most_important_item_5",
            "criterion": "The asnwer should provid advantages of triboelectric tactile sensors in robots.",
            "weight": 0.08571428571428572,
            "evidence": [
              "The triboelectric signal relates strongly to the specific charge condition of the surface material of a target object, while the inductive signal manifests the electromagnetic characteristics at a certain depth inside the object. With the help of machine learning, the triboelectric signals and inductive signals can be used for object identification.",
              "One of the primary advantages of TENG-based tactile sensors is their ability to function without an external power source. This self-powered characteristic makes them particularly attractive for use in autonomous robots and wearable devices, where energy efficiency is crucial [LLM MEMORY | 2024]. Additionally, these sensors exhibit high sensitivity to a wide range of pressures and forces, enabling robots to detect and respond to subtle environmental changes."
            ]
          },
          {
            "name": "most_important_item_6",
            "criterion": "The answer should present the disadvantages associated with triboelectric tactile sensors in robots.",
            "weight": 0.08571428571428572,
            "evidence": [
              "To improve the tactile perception of robots, tremendous sensors have been designed based on different mechanisms, such as piezoelectric [21], [22], triboelectric [23], [24], and piezoresistive [25]-[27] sensors. Nevertheless, these sensors are limited by the complicated fabrication process and the expensive data acquisition circuits, and it is challenging to achieve highresolution and large-scale tactile perception in a cost-efficient way.",
              "However, TENG-based tactile sensors also face several challenges. Durability remains a concern, as repeated mechanical stress can degrade the sensor's performance over time [LLM MEMORY | 2024]. Environmental factors such as humidity and temperature can also affect the triboelectric effect, potentially leading to inconsistent readings in varying conditions.",
              "Another limitation is the susceptibility of TENG sensors to electromagnetic interference, which can impact their accuracy in certain environments [LLM MEMORY | 2024]. This issue may require additional shielding or signal processing techniques in robotic applications where electromagnetic noise is prevalent."
            ]
          }
        ]
      }
    },
    "case_id": "71b0b99a7954bf3dadd8cfd30ece359e",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "what kinds of challenges arise when trying to transfer robotics systems trained/designed in simulation to the real world?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "what kinds of challenges arise when trying to transfer robotics systems trained/designed in simulation to the real world?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should provide a background on robotic systems.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Robotic systems are increasingly a part of everyday life. Characteristics of robotic systems such as interaction with the physical world, and integration of hardware and software components, differentiate robotic systems from conventional software systems."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide challenges which arise when trying to transfer robotics systems trained/designed in simulation to the real world.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Unpredictable corner cases: Robots are typically expected to operate in many different environments and conditions. In most cases, the robotic state space is infinite, since it interacts with the real world; predicting the exact behavior of the physical environment is not viable [34], [35]. Attempting to account for all possible conditions and scenarios when designing tests is extremely difficult, if not impossible.",
              "The challenge of attempting to anticipate and cover for all possible edge cases within a large (and possibly unbounded) state space when designing tests.",
              "The cost of running and automating the tests in terms of human hours, resources and setup, and running time.",
              "Environmental complexity: The intended operating environment for a robot can be very complex: robots are embodied within the unpredictable and practically boundless state of space of reality, and their behavior may be dependent upon certain physical features (e.g., terrain) and phenomena (e.g., lighting, weather). Finding a suitable environment for testing the robot under expected operating conditions (e.g., on Mars or in the deep ocean) can be challenging",
              "Because of the noisy and non-deterministic nature of robotic systems, it is difficult to discretely specify the exact behavior that is intended.",
              "Even though in some situations robotic grasping is reliable and functions correctly, it still remains a fundamental challenge in industrial robotics. Several methods on the grasping of unknown and geometrically complex objects are proposed; however, a precise placing of such objects has not been widely addressed. In some scenarios, the grasping process can include several additional operations, which is the case of tangled objects, where proposed methods only partially address this issue by mostly avoiding grasping such kinds of objects.",
              "In the context of the reviewed smart industrial robot control methods, the challenges are highly connected to differences between simulations and real-life, which therefore usually decreases the precision of the system when deployed in real-life scenarios. Physic simulations, object virtual representations, sensor data recreation, artificial lightning and other countless bits of real environment all contribute to the issue of transferring learned models from simulation to reality.",
              "Due to specific elements of smart manufacturing and precision requirements, a different situation can be seen in this field. The product variety is changing more quickly and algorithms have to be trained on new data sets. Therefore, the re-usability of existing data sets is fairly low. Manual labeling methods cannot meet the requirements of agile production as it is time-consuming, expensive and usually requires expert knowledge of the specific field, and thus, human error can arise in the labeling process.",
              "The primary issue is the \"reality gap,\" which refers to the differences between the simulated environment and the real world. Factors like gravity, friction, and material properties can be approximated but not perfectly replicated in simulators.",
              "Robotic actuators in the real world may exhibit discrepancies such as backlash, wear and tear, and non-linearities that are not accounted for in simulations. These differences can affect the performance and reliability of the robot.",
              "Real-world environments contain unmodeled dynamics such as air resistance, temperature variations, and unexpected interactions.",
              "Sensors in the real world can experience calibration drift over time, degrading performance. In simulation, sensor data is usually noise-free and perfectly calibrated, which is rarely the case in real-world scenarios.",
              "In simulation, sensors often have perfect range and resolution. In the real world, sensors may suffer from limited range, resolution, and field-of-view, along with occlusions and line-of-sight issues."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should start with providing the applications of trained of robotic systems.",
            "weight": 0.09999999999999999,
            "evidence": [
              "Although robotics was started as a form of entertainment, it gradually became used in different branches of science. Medicine, particularly in the operating room, has been influenced significantly by this field. Robotic technologies have offered valuable enhancements to medical or surgical processes through improved precision, stability and dexterity.",
              "The systems include the AcuBot for active needle insertion under CT or fluoroscopy, the B-Rob systems for needle placement using CT or ultrasound, the INNOMOTION for MRI and CT interventions, and the MRBot for MRI procedures.",
              "Robotics evolved as a central issue in teaching for scientific and engineering courses and inherently encompasses a spectrum of sciences and technologies and qualification levels. However, most current teaching approaches, related to robotics, concentrate on individual aspects or small student groups and do not involve dangerous situations. With a mixed-reality robotics teaching, a true interdisciplinary setup can be reached, this approach has been used in a robotics course at higher education, to prepare students for future activities in industry and also in research.",
              "In order to simplify the effort towards data collection by reducing the human involvement to a minimum and to reduce power requirement of the sensors for data transmission, thereby increasing their life, use of marine robotic systems has been experimented with successfully."
            ]
          },
          {
            "name": "nice_to_have_item_1",
            "criterion": "The answer should provide challenges involved in robotic system in real world using reenformcement learning.",
            "weight": 0.09999999999999999,
            "evidence": [
              "The second challenge to reliable and stable learning is local optima and delayed rewards. In contrast to supervised learning problems, which put a convex loss function on top of a nonlinear neural network function approximator, the RL objective itself can present a challenging optimization landscape independently of the policy or value function parameterization, which means that the usual benefits of over-parameterized networks do not fully resolve issues relating to local optima. This is indeed part of the reason why different runs of the same algorithm can produce drastically different solutions, and it presents a major challenge for real-world deployment, where even a single run can be exceptionally time-consuming.",
              "Deep RL algorithms are notoriously difficult to use in practice (Irpan, 2018). The performance of commonly used RL methods depends on careful settings of the hyperparameters, and often varies substantially between runs (i.e., for different \"random seeds\" in simulation). Off-policy algorithms, which are particularly desirable in robotics owing to their improved sample efficiency, can suffer even more from these issues than on-policy policy gradient methods. We can broadly classify the challenges of reliable and stable learning into two groups: (1) reducing sensitivity to hyperparameters; and (2) reducing issues owing to local optima and delayed rewards."
            ]
          }
        ]
      }
    },
    "case_id": "ad5d8d8dcdab789fbc9231709db8c9cb",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "is there any evidence that large language models can be effectively applied to robot planning tasks?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "is there any evidence that large language models can be effectively applied to robot planning tasks?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should provide a background information in using large language models in robot planning tasks.",
            "weight": 0.17142857142857143,
            "evidence": [
              "The use of large language models (LLMs) in robot planning tasks has attracted significant interest in recent years. Given the capabilities of LLMs in understanding and generating human-like text, researchers have explored their applications in various domains, including robotic planning.",
              "There is indeed growing evidence that large language models (LLMs) can be effectively applied to robot planning tasks. This emerging field of research combines the natural language processing capabilities of LLMs with robotics to enhance planning, reasoning, and decision-making in robotic systems. Let's explore this topic in detail."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide evidence from research justifying that large language models can be effectively applied to robot planning tasks.",
            "weight": 0.17142857142857143,
            "evidence": [
              " Through visual language processing systems, robots are capable of understanding image content and integrating it with relevant linguistic information, such as image descriptions and command execution. This multimodal information processing is similarly applied in audio-visual integration. Another major progress with LLMs is in human-robot interaction, facilitated by interactive learning processes that better align with human needs and preferences.",
              "In the realm of complex task reasoning and decision-making, robots empowered by LLMs have shown remarkable proficiency. These LLM-based robotic planning tasks have significantly transcended the realms of mere text generation and language comprehension. Recent research highlights the immense capabilities of Language Models in managing intricate tasks, engaging in logical reasoning, making informed decisions, and partaking in interactive learning.",
              "Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information.",
              "Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed.",
              "SayPlan is a natural language-driven planning framework for robotics that integrates hierarchical 3D scene graphs and LLMs to plan across large-scale environments spanning multiple floors and rooms.",
              " we present a scalable approach to ground LLM-based task planners across environments spanning multiple rooms and floors. We achieve this by exploiting the growing body of 3D scene graph (3DSG) research.",
              "Smart Multi-Agent Robot Task Planning using Large Language Models (LLMs), harnesses the power of LLMs to convert high-level task instructions provided as input into a multi-robot task plan. It accomplishes this by executing a series of stages, including task decomposition, coalition formation, and task allocation, all guided by programmatic LLM prompts within the few-shot prompting paradigm.",
              "One of the key pieces of research in this area is Google's \"SayCan\" [1]. SayCan integrates large language models with robot planning by using natural language inputs to guide a robot to execute tasks. The LLM, in this case, processes natural language commands and predicts a sequence of actions that the robot needs to perform. A reinforcement learning (RL) model then verifies the feasibility and safety of these actions.",
              "Another important research project, CLIPort by OpenAI [2], combines vision and language by leveraging CLIP (Contrastive Language-Image Pretraining) and models like GPT-3 for robotic task planning. CLIPort demonstrated effectiveness in understanding instructions grounded in visual context, making significant strides in tasks requiring multimodal comprehension.",
              "Recent large language models (LLM) can embed rich semantic knowledge for agents in plan generation of complex tasks, while they lack the information about the realistic world and usually yield infeasible action sequences... Experimental results show that the generated plan from our TaPA framework can achieve higher success rate than LLaVA and GPT-3.5 by a sizable margin, which indicates the practicality of embodied task planning in general and complex environments."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide limitations of large language models when applied to robot planning tasks",
            "weight": 0.17142857142857143,
            "evidence": [
              ". Notably, in the realm of robot task planning, LLMs harness their advanced reasoning and language comprehension capabilities to formulate precise and efficient action plans based on natural language instructions. However, for embodied tasks, where robots interact with complex environments, text-only LLMs often face challenges due to a lack of compatibility with robotic visual perception.",
              "The generated plans are homogenous, lacking in detailed embodiment and specific, robust designs to manage complex environments and tasks.",
              " Current multimodal LLMs, such as GPT-4V and Google Gemini, necessitate carefully crafted, lengthy prompts to produce reliable outputs, which require domain expertise and extensive tricks.",
              "The robot is constrained by predefined actions, limiting its executional freedom and robustness.",
              "The closed-source nature of the GPT-4V API and associated time delays may impede embedded system development and real-time commercial applications.",
              "SayPlan is notably constrained by the limitations inherent in current large language models (LLMs), including biases and inaccuracies, affecting the validity of its generated plans. More specifically, SayPlan is limited by the graph-based reasoning capabilities of the underlying LLM which fails at simple distance-based reasoning, node count-based reasoning and node negation.",
              "One of the main challenges in using LLMs for robot planning is ensuring the safety and reliability of the generated plans. While LLMs can generate plausible instructions, they may not always be practical or safe in real-world scenarios. Ensuring robustness and validation through methods like reinforcement learning remains critical [3].",
              "Understanding why an LLM-generated a specific plan can be difficult, complicating debugging and refinement processes. Researchers are actively investigating ways to improve the interpretability of these models within robotic systems [4].",
              "While effective, LLMs and combined architectures involving them (like vision-language models) are computationally intensive. This constraint can limit their application in real-time or resource-constrained environments."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should provide the large language models used in effective robot task planning.",
            "weight": 0.08571428571428572,
            "evidence": [
              "Large language models, such as GPT-3, BERT, and their successors, have demonstrated remarkable capabilities in understanding and generating human-like text. Researchers have begun to explore how these models can be leveraged to improve robot planning tasks, which traditionally rely on specialized algorithms and domain-specific knowledge representations."
            ]
          }
        ]
      }
    },
    "case_id": "6453c86c72949978fb8db5405b48c923",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "what are some datasets used for evaluating variable type inference for decompiled binaries?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "what are some datasets used for evaluating variable type inference for decompiled binaries?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with providing a background on challenges in binary type inference.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Binary type inference is a challenging problem due partly to the fact that during the compilation much type-related information has been lost. Most existing research work resorts to program analysis techniques, which can be either too heavyweight to be viable in practice or too conservative to be able to infer types with high accuracy.",
              "Reverse-engineering program binaries often relies on the recovery of high-level data abstractions. In particular, recovering variables and their type is challenging as most such information is lost during compilation. Although past proposals seem to have addressed this problem, their approaches are either not scalable and suffer from coverage issues (e.g., dynamic analysis), or yield insufficient precision by staying too conservative (e.g., static analysis). Furthermore, most recent works lift assembly to Intermediate Representation (IR), which standardizes low-level operations, and may lose some useful semantics for type inference.",
              "Evaluating variable type inference for decompiled binaries typically relies on synthetic datasets or real-world programs. These datasets often include a variety of binary types and architectures to test the robustness of inference algorithms."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide the datasets used for evaluating variable type inference for decompiled binaries.",
            "weight": 0.19999999999999998,
            "evidence": [
              "we train a classifier with basic types as its levels via various machine-learning methods, based on binaries with debugging information compiled from a dataset of C programs. After the classifier is trained, we then can use it to learn the most possible types for the recovered variables.",
              ". Our dataset are collected from binaries which are complied by GCC and MSVC with none optimization on x86_64 platforms.",
              "another reason why BITY-Core performs better than BITY-Own on diffutils and findutils is that both the training dataset and the test dataset used by BITYCore are collected via GCC, while the training dataset used by BITY-Own is via MSVC and GCC but the test dataset is via GCC.",
              "For convenience, we collect the data of findutils and diffutils from the dataset of EKALVYA, which are obtained from GCC compiler without optimization and have eliminated the duplicated functions.",
              "BinType is 45% more precise than TIE (NDSS'11) on a dataset 3.5 times larger, and orders of magnitude faster than its underlying algorithm. We also show that our tool makes a significant impact on the accuracy of a recent tool on binary to source matching.",
              "We develop an application called CodeBinPlus by extending an existing proposal CodeBin, which is used to find the binary to source function similarities. The main challenge in identification of reused functions in a binary program is to find out common features within a binary and source code. As extended features in CodeBinPlus, we use the extracted function prototypes and string literals of BinType, and modify the score matrix corresponding to features.",
              "Particularly, we improve the performance of existing decompilers in predicting the types returned by functions in high-level programs. For that purpose, we instrument C source code to label binary patterns with high-level type information. That labeled information is then used to build predictive models. Moreover, the dataset created is used to document the binary patterns found by the classifiers and facilitate its inclusion in the implementation of any decompiler.",
              "After pattern generalization, the datasets are created before training the models (Table 1). Each cell in the dataset indicates the occurrence of each pattern (column) in every single function (row) in the program. RET patterns are associated with the function bodies, but POST CALL patterns are related to the invoked function. For example, if a function f is invoked in the body of a function g, the POST CALL pattern will be associated with the row representing the function f, not g.",
              "SPEC CPU BenchmarksThe Standard Performance Evaluation Corporation (SPEC) CPU benchmarks are widely used for evaluating various aspects of program analysis, including variable type inference [1].- SPEC CPU2006: This suite contains both integer and floating-point benchmarks written in C and C++.- SPEC CPU2017: An updated version of the benchmark suite with more modern applications.\"Debian Package RepositoryResearchers often use open-source software packages from the Debian repository to create custom datasets [2]. This approach allows for a diverse set of real-world programs to be analyzed.",
              "BEEN DatasetThe Binary Executable Enrichment and Normalization (BEEN) dataset, introduced by Katz et al. [3], is specifically designed for machine learning tasks on binary code, including type inference. It contains:- 6,000 binary functions- Compiled from C and C++ source code- Various optimization levels and compiler versions",
              "Debin DatasetThe Debin dataset, created by He et al. [4], focuses on debug information recovery for stripped binaries. While not exclusively for type inference, it includes type information and can be adapted for this purpose. It contains:- 3,000 Debian packages- Compiled for x86-64 architecture- Both stripped and unstripped versions of binaries",
              "TypeArmor Dataset\"Introduced by Veen et al. [5], this dataset was created to evaluate the TypeArmor system but can be used for general type inference evaluation. It includes:- Synthetic C++ programs- Various object-oriented patterns and inheritance structures",
              "Retypd DatasetNoonan et al. [6] created a dataset to evaluate their Retypd system, which performs type inference on stripped binaries. The dataset consists of:- Open-source programs- Custom-written test cases- Binaries compiled with different compilers and optimization levels",
              "Industrial Control Systems (ICS) BinariesFor evaluating type inference in specific domains, researchers have created datasets focusing on ICS binaries [7]. These datasets typically include:- Firmware from various ICS devices- SCADA (Supervisory Control and Data Acquisition) system binaries",
              "Malware DatasetsWhile not primarily for type inference, malware datasets can be used to evaluate type inference techniques in the context of malware analysis [8]. Examples include:- Microsoft Malware Classification Challenge dataset- VirusShare database"
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide information on the open-source projects which can be used to generate the datasets.",
            "weight": 0.19999999999999998,
            "evidence": [
              "The following urls can be used to generate the datasets by your own . Although they sum 2329 functions and 89,038 lines of code, they do not represent sufficient data to infer the types returned by functions. Unfortunately, there are not many open-source C (not C++) programs compilable with Microsoft cl compiler. The urls are (i) https://github.com/kimtg/arcadia (ii) https://github.com/elektrischermoench/bgrep (iii) https://web.archive.org/web/20150110171135/http://patrickomatic.com/c-ray-tracer (iv) https://github.com/akheron/jansson (v) https://github.com/jedisct1/libsodium (vi) https://lua.org/download.html g https://github.com/robertdavidgraham/masscan (vii)https://github.com/cesanta/slre"
            ]
          }
        ]
      }
    },
    "case_id": "7bb6a69f01d91674d5a925266e360b44",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "how do post-quantum cryptography algorithms such as kyber/dilithium compare to corresponding non-quantum-safe methods in terms of performance and energy cost, and how could this be affected by future hardware optimizations?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "how do post-quantum cryptography algorithms such as kyber/dilithium compare to corresponding non-quantum-safe methods in terms of performance and energy cost, and how could this be affected by future hardware optimizations?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with the advancements in post-quantum cryptography compared to the non-quantum.",
            "weight": 0.12,
            "evidence": [
              "Post Quantum cryptography is the field of cryptography where encryption algorithms are developed which are secure from an adversary with quantum computers. All 10 Short form of author list the present-day asymmetric algorithms(RSA, ECC, DH, DSA) are crackable using quantum computers. They are based on the Prime Factoring problem or the Discrete Logarithm Problem which are easy to solve on quantum computers using Shor's Algorithm.",
              "Post-quantum cryptography algorithms, such as Kyber and Dilithium, are designed to be resistant to quantum attacks, unlike traditional public-key cryptography methods like RSA and ECC. These algorithms are built upon hard problems that are believed to be resistant to quantum cryptanalysis."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide information on the performance of post-quantum cryptography compared to the non-quantum.",
            "weight": 0.12,
            "evidence": [
              "the packet protection is the primary contributor to the CPU time consumption on both endpoints. As expected, the results with an AES cipher show a lower percentage of samples for packet and header protection than those with a ChaCha20 cipher.",
              "Comparing the results for MsQuic with NOENC to the results with the NOOP cipher, we can see a further reduction of the collected samples for packet protection. This computational difference is the cost of the NOOP cipher, which still calls OpenSSL then executing memcpy().",
              "For the RSA-4096, the TTFB for the client and server rises to an extreme 14 ms, making it the slowest of the tested pre-quantum schemes. It was slower than all the postquantum schemes we looked at, except for SPHINCS+ . The SPHINCS+ variants reach by far the highest TTFB of all evaluated signature schemes, caused by the huge signature sizes.",
              " Kyber, Dilithium, and FALCON are promising candidates for post-quantum secure QUIC, as they have a low impact on the handshake duration. Algorithms like SPHINCS+ with larger key sizes or more complex calculations significantly impact the handshake duration and cause additional issues in our measurements.",
              " Our findings indicate that Dilithium offers advantages in low-power scenarios, Falcon excels in signature verification speed, and Sphincs+ provides robust security at the cost of computational efficiency.",
              "The authors of [20] listed the following advantages of Dilithium: * Significantly higher speed and significantly smaller size compared to hash-based schemes (e.g., the Sphincs+ scheme). * Simple implementation, as Gaussian sampling is not required.",
              "Kyber and Dilithium have higher computational complexity compared to traditional methods. This is due to the use of lattice-based cryptography, which involves matrix-vector operations and error correction mechanisms. These operations are also computationally intensive, especially for decapsulation, which involves error correction and polynomial arithmetic.",
              "Key generation in RSA and ECC is relatively fast and efficient, relying on the difficulty of integer factorization and discrete logarithm problems.\"Post-quantum cryptography (PQC) algorithms generally require more computational resources and memory than traditional methods. However, some PQC algorithms, particularly lattice-based ones, show promising performance that can be comparable or even superior to conventional cryptography in certain aspects."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide information on the energy cost of post-quantum cryptography compared to the non-quantum.",
            "weight": 0.12,
            "evidence": [
              "Post-quantum algorithms like Kyber and Dilithium generally consume more energy due to the increased computational complexity. However, some studies have shown that optimized hardware implementations can make them equally or more energy-efficient compared to traditional methods.",
              " Traditional methods like RSA and ECC are generally more energy-efficient due to their simpler arithmetic operations. However, they are vulnerable to quantum attacks, making them less secure in the long run.",
              "Post-quantum cryptography algorithms generally consume more energy than traditional methods, but the extent varies widely depending on the specific algorithm and implementation. Hardware optimizations and co-design approaches can significantly reduce energy consumption for some PQC algorithms.",
              "Post-quantum cryptography (PQC) algorithms typically consume more energy than their classical counterparts, which is a critical consideration for resource-constrained devices and energy-efficient systems. The energy consumption of PQC algorithms varies significantly depending on the specific algorithm, implementation, and hardware platform."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should provide information on how could post-quantum cryptography algorithms such as kyber/dilithium compare to corresponding non-quantum-safe methods be affected by future hardware optimizations",
            "weight": 0.12,
            "evidence": [
              "1. **Hardware Acceleration**: Implementing post-quantum algorithms on specialized hardware, such as FPGAs, can significantly improve performance and reduce energy consumption. High-Level Synthesis (HLS) can be used to optimize these implementations.2. **Masked Implementations**: Techniques like masking can be used to protect against side-channel attacks, which are crucial for secure implementations of post-quantum algorithms.3. **Optimized Architectures**: Designing flexible and scalable hardware architectures can help in achieving better performance and energy efficiency trade-offs for post-quantum algorithms."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer hsould provide advantages of post-quantum cryptpgraphy with repsect to the non-quantum safe methods.",
            "weight": 0.06,
            "evidence": [
              "The potential of quantum computing is continuously enhanced by exploring hardwarespecific quantum algorithms, advancing the theoretical framework for quantum heuristics [1,2], and by contributing to a broad array of other applications that drive the field's overall development."
            ]
          },
          {
            "name": "nice_to_have_item_1",
            "criterion": "The answer should discuss various algorithms used in cryptography related to quantum.",
            "weight": 0.06,
            "evidence": [
              "Of the algorithms chosen, Crystals-Dilithium, Falcon, and Sphincs+ pertain to digital signatures. Crystals-Dilithium and Falcon are lattice-based schemes, with considerations for both depending on the context of application--Dilithium is easier to implement, while Falcon yields shorter signatures [9]. Sphincs+, although slower and larger, is significant because it is based on a different mathematical approach, i.e., hash functions.",
              "Dilithium is a digital signature scheme that is highly secure against chosen-message attacks, based on the hardness of lattice problems over module lattices [14]. The design of Dilithium is grounded in Lyubashevsky's \"Fiat-Shamir with Aborts\" technique [15,16], which employs sample rejection to make lattice-based Fiat-Shamir schemes compact and secure."
            ]
          }
        ]
      }
    },
    "case_id": "3a19520deab02250c7384cfbc79fcbff",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "how can software engineering process improvements be effectively measured in real-world settings?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "how can software engineering process improvements be effectively measured in real-world settings?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with the explanation of software engineering process improvements.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Studies have been conducted in Software process improvement (SPI) for decades. These studies discussed different aspects related to SPI like small and medium enterprise (SME), critical success factors, and gamification. Some researchers conjectured that typical SPI is progressively put into the backdrop.",
              "SPI practices are oriented toward total process improvement rather than the final product. The assumption is that well-defined and clearly documented processes will eventually result in quality products.",
              "Over the last two decades, the Software Engineering community has expressed special interest in software process improvement (SPI) in an effort to increase software product quality, as well as the productivity of software development.",
              "During the last decade, the software industry has been more and more concerned about software process improvement (SPI). Consequently, we have witnessed a proliferation of models and initiatives all claiming to increase the likelihood of succeeding with SPI initiatives.",
              "Software Process Improvement (SPI) methodology is defined as definitions of sequence of tasks, tools and techniques to be performed to plan and implement improvement activities. Well-known SPI frameworks like CMMI and ISO/IEC 15504 define SPI methodologies in an abstract manner.",
              "Software Process Improvement (SPI) focuses on improving the time, cost and quality of engineering and management practices in software organizations. SPI initiatives in software organizations are frequently performed based on well defined reference models such as CMMI and ISO 15504."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide the methods to effectively measure software engineering process improvements in real-world settings.",
            "weight": 0.19999999999999998,
            "evidence": [
              "ArSPI proposes a two-staged design process (reflected by the artefacts conceptual process design and technical process design) to separate conceptualisation and (technical) realisation. However, as SPI projects can be performed on a small scale, conceptual-and technical process design can be integrated into a unified process design artefact.",
              "As the ArSPI model precisely defines the artefacts, their structure and the dependencies, process engineers get a set of consistent and complete artefacts for planning and monitoring the SPI projects while preserving the necessary degree of flexibility in the way of working.",
              " The backbone of the evaluation strategy is a combination of different empirical instruments applied in academic (internal) as well as in industry-hosted (external) settings. The internal validation aims at creating settings to (1) validate ArSPI in controlled environments, (2) to analyse the model's consistency and completeness and to (3) develop/refine the instruments to be used in the external validation.",
              "The external validation aims at providing insights into practical settings regarding benefits and shortcomings to prepare dissemination and further investigations.",
              "Experts consider ArSPI useful, as, for instance, it helps to structure SPI projects, and to reflect on SPI activities [14]. A major finding was the flexibility of the ArSPI model that allows for tailoring and applying ArSPI in different contexts, for example, large and small, and short- and long-term SPI projects/programmes.",
              "The major premise was that it is critical to understand the important factors affecting SPI success in order to improve software processes. However, central to this understanding is the development of an instrument for measuring these factors.",
              "1. **Automated Code Review and Testing**: Implementing automated code review tools and continuous integration/continuous delivery (CI/CD) pipelines can help catch issues early, reduce manual effort, and improve overall quality.2. **Incident Retrospectives**: Conducting retrospectives after incidents helps teams understand the root causes and implement changes to prevent similar incidents in the future.3. **Breaking Down User Stories**: Breaking down user stories into smaller, manageable chunks can help speed up development and deployment, leading to faster time-to-market and higher productivity.4. **Continuous Improvement**: Regularly reviewing and refining processes based on data and feedback helps to identify inefficiencies and implement improvements.5. **Software Process Improvement (SPI) Methodology**: SPI involves a structured approach to identifying inefficiencies, setting goals, and implementing changes to achieve those goals. It helps to align process improvements with organizational objectives.",
              "Before implementing any improvements, establish baseline metrics. This historical data serves as a comparison point to evaluate the impact of process changes.",
              "Regularly monitor metrics and establish feedback mechanisms, such as retrospectives and after-action reviews, to identify areas of improvement and take timely corrective actions.",
              "It's important to note that while a comprehensive set of metrics can provide valuable insights, some experts recommend limiting the number of measures to reduce the complexity of collecting, analyzing, and maintaining performance data  (18, Paulish et al., 1994). The selection of appropriate KPIs and metrics should align with the organization's specific goals and context, enabling objective and quantitative evaluation of software processes  (178, Springer et al., 2020). These measurements provide organizations with the objective information needed to make informed decisions that impact their business performance  (102, Rocha et al., 2010)."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide challenges associated with software engineering process improvements.",
            "weight": 0.19999999999999998,
            "evidence": [
              "The SPI literature is full of case studies and anecdotal evidence of successful companies and descriptions of their SPI programs. Several authors repeatedly discuss the importance of certain critical success factors. However, there has been no systematic attempt to synthesize and organize the prescriptions offered. The research efforts to date are limited and inconclusive and without adequate theoretical and psychometric justification.",
              "1. **Complexity of Measuring Software Development Efficiency**: Measuring software development efficiency can be challenging due to the complexity of software projects and the various factors that influence productivity.2. **Ambiguity in Evaluating SPI Outcomes**: Evaluating the outcomes of SPI initiatives can be ambiguous due to the complexity of causal relationships and the need to consider multiple stakeholders' perspectives.3. **Need for Effective Measurement Frameworks**: Developing and implementing effective measurement frameworks is crucial to ensure that the correct metrics are used to evaluate process improvements."
            ]
          }
        ]
      }
    },
    "case_id": "57f1dc7fb4c25e0cceb2ae6e8653b795",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "what are the best recent techniques for text watermarking, what kinds of transformations are they robust to, and what transformations or attacks are they not robust against",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "what are the best recent techniques for text watermarking, what kinds of transformations are they robust to, and what transformations or attacks are they not robust against",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with providing the recent techniques for text watermarking.",
            "weight": 0.19999999999999998,
            "evidence": [
              "According to software based watermarking system, spatial and transform-domain approaches are used to embed watermarks.",
              "In the spatial domain based watermarking techniques, the pixel, code value or bit stream of the cover image are altered directly in order to embed the watermark media. Least Significant Bit (LSB), spread spectrum, correlation based techniques and patchwork methods are some of the techniques under this domain. InLSB, the watermark is hidden by replacing the least significant bit of the binary representation of the cover pixels.",
              "Patchworkis another spatial domain technique chooses the positions of pixels by generating pseudo-random generated numbers.Spread spectrumhelps in distributing the data in the frequency bins in such a way that the energy of each bin is very less and hence not predictable",
              "According to the type of embedding and extraction procedure, watermarking technique can be considered as blind, semi-blind or non-blind.",
              "DCT transforms the image into three types: low, middle and high frequency coefficients. It offers excellent energy compaction property, which makes DCT more suitable for watermarking. Generally, middle frequency coefficients of DCT are the best choice for the embedding of watermark",
              "The DWT is used to decompose the image into four non-overlapping sub-bands: approximation sub-band (LL), horizontal sub-band (LH), vertical sub-band (HL) and diagonal sub-band (HH). The decomposition can be implemented as multistage transform. It is important to notice that 'LL' sub-band is more sensitive to human eye, in the sense that any operation in this sub-band may be noticeable to eye.",
              "There are many other transform based techniques used in watermarking. LWT, IWT, Curvelet Transform and many more are used in recent watermarking techniques.",
              "ANiTW which utilizes an instance-based learning algorithm to hide an invisible watermark into Latin text-based information such that the hidden watermark can be extracted, even if a malicious user manipulates a portion of the watermarked information.",
              "In [80] a blind watermarking technique to achieve robustness is proposed which uses CNN technique. In this 4000 24-bit color images from the BOSSBase dataset are used for training the model and for learning model have taken 1 day. This learning process includes watermark embedding, simulation of various attacks on it and then updating the weights of the network.",
              "Natural Language Watermarking:This technique involves modifying the text's syntactic and semantic structure while preserving its meaning.",
              "GAN-based watermarking: Using Generative Adversarial Networks to embed and extract watermarks [4].- Transformer-based watermarking: Utilizing transformer models like BERT for watermark embedding and extraction",
              "Blockchain-based Watermarking:Integrating blockchain technology with text watermarking for enhanced security and traceability [6].",
              "Zero-watermarking techniques have emerged as a non-intrusive method for text protection. These approaches embed watermarks logically without altering the plain text, as demonstrated by the INLPETWA method, which integrates zero text watermarking with hidden Markov models (50, Al-Wesabi et al., 2020)."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide the information about robustness and reliablity of text watermarking.",
            "weight": 0.19999999999999998,
            "evidence": [
              " One of the main characteristics of watermarking is security that is to resist the integrity and confidentiality of the watermark against various attacks performed by some unauthorized persons. An attacker should not be able to detect the watermark, which is called unauthorized detection, he should not be able to change it, called unauthorized modification or should not be able to embed some other data in the watermarked image, called unauthorized embedding."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide transformations are they robust to and to which they are not.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Spatial domain techniques have less computational cost while transform domain have high computational cost but the robustness provided by spatial domain is very less in comparison to the transform domain techniques.https://link.springer.com/article/10.1007/s11042-020-09262-1\"Spatial domain techniques provide very good imperceptibility and can be applied in data hiding or copyright applications but these do not resist against attacks because of that researchers move towards the transform domain techniques.",
              "Transform based techniques can be applied to provide imperceptibility, robustness and temper recognition applications.",
              "DCT based techniques are very imperceptible and robust but these do not work well against cropping and scaling attacks and these are difficult to implement. Most of the researchers have used middle frequency components of DCT to embed the watermark as embedding in low frequency components introduce distortions but provides good robustness while embedding in high components decreases the quality of the image and increases robustness. So embedding in these two frequency components does not provide a balance between robustness and imperceptibility.",
              "DFT has a problem of shift invariance because of that some researchers has used QDFT based scheme to provide more robustness against geometric attacks.",
              "3. [(66, Pan et al., 2023) | n_citations: 20 | A Semantic Invariant Robust Watermark for Large Language Models ]:However, current watermarking algorithms cannot possess both attack robustness (robustness to modifications of the watermarked text) and security robustness... we propose a semantic invariant watermarking method for LLMs that provides both attack robustness and security robustness. The watermark logits in our work are determined by the semantics of all preceding tokens.",
              "a robust approach INLPETWA (an Intelligent Natural Language Processing and English Text Watermarking Approach) is proposed to tampering detection of English text by integrating zero text watermarking and hidden Markov model... embedding and detecting the watermark key logically conducted without altering the plain text.",
              "Some watermarking techniques have shown specific strengths against certain types of attacks. For example, the Exponential and GumbelSoft watermarks have demonstrated particular robustness against the T5-span attack  (85, Yang et al., 2024). Additionally, techniques based on synonymy substitution and majority voting have shown improved robustness against distortions caused by printing, phototypesetting, scanning, faxing, and re-typing  (4, Liu et al., 2009)  (13, Chuan, 2010)."
            ]
          }
        ]
      }
    },
    "case_id": "6e7e3524d565599b3064ae05375956f7",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "what datasets are commonly used to evaluate SLAM methods",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "what datasets are commonly used to evaluate SLAM methods",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with providing background on SLAM models.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Simultaneous Localization and Mapping (SLAM) is a key problem in the field of Artificial Intelligence and mobile robotics that addresses the problem of localization and mapping when a prior map of the workspace is not accessible.",
              "In SLAM, on the other hand, the classic problem is to estimate the motion of a moving robot in real-time as it continuously observes and maps its unknown environment with sensors which may or may not include cameras."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide datasets used to evaluate SLAM methods",
            "weight": 0.19999999999999998,
            "evidence": [
              "Based on the techniques mentioned above, we introduce\"Tongji Computer Graphic\" (TCG) dataset, asynthetic datasetfor VSLAM evaluation. This dataset contains multiple sequences in different indoor environments with more than 15k images in total. Most scene models are modified from high precision undergroundgaragemodels. All the scene models used for rendering the sequences are built in Autodesk 3DS-MAX2and images are rendered by V-Ray3industrial off-line renderer",
              "Another usage for this dataset is that, by usingsynthetic datagenerated by easy-to-use 3D modeling software like 3DS-MAX, every developer can validate claimed performance of a VSLAM algorithm in particular environments, without needs for financial resources or professional knowledges to setup and use expensive sensors.",
              "These datasets are adopted to run and to test SLAM methods. Given the difficulty to record a dataset, software tools such as the Underwater Simulator (UWSim) [5], one of the most adopted underwater simulator, are developed to simulate an underwater intervention mission and to reproduce real missions from the captured logs.",
              "We recorded a large set of image sequences from a Microsoft Kinect with highly accurate and time-synchronized ground truth camera poses from a motion capture system. The sequences contain both the color and depth images in full sensor resolution (640 x 480) at video frame rate (30 Hz). The ground-truth trajectory was obtained from a motion-capture system with eight high-speed tracking cameras (100 Hz). The dataset consists of 39 sequences that were recorded in an office environment and an industrial hall.",
              "We present a challenging dataset, the TartanAir, for robot navigation tasks and more. The data is collected in photo-realistic simulation environments with the presence of moving objects, changing light and various weather conditions. By collecting data in simulations, we are able to obtain multi-modal sensor data and precise ground truth labels such as the stereo RGB image, depth image, segmentation, optical flow, camera poses, and LiDAR point cloud.",
              "We introduce M2DGR: a novel large-scale dataset collected by a ground robot with a full sensor-suite including six fish-eye and one sky-pointing RGB cameras, an infrared camera, an event camera, a Visual-Inertial Sensor (VI-sensor), an inertial measurement unit (IMU), a LiDAR, a consumer-grade Global Navigation Satellite System (GNSS) receiver and a GNSS-IMU navigation system with real-time kinematic (RTK) signals.",
              "**TUM RGB-D Dataset**: This dataset provides RGB-D data (color and depth images) collected using Microsoft Kinect sensors. The dataset includes sequences from both hand-held and robot-held sensors in indoor environments, along with ground truth for trajectory and object annotations.",
              "**KITTI Dataset**: The KITTI Vision Benchmark Suite provides datasets for mobile robotics and autonomous driving. It includes stereo camera data, GPS, IMU, and LiDAR data, as well as ground truth for evaluations.",
              "TUM VI Dataset**: This is designed for visual-inertial SLAM and uses both real-world data and synthetic data. The dataset offers high-resolution images coupled with IMU data, providing a benchmark for SLAM algorithms that leverage visual-inertial information.",
              " **EuRoC MAV Dataset**: This dataset contains sequences recorded by a micro aerial vehicle (MAV) equipped with synchronized stereo cameras and IMU sensors. It provides data from a variety of indoor environments, including industrial sites and machine halls.",
              "4Seasons Dataset: Includes data from different weather conditions, particularly useful for evaluating SLAM performance in rainy weather  (77, Prasetyo et al., 2023)."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should provide information of how the SLAM models are evaluated.",
            "weight": 0.09999999999999999,
            "evidence": [
              "every developer can validate claimed performance of a VSLAM algorithm in particular environments, without needs for financial resources or professional knowledges to setup and use expensive sensors.",
              "By substituting information estimated by VSLAM system into ground truth data from the dataset (e.g. using ground truth optical flow to replace 2D feature matching results), we can eliminate the effect of interaction among different modules, which helps us debug this algorithm.",
              "A new VSLAM evaluation method with detailed rating criteria for mapping block of VSLAM. The criteria are listed below:(a) Reconstruction Error (RE) is the average error of world positions of 3D features on keyframes across entire global map;(b) Reconstruction Error Standard Deviation (RE_STD) is the average standard deviation of Reconstruction Error (RE) across entire global map;(c) Cruciality of Features (CF) measures percentage of features lie on static area of the surroundings;(d) Significance of Features (SF) measures how well features sampled by VSLAM describe general shape of the objects and structures in the surroundings.(e) Outlier Removal Capability (ORC) measures VSLAM's capability to remove outliers sampled from moving objects."
            ]
          },
          {
            "name": "nice_to_have_item_1",
            "criterion": "The answer should provide selection criteria of the datasets.",
            "weight": 0.09999999999999999,
            "evidence": [
              "The choice of dataset depends on the specific SLAM application and research objectives. Researchers often use a combination of datasets to ensure robust evaluation across different scenarios and sensor configurations [7 | 83459364 | Labbe et al. | 2018 | Citations: 593]. As the field of SLAM continues to evolve, new datasets are being developed to address emerging challenges and applications in robotics and computer vision  (72, Maalouf et al., 2023)."
            ]
          }
        ]
      }
    },
    "case_id": "83360f179fbaf73e400858ee4e17eda2",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are some practically-relevant properties of k-node subgraphs that can be counted in faster than cubic time in the size of the input graph?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are some practically-relevant properties of k-node subgraphs that can be counted in faster than cubic time in the size of the input graph?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with expaining the k-node subgraphs.",
            "weight": 0.3,
            "evidence": [
              " K-node subgraphs include cliques, cycles, and paths of various sizes. Efficient counting algorithms exist for specific types and sizes of subgraphs, with performance varying based on graph properties and subgraph complexity.",
              "It's worth noting that as k increases, the computational complexity of counting k-node subgraphs grows rapidly, making the problem intractable for larger values of k in practice  (26, Jain et al., 2016). This has led to the development of approximation algorithms and specialized techniques for specific types of subgraphs or graph properties  (13, Charikar et al., 2010)."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "Answer should provide properties of k-node subgraphs that can be counted in faster than cubic time in the size of the input graph.",
            "weight": 0.3,
            "evidence": [
              "Given a minimization graph problem P, findkvertices such that the optimum value of P is the highest amongst allk-node subsets. One simple example forREMOTE-P is computing the graph diameter where Pis the shortest path problem and k=2.",
              "we describe greedy algorithms forREMOTE-P that perform well while computing only a small amount of shortest paths.",
              "The best known approximation guarantees for this problem were[?](min[?]{,/[?][?]})for both directed and undirected graphs, and[?](ln[?])for undirected graphs with[?]6[?]2, wherenis the number of nodes in the input graph.https://epubs.siam.org/doi/abs/10.1137/S0097539703435753\"For the problem defined, we demonstrate a factor 3 approximation algorithm with complexity O(kn) for a graph onnnodes. For the unweighted version of the the problem in a graph onmedges we describe a factor 2 approximation algorithm of greedy type, with complexity O(n+m). For trees and forests we present a polynomial time algorithm applicable to our problem and also to a problem seeking to maximize (minimize) the weight of a subtree onknodes.",
              "We present approximation algorithms for the k-edge-incident subgraph problem: For general unweighted graphs we introduce a greedy method for finding a maximum cardinality set of nodes W such that the number of edges with at least one endpoint in W is at most k. The cardinality of a set obtained by such greedy algorithm is proved to be at least 1/2 - o( 1) times the size of an optimal solution. For weighted graphs, we provide a 1/3-approximation algorithm of complexity O(kn). Both approximations are based on an idea of relaxing the problem to a Knapsack-like problem.",
              "We present a 6-approximation algorithm for the minimum-cost-node connected spanning subgraph problem, assuming that the number of nodes is at least3[?]([?]1)+. We apply a combinatorial preprocessing, based on the Frank--Tardos algorithm for-outconnectivity, to transform any input into an instance such that the iterative rounding method gives a 2-approximation guarantee.",
              "For undirected graphs andk=2, a (deterministic) parallel NC version of the heuristic finds a 2-node connected (or 2-edge connected) spanning subgraph whose size is within a factor of (1.5+) of minimum, where>0is a constant.",
              "Algorithms exist for finding and counting induced subgraphs of specific sizes. For instance, in intersection graphs of axis-aligned boxes, any k-vertex induced subgraph can be found in O(n0.429k+O(1)) time for constant k  (78, Chan, 2022).",
              "The current fastest algorithm [NP85] of counting Ka subgraphs runs in time O(n o a/3 ) on n-vertex graphs, where o denotes the matrix multiplication exponent... counting K 2,2 (i.e., 4-cycle) subgraphs in a graph on n vertices with m edges attracts particular interest: The current fastest counting algorithm runs in time O(n o ) or O(m 1.48 ) [AYZ97]",
              "Assuming that the pattern graph P is connected and admits a vertex balancer of size b, we present an algorithm that counts the occurrences of P in G in O ((2 Delta-2)^{(k+b)/2} 2^{-b} n/(Delta) k^2 log n) time... A corollary of our main result is that we can count the number of k-vertex paths in an n-vertex graph in O((2 Delta-2)^{floor[k/2]} n k^2 log n) time",
              "Clique counts reveal important properties about the structure of massive graphs, especially social networks... For larger cliques (even, say 6-cliques) the problem quickly becomes intractable because of combinatorial explosion... We present a new randomized algorithm that provably approximates the number of k-cliques, for any constant k.",
              "A classic result of Chiba and Nishizeki gives an O(n+ma k-2 ) algorithm for exact counting of k-cliques in graphs of arboricity at most a [CN85].",
              "Assuming that the pattern graph P is connected and admits a vertex balancer of size b, we present an algorithm that counts the occurrences of P in G in O ((2 Delta-2)^{(k+b)/2} 2^{-b} n/(Delta) k^2 log n) time... A corollary of our main result is that we can count the number of k-vertex paths in an n-vertex graph in O((2 Delta-2)^{floor[k/2]} n k^2 log n) time",
              "For directed acyclic graphs (DAGs), a novel tree-like decomposition called DAG-treewidth t(H) has been introduced. If a subgraph H has DAG-treewidth t(H) and the host graph G has degeneracy d, induced copies of H in G can be counted in time f(d,k) * O(n^t(H))  (37, Bressan, 2018)."
            ]
          }
        ]
      }
    },
    "case_id": "0dd4500b3ca1daf372dffb4378ee2c8f",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "The following paper focuses on a problem of counting the number of subgraphs in a graph that have certain properties.  What are the most significant downstream applications that either are or could be using these algorithms?  Concrete examples of the graph algorithms in use in real-world applications would be especially helpful.  https://arxiv.org/pdf/2407.06801",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "The following paper focuses on a problem of counting the number of subgraphs in a graph that have certain properties.  What are the most significant downstream applications that either are or could be using these algorithms?  Concrete examples of the graph algorithms in use in real-world applications would be especially helpful.  https://arxiv.org/pdf/2407.06801",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with providing insights into countig the number of subgraphs in a graph.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Computing subgraph frequencies is a fundamental task that lies at the core of several network analysis methodologies, such as network motifs and graphlet-based metrics, which have been widely used to categorize and compare networks from multiple domains.",
              "The problem of counting occurrences of query graphs in a large data graph, known as subgraph counting, is fundamental to several domains such as genomics and social network analysis."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide applications counting the number of subgraphs in a graph that have certain properties.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Color coding is a very general and powerful algorithmic technique for subgraph counting. Color coding has been shown to be effective in several applications, but scalable implementations are only known for the special case of tree queries (i.e. queries of treewidth one).",
              "efficient distributed implementation for color coding that goes beyond tree queries: ouralgorithm applies to any query graph of treewidth 2. Since tree queries can be solved in time linear in the size of the data graph, our contribution is the first step into the realm of color codingfor queries that require superlinear worst case running time. This superlinear complexity leads to significant load balancing problems on graphs with heavy tailed degree distributions.",
              "the proof is based on a martingale convergence theorem. the evolving random graph allows us to study both the random graph modelKn, p, by fixing attention to a fixed time, and the modelKn, N, by studying it at the random time it contains exactlyNedges.",
              "In contrast to existing subgraph extraction techniques which are based on a complete clustering of the graph nodes, the proposed algorithm takes into account the fact that not every participating node in the network needs to belong to a community. Another advantage is that the method does not require to specify the number of clusters",
              "Standard topological properties: We consider 16 different measures for every node, including indegree, outdegree, average indegree of successors, average outdegree of predecessors, assortativity, edge reciprocity, PageRank, and local number of triangles. In order to assign the considered microscopic measures to each graph, we compute the mean, the variance, the median, the 10-percentile and 90-percentile of them, for a total number of 81 features.",
              "we apply the color coding technique to determine approximate counts of non-induced occurrences of the sub graph in the original network. Color coding gives a fixed-parameter algorithm for this problem, using a dynamic programming-based counting approach. Our new contributions are a multilevel shared-memory parallelization of the counting scheme and several optimizations to reduce the memory footprint. We show that approximate counts can be obtained for templates with up to 12 vertices, on networks with up to millions of vertices and edges.",
              "Social media platforms can use subgraph counting algorithms to identify clusters of users with similar interests, facilitating friend recommendations and enhancing user engagement.",
              " Graph theory is used to optimize crew scheduling for airlines, ensuring the minimum number of crew members are needed to operate all flights.",
              "Subgraph counting can be applied to optimize the flow of goods from warehouses to stores, minimizing the number of trucks required.",
              "Subgraph counting algorithms can be used to identify patterns in large datasets, such as in bioinformatics for DNA analysis.",
              "These algorithms can help detect anomalies in networks, such as unusual patterns of behavior in social networks or unusual traffic patterns in transportation networks.",
              "These algorithms can be used to find the shortest paths in transportation networks, optimizing routes for delivery trucks or ride-sharing services.",
              "Subgraph counting can be used to analyze protein-protein interactions, helping to understand the structure and function of proteins.",
              "Subgraph counting algorithms can be used to optimize web crawlers, ensuring that all relevant web pages are indexed efficiently.",
              "These algorithms can be used to optimize peer-to-peer networks, such as those used in file-sharing applications.",
              "Graph-based algorithms for detecting dense subgraphs and anomalous patterns are powerful tools in fraud detection and cybersecurity. These methods have shown significant improvements in identifying fraudulent activities in social networks, financial transactions, and online platforms.",
              "Anomaly detection in tensor data, which can represent complex multi-dimensional relationships, has been applied to various domains including social media, Wikipedia, and network traffic analysis. The DENSESALERT algorithm has shown success in identifying anomalous patterns in real-world tensors, particularly those that might be missed by other methods  (66, Faloutsos et al., 2017).",
              "Subgraph analysis techniques are valuable in computer-aided design and engineering for circuit analysis, network topology optimization, and efficient system design. These methods enable the extraction of high-level modules from complex circuits and the creation of optimized network structures."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer shoould provide challenges involved in counting the subgraphs.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Counting subgraphs is, however, computationally very expensive, and there has been a large body of work on efficient algorithms and strategies to make subgraph counting feasible for larger subgraphs and networks.",
              "The problem of identifying dense subgraphs helps analyze graph structures and complex networks and it is known to be challenging. It bears some similarities with the problem of reordering/blocking matrices in sparse matrix techniques.",
              "In bioinformatics, sub graph counting is used to detect and characterize local structure (motifs) in protein interaction networks. Exhaustive enumeration and exact counting is extremely compute-intensive, with running time growing exponentially with the number of vertices in the template."
            ]
          }
        ]
      }
    },
    "case_id": "f7d0d99e1d158e6f89eee62b5aace89c",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "How have theories of distributed cognition informed the design of features in social computing systems?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "How have theories of distributed cognition informed the design of features in social computing systems?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with introducing distributed cognition.",
            "weight": 0.3,
            "evidence": [
              "DCog focuses on the socio-technical system, which usually (but not necessarily) includes individuals. DCog uses the same theoretical language for both people and artifacts. This common language has led others to critique the theory for assuming people are equated with artifacts in some way that denies their humanity.",
              ". As a cognitive theory, DCog is focused on the organization and operation of cognitive systems; that is, with the mechanisms that make up cognitive processes, which result in cognitive accomplishments. It recognizes that \"a process is not cognitive simply because it happens in a brain, nor is a process non-cognitive simply because it happens in the interactions among many brains\" (Hollan et al., in press).",
              "The theory of distributed cognition, like any cognitive theory, seeks to understand the organization of cognitive systems. Unlike traditional theories, however, it extends the reach of what is considered cognitive beyond the individual to encompass interactions between people and with resources and materials in the environment.",
              "Theories of distributed cognition have significantly influenced the design of features in social computing systems. Distributed cognition is a theoretical approach that emphasizes the interactions among people, artifacts, and both internal and external representations.",
              "Distributed cognition is a framework for studying cognition that emphasizes the coordination between individuals, artifacts, and the environment. It involves the embodiment of information, the coordination of enaction among embodied agents, and ecological contributions to a cognitive ecosystem."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide theories of distributed cognition informed the design of features in social computing systems",
            "weight": 0.3,
            "evidence": [
              "The DBO (Desires, Beliefs and Opportunities) theory of action proposed by analytical sociologists aims to provide an action-theoretical basis for building explanatory theories in sociology. Peter Hedstrom claims that the DBO theory is realistic because it does not make assumptions that are known to be false or seriously incompatible with the current scientific understanding about the nature of human action and cognition.",
              "However, when computers are integrated into the flow of classroom action, a qualitac.re transformation occurs regarding the ways teachers teach and students learn. Cognition ic technology-enhanced classrooms is distributed throughout the system, which includes teachers, students, tools, and various artifacts. In this context, the potential of computers is best exploited \"when used in conjunction with the students' intellectual resources, social resources, symbolic resources:. and otier pbysical resoLrces as they function together within a social, constructivist, learning environment\" (Steketee, 2006, p. 3).https://www.tandfonline.com/doi/pdf/10.1080/15391523.2008.10782508\"Narciss and Koerndle (this issue) examine the enactment of technologyenhanced social-constructive learning scenarios (TecSocCon-LS) in a seventhgrade classroom for foreign language learning. The authors seek to understand the value and limitations of distributed cognition as a heuristic framework for evaluating the viability ofTecSocCon-LS as a means for designing and creating technology-rich classrooms.",
              "Distributed cognition has also been used to study problem-solving and Bayesian reasoning. Research has shown that the use of external manipulable materials can improve performance and reduce cognitive bias.",
              " The theory of distributed cognition has been proposed as a new foundation for human-computer interaction research. It emphasizes the need to understand interactions among people and technologies in complex networked environments.",
              "Distributed cognition has emerged as a significant theoretical foundation in Human-Computer Interaction (HCI), offering a novel perspective on how humans interact with technology and each other.",
              "The theory posits that cognitive processes are distributed across individuals, artifacts, and environments, rather than being confined to individual minds. Hutchins et al. position distributed cognition as an alternative to information processing psychology, providing a more comprehensive foundation for HCI  (8, Hutchins et al., 2003)."
            ]
          }
        ]
      }
    },
    "case_id": "c6740fc5bd8daa6a8628ae279f07806a",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "How has literature discussed AI as a design material?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "How has literature discussed AI as a design material?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with exploring the related advancements in AI.",
            "weight": 0.24,
            "evidence": [
              "Advances in AI, combined synergistically with other technologies such as cognitive computing, Internet of Things, 3D (or even 4D) printing, advanced robotics, virtual and mixed reality, and human-machine interfaces are transforming what, where, and how products are designed, manufactured, assembled, distributed, serviced, and upgraded.https://asmedigitalcollection.asme.org/memagazineselect/article-abstract/139/10/38/380304/AI-and-the-Future-of-the-Machine\"Artificial Intelligence (AI) is increasingly being conceptualized not just as a tool or set of technologies but as a \"design material\" in both academic and practical fields. Researchers and designers are exploring how AI can be integrated into the design process, influencing creative endeavors ranging from architecture to interactive media. The idea is that AI can be manipulated, shaped, and incorporated into projects in ways akin to traditional materials like wood or metal, but with properties and affordances that are unique to its computational nature."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The asnwer should provide information on AI as a design material.",
            "weight": 0.24,
            "evidence": [
              "ML models trained from large material datasets that relate structure, properties and function at multiple hierarchical levels have offered new avenues for fast exploration of the design spaces. The performance of a ML-based materials design approach relies on the collection or generation of a large dataset that is properly preprocessed using the domain knowledge of materials science underlying chemical and physical concepts, and a suitable selection of the applied ML model.",
              "Wang and Sun leveraged RNNs and the concept of directed graph to address the issues on the linkages between multi-scale models of porous media using a recursive data-driven approach, where the databases generated from smaller-scale simulations are used to train RNN models at larger scales. They also implemented reinforcement learning to generate traction-separation laws for materials with heterogeneous microstructures.",
              "Capuano and Rimoli developed a new type of finite elements called \"smart elements\" in which ML models provide force predictions based on the elements' states, circumventing the computation of internal displacement field and the need for numerical iterations.175Chanet al.reported an unsupervised approach that combines techniques such as topology classification, image processing, and clustering algorithms to promptly identify and characterize microstructures, including grains in polycrystalline solids, voids in porous materials, and micellar distribution in complex solutions.",
              "In a recent work by Samaniegoet al., deep neural networks based on the variational form of the boundary value problems were implemented as solvers for partial differential equations (PDEs) in various solid mechanics problems, using a fundamental idea that the energy of the system to be minimized can be naturally treated as a loss function for the neural networks.",
              "Atom2Vec, an unsupervised ML program, reconstructed the periodic table of elements only in a few hours. Atom2Vec first learns to distinguish different atoms by analyzing the list of compounds in the online database. Then, we borrow the simple concept of natural language processing: the characteristics of a word can be derived from other words around it; chemical elements are clustered according to their chemical environment.",
              "Many of these algorithms help with the classification (e.g., logistic regression and support vector machines) and statistical regression analysis (e.g., decision trees and random forests, which are used in both classification and regression) of nanomaterials based on their categorical or continuous numerical characteristics such as pulmonary toxicity, cell-specific targeting, and nanomaterial grouping in the case of nanotoxicology.:https://onlinelibrary.wiley.com/doi/full/10.1002/aisy.202000084\"Generative AI technologies enable more rapid and parallel prototyping, potentially elevating design outcomes beyond what was previously limited by human capacity  (170, Zhang et al., 2024).",
              "Large language models like GPT, with domain-specific fine-tuning, can engage in designerly thinking to generate original and useful design concepts  (151, Luo, 2023).",
              "AI has been used in architecture to optimize building designs and adapt to environmental conditions. For instance, research by Braumann and Brell-Cokcan (2012) outlines how AI algorithms can assist in the design process by simulating structural performance and optimizing architectural forms. This pushes the boundaries of what is traditionally achievable with static materials.",
              "In the arts, AI's role as a design material has been explored in creating new forms of music, visual arts, and literature. The work by McCormack et al. (2019) investigates how AI can be seen not just as an assistant but as a co-creator, contributing to the artwork in ways that are unpredictable and collaborative."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should provide the available datasets which can be used for training purposes using AI for material design.",
            "weight": 0.12,
            "evidence": [
              "The advent of high-throughput computational materials design leads to the construction of many materials databases,82such as AFLOW,83Materials Project (MP),84MATDAT,85MatWeb,86MatMatch,87MakeItForm,88and MatNavi.89These databases consist of enormous materials properties obtained from experimental measurements and first-principles calculations, including mechanical properties like elastic constants, tensile/flexural/shear/fatigue strengths, fracture toughness, hardness, and so on."
            ]
          }
        ]
      }
    },
    "case_id": "34b00939c1190990c1c9be590cb07476",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are the key differences between participatory design and co-design?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are the key differences between participatory design and co-design?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should provide information about the origin of participatory and co-design.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Participatory design was first defined in Scandinavian literature \"as a model for involving users and designers on the technology itself in a process of technological development\" (Asaro, 2000, p. 257).",
              "co-designrefers to \"the creativity of designers and people not trained in design working together in the design development process\" (Sanders & Stappers, 2008, p. 6).",
              "The tradition of Scandinavian PD that developed in the 1970s shared concerns and values with labour unions in emancipating workers at the workplace (Bannon and Ehn 2012, 39). Co-Design is also intertwined with a political project of questioning and reconfiguring power relations and notions of expertise (Lenskjold, Olander, and Halse 2015)."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide key deifference between",
            "weight": 0.19999999999999998,
            "evidence": [
              "Participatory design implicitly and explicitly intends to create artifacts while transforming people collectively by taking into account other's perspectives (Konings et al., 2014).",
              "Co-design is collaborative by mixing together designers, users, novices, experts, citizens, or customers with the assumption that anybody is an expert regarding their own experience and mobilizes their practical and experiential knowledge as well as their conceptual knowledge.",
              "A few years ago, the Institution decided to transform its activities by introducing what they called \"co-design.\" The main objectives were to develop collaborative learning to accelerate the use of digital tools in both face-to-face and distance learning and to modernize their range of services, including developing a blended learning offer.",
              "In participatory design, users participating in the design make the tests.",
              "The co-design session was organized to produce and test prototypes.",
              ". To function as a collaborative tool, the framework is created into a corresponding tangible form as a series of canvases, constructed for co-design participants to work on together.",
              "PD is a practice of User-Centered Design (UCD), where multiple stakeholders contribute to the design process as active co-designers.",
              "The aim of PD is for designers to incorporate the realities of the stakeholders' situation into the design process, while for stakeholders the aim is to be able to articulate their needs and goals, and to find appropriate technological means to achieve them.",
              "PD has the advantages of examining users' needs, challenging assumptions, encouraging reciprocal learning, and creating new ideas [36, 67]. By treating stakeholders as designers rather than consumers, the design team can boost its ability to frame and solve problems, and create value",
              "Participatory Design is concerned with the needs, wants, and desires of users, both as individuals and as a whole. However, it is not concerned as such with building technologies that users like or are pleasing. On the contrary, sometimes part of what Participatory Design does is to help users understand possibilities and alternatives, and what it takes to get there.",
              "Participatory Design embraces ways of working with people who are in some form or another going to be the end-users of the technology. But it does not stop there. Participatory Design works with various ways in which people can represent their peers, qua skills, particular insights, elected representation, etc.",
              "Participatory Design: Typically focuses on involving end-users and stakeholders in the design of systems, products, or services that directly affect their work or lives. It often has a strong emphasis on power dynamics and democratization of the design process [4].- Co-Design: Has a broader scope and can involve a wider range of stakeholders, including designers, researchers, developers, and end-users. It focuses on collaborative creativity throughout the entire design process [5].",
              "Participatory Design: Users are often involved as equal partners in the design process, with a strong emphasis on their expertise and knowledge of their own needs [1].- Co-Design: While users are involved, their role may vary from consultants to co-creators, depending on the specific project and approach [6].",
              " Participatory Design: Often employs ethnographic methods, workshops, and prototyping sessions to gather insights and involve users in decision-making [2].- Co-Design: Utilizes a wider range of creative methods, including design games, scenarios, and generative tools to facilitate collaborative ideation and concept development [3].",
              "Participatory Design: Explicitly addresses power imbalances and aims to empower users in the design process [4].- Co-Design: While it values user input, it may not always have the same explicit focus on power dynamics as participatory design [5].",
              "- Participatory Design: Often aims for consensus-based solutions that meet the needs of all stakeholders [1].- Co-Design: May prioritize innovative and creative outcomes, even if they don't always achieve full consensus among all participants [6]."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answe should provide informaation on the use of subsequently using participatory and co-design.",
            "weight": 0.09999999999999999,
            "evidence": [
              "Using a co-design and a participatory design process (Sanders & Stappers, 2008, 2014) enhanced the learner experience in three ways: a) by creating a pre-tested learning environment that used co-design and co-construction of the MOOC architecture, b) by committing that future learners who participated in the design would be influencers in the MOOC, and c) by changing the facilitators and designer's mindset through their involvement in the co-design sessions working with users as partners."
            ]
          },
          {
            "name": "nice_to_have_item_1",
            "criterion": "The answer should include challenges involved in participatory and co-design.",
            "weight": 0.09999999999999999,
            "evidence": [
              "Major issues of concern in PD include taking into account the domain specialists' expertise, beneficial and sustainable innovation, taking seriously multiple viewpoints and the facts and resources they provide, taking into account the context of the product, the authentic experience of the experts regarding the problem being solved, hands-on methods to solve real world problems, and reflective practice of design [98]."
            ]
          }
        ]
      }
    },
    "case_id": "95a152ddae491933ef75c797f3584b08",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are some systems papers that conduct an observational study as the formative study instead of just interviews?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are some systems papers that conduct an observational study as the formative study instead of just interviews?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with defining the observational study.",
            "weight": 0.24,
            "evidence": [
              "Cochran [12] defined an observational study as an empiric comparison of treated and control groups in which: the objective is to elucidate cause-and-effect relationships [... in which it] is not feasible to use controlled experimentation, in the sense of being able to impose the procedures or treatments whose effects it is desired to discover, or to assign subjects at random to different procedures.",
              "Analytical adjustments, such as matching, are used to control for overt biases, that is, pretreatment differences between treated and control groups that are visible in observed covariates. Analytical adjustments may fail because of hidden biases, that is, important covariates that were not measured and therefore not controlled by adjustments. Sensitivity analysis indicates the magnitude of hidden bias that would need to be present to alter the qualitative conclusions of the study."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide the list of papers with contribution of each along with reference which conduct observational study as the formative study in systems instead of just interviews.",
            "weight": 0.24,
            "evidence": [
              "To gather multi-modal data, we propose an IoT-based smart ambient behavior observation system (SABOS). SABOS provides unobtrusive monitoring of daily living activities by utilizing various sensors integrated into the residential house. To reduce the amount of data, we present a data reduction algorithm. The data reduction algorithm effectively reduces over 90% of the submitted data with full recovery in the cloud.Irfan, M., Jawad, H., Felix, B. B., Abbasi, S. F., Nawaz, A., Akbarzadeh, S., ... & Chen, W. (2021). Non-wearable IoT-based smart ambient behavior observation system.IEEE Sensors Journal,21(18), 20857-20869.\"https://ieeexplore.ieee.org/abstract/document/9486939\"Objective To assess the short term association of inpatient implementation of electronic health records (EHRs) with patient outcomes of mortality, readmissions, and adverse safety events.Barnett, M. L., Mehrotra, A., & Jena, A. B. (2016). Adverse inpatient outcomes during the transition to a new electronic health record system: observational study.bmj,354.\"https://www.bmj.com/content/354/bmj.i3835.abstract\"Objective To evaluate on a large scale, across 272 common types of laboratory tests, the impact of healthcare processes on the predictive value of electronic health record (EHR) data.Agniel, D., Kohane, I. S., & Weber, G. M. (2018). Biases in electronic health record data due to processes within the healthcare system: retrospective observational study.Bmj,361.\"https://www.bmj.com/content/361/bmj.k1479.abstract\"In an observational study in head-injured patients, cerebrovascular pressure transmission was investigated using a systems analysis approach whereby the blood pressure (BP) waveform was used as a measure of an input stimulus to the cerebrovascular bed (CVB) and the intracranial pressure (ICP) waveform as the response to that stimulus. The transfer function is a measure of how much pressure is transmitted through the CVB at a given frequency and is calculated using Fourier analysis of the pressure waveforms. The transfer function allows quantification of the pressure transmission performance of the CVB, thus providing a basis for comparison between normal and abnormal function.Piper, I. R., Miller, J. D., Dearden, N. M., Leggate, J. R., & Robertson, I. (1990). Systems analysis of cerebrovascular pressure transmission: an observational study in head-injured patients.Journal of neurosurgery,73(6), 871-880.\"https://thejns.org/view/journals/j-neurosurg/73/6/article-p871.xml\"[Aim]: The detection and characterization of common bug-fix patterns in software repositories play an important role in advancing the field of automatic program repair. In this paper, we aim to characterize the occurrence of known bug-fix patterns in Java repositories at an unprecedented large scale. [Method]: The study was conducted for Java GitHub projects organized in two distinct data sets: the first one (i.e., Boa data set) contains more than 4 million bug-fix commits from 101,471 projects and the second one (i.e., Defects4J data set) contains 369 real bug fixes from five open-source projects. We used a domain-specific programming language called Boa in the first data set and conducted a manual analysis on the second data set in order to confront the results.Campos, E. C., & de Almeida Maia, M. (2017, November). Common bug-fix patterns: A large-scale observational study. In2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)(pp. 404-413). IEEE.\"https://ieeexplore.ieee.org/abstract/document/8170127\"We start with a general characterization of the failure dataset examined in this study, where interesting findings are presented, e.g., the most frequent failure types per period of a day and per different workplaces. Next, we investigate the existence of failure patterns. For this purpose, we introduce an OS failure pattern discovery protocol that identifies failure patterns exhibiting consistency across different computers used in the same as well as different workplaces. In total, we discovered 45 failure patterns with 153,511 occurrences.dos Santos, C. A. R., & Matias Jr, R. (2018). Failure patterns in operating systems: An exploratory and observational study.Journal of Systems and Software,137, 512-530.",
              "Bohm et al. (10, Bohm et al., 2010) conducted a longitudinal field study of IYOUIT, a context-aware mobile application, observing 19 users over a one-month period to evaluate its usage and impact on digital lifestyle sharing.",
              "Wixon et al.  (3, Wixon et al., 1985) recorded and analyzed command usage on a VMS system, observing frequency patterns and comparing them to UNIX systems, demonstrating the applicability of observational techniques across different computing environments.",
              "\"A Field Study of Refactoring Challenges and Benefits\" by Kim et al. (2012)This paper used a combination of observational techniques, including tool usage monitoring and mining of version control systems, to study refactoring practices in industry. The researchers observed professional developers at Microsoft over a three-month period [2].Citation: [2] Kim, M., Zimmermann, T., & Nagappan, N. (2012, November). A field study of refactoring challenges and benefits. In Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering (pp. 1-11).",
              "\"An Empirical Study on Configuration Errors in Commercial and Open Source Systems\" by Yin et al. (2011)This paper conducted a large-scale observational study of configuration errors in real-world systems. The researchers analyzed customer support databases and bug repositories to understand the nature and impact of configuration errors [4].Citation: [4] Yin, Z., Ma, X., Zheng, J., Zhou, Y., Bairavasundaram, L. N., & Pasupathy, S. (2011, October). An empirical study on configuration errors in commercial and open source systems. In Proceedings of the Twenty-Third ACM Symposium on Operating Systems Principles (pp. 159-172)."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should discuss the benefits of observational study with others.",
            "weight": 0.12,
            "evidence": [
              " Observational studies in systems research are often combined with other methodologies, particularly interviews, to provide a more comprehensive understanding. This multi-method approach allows researchers to capture both observed behaviors and participant perspectives.",
              "Observational studies in systems research are frequently complemented by other research methodologies to enhance the depth and breadth of insights gained. A notable trend is the combination of observational techniques with interview methods. According to a comprehensive review by McDonald et al., observations were reported in 30.3% of the papers analyzed, and among these, the vast majority (81.6%) also incorporated interviews alongside observations  (18, MCDONALD, 2019). This combination allows researchers to not only observe user behaviors directly but also to gain insights into participants' thoughts, motivations, and explanations for their actions."
            ]
          }
        ]
      }
    },
    "case_id": "6296a33a2bcc31f1b8f9a765e63aaf3b",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What principles of interactive machine teaching can be applied to interactively curating social media feeds?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What principles of interactive machine teaching can be applied to interactively curating social media feeds?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with the background on personalized social media feed.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Social media feeds shape our everyday online interactions with others. Their interface designs and affordances define boundaries onhowwe can engage with people and content, while their algorithms dictate bothwhatandwhowe engage with in the first place.",
              "Social media feeds are deeply personal spaces that reflect individual values and preferences. However, top-down, platform-wide content algorithms can reduce users' sense of agency and fail to account for nuanced experiences and values. Drawing on the paradigm of interactive machine teaching (IMT), an interaction framework for non-expert algorithmic adaptation, we map out a design space for teachable social media feed experiences to empower agential, personalized feed curation.",
              "Modern systems can augment people's capabilities by using machine-learned models to surface intelligent behaviors. Unfortunately, building these models remains challenging and beyond the reach of non-machine learning experts."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide the introduction of interactive machine teaching",
            "weight": 0.17142857142857143,
            "evidence": [
              "interactive machine teaching (IMT), an IML process in which the human-in-the-loop takes the role of a teacher, and their goal is to create a machine-learned model.",
              "Researchers studying IML have considered a wide variety of different roles for the humans engaged in the interactive loop with the machine learner. These include the roles of ML experts, data scientists, crowdsource workers, and domain experts. Logically, the form and function of IML systems are, in large part, determined by the nature of these roles.",
              "IMT is an iterative process. In particular, it is one in which a person (or people) with a role of a teacher(s) interacts with a machine learner in order to transfer the knowledge of how to accomplish a task to a machine-learned model.",
              "While machine learning focuses on creating new algorithms and improving the accuracy of \"learners\", the machine teaching discipline focuses on the efficacy of the \"teachers\".",
              "Interactive machine teaching principles can indeed be applied to the curation of social media feeds, offering a more personalized and engaging user experience. This approach combines elements of machine learning, human-computer interaction, and information retrieval."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide the principles  of interactive machine teaching can be applied to interactively curating social media feeds",
            "weight": 0.17142857142857143,
            "evidence": [
              "MT consists of three main stages that form a \"teaching loop\" [90]:(1) Planning: the human teacher (the subject matter expert) identifies a task for the algorithmic learner (the machine learning model) to complete, along with a curriculum (set of examples and representations to help teach the learner, typically in the form of a small dataset).(2) Explaining: the teacher shows the learner examples and explicitly identifies concepts the agent should learn.(3) Reviewing: the teacher allows the learner to predict some unseen examples, corrects any erroneous predictions, and updates the teaching strategy/curriculum accordingly.",
              "IMT shares similar goals with the paradigm of reinforcement learning and, more specifically, reinforcement learning with human feedback (RLHF) [81]. Both IMT and RLHF strive to interactively and iteratively embed specialized human knowledge into a machine-learned system.",
              "The key difference between IMT and RLHF, however, is that in IMT, the human teacher has agency--defining the curriculum, explaining concepts, and evaluating performance--whereas in RL(HF), the human is merely an oracle that the agent queries to guide its decisions [18].",
              "To formalize the knowledge decomposition process [79], we defined asignalas an information unit consisting of two primary components: a feature and a characteristic (see Fig.1). A feature is a class of information that can be extracted from a post. Example features include the post's author, the textual content, image(s), the number of likes, and the post's topic. A characteristic is a subjective statement that describes a feature. It is subjective in the sense that its significance may vary between participants. For example, the feature \"the post's author.\" is described by the characteristic \"is someone I know from in-person interactions.\" A third, optional component, an action, is something the participant can perform to the post in response to a feature-characteristic combination.",
              "The goal in IMT is that of building a machine-learned model to complete future tasks. This goal is shared with the field of ML. As a field, ML focuses on the development of algorithms for learning from data (e.g., labels, user interactions, and demonstrations) and optimizing their parameters.",
              "The role of the people interacting in IMT is that of a teacher. That is, they are experts onparticular subject matter and have the explicit goal of creating an ML model about it through a teacher-student-type of interaction.",
              "In the context of IMT, we say that a person acts as a teacher if they: Plan (and update) the teaching curriculum. A teacher can decide how to solve a problem or how to teach. A teacher may choose what examples to teach with, choose when and how to evaluate the task against the task's goal, choose when and how to fix/update the model, decompose or merge concepts, features or tasks, etc. Explain knowledge pertinent to the subject domain. One of the key aspects of IMT is that it advocates for teachers expressing rich semantic knowledge to a machine learner to make the teaching process efficient. Review the learner's progress while integrating the given knowledge. As with traditional IML, teaching takes the form of a dialog, as the teacher's next action is influenced by the learner's current state.",
              "The planning activity is one in which the teacher considers and identifies the materials that are useful for teaching. These can include examples, features, tests, etc. The nature of these materials depends on the goal of the teaching activity.",
              "The explaining activity is the activity by which the teacher explicitly provides the knowledge required to the learning algorithm by perform the task of interest.",
              "During the reviewing activity, the teacher evaluates the performance of the learned model and determines whether and how to continue to teach the system. This activity can include debugging erroneous predictions, assessing the quality of predictions, identifying problematic examples, identifying incorrect or incomplete labels that the teacher has provided to the learner, and building trust in the performance of the system in various use scenarios.",
              "According to Ramos et al.[18], the machine teacher is engaged in three activities: Planning, to identify diverse, challenging examples to teach, reflect on their strategy, and adjust their approach as they assess the evolution of concepts.; Explaining, to provide the necessary knowledge to the learning algorithm such as labelling data for classification; and Reviewing, to evaluate the confusions, debug errors, and correct labels to gain a comprehensive understanding of the model performance.",
              "1. **User-Teachable Agents**: By incorporating user-teachable agents, social media platforms can empower users to explicitly teach the algorithm about their preferences. This can be achieved through in-feed affordances that enable users to articulate content preferences, evaluate the agent's effectiveness, and iteratively formulate a curriculum of teaching goals and examples.",
              "2. **Personalized Curation**: IMT principles can be used to create personalized social media feeds that align with users' interests and preferences. This can be achieved by allowing users to provide explicit examples of content they like or dislike, which the algorithm can then use to curate the feed.",
              "3. **Iterative Feedback**: Social media platforms can incorporate iterative feedback mechanisms that enable users to correct the algorithm's mistakes and adapt to new information. This can be achieved through features such as \"like,\" \"dislike,\" or \"not interested\" buttons that provide explicit feedback to the algorithm.",
              "The primary principles of IMT include:1. **Iterative Feedback**: Continuous, real-time input from humans to refine the model.2. **Personalization**: Tailoring the learning content and pace based on the user's needs and preferences.3. **Active Learning**: Allowing the model to query the teacher actively for ambiguous or uncertain data points.4. **Transparency and Interpretability**: Ensuring the model's decisions and learning process are understandable to humans."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should provide platform which can used for interactive machine teaching.",
            "weight": 0.08571428571428572,
            "evidence": [
              "We designed PICL to as a platform to help our IMT research, thus we aimed at it providing teachers with an interface for building ML models by supporting their capabilities while engaged in an IMT loop. Teachers bootstrap a teaching session in PICL by creating a teaching project where they import their text data source and specify the type of ML model they want to build. At any point during a teaching session, teachers can export or publish the latest in-progress ML model for others to use as a runtime to be imported in their software solution.",
              "Systems that aim at achieving the same goal of helping end-users to build text classification or extraction ML models compare only partially. Most cloud services such as IBM's Watson Studio (IBM, 2020), Amazon Sagemaker (Amazon, 2020), or Google's AI Platform (Google, 2020a) take a one-shot training approach, where an end-user feeds a system a large number of labeled examples before a model is trained. ",
              "TeachTOK draws upon interactive machine teaching systems previously described in the literature [6, 18, 19] but considers that data and models are shared among a group of teachers."
            ]
          }
        ]
      }
    },
    "case_id": "e61be146ef53f1a5483c668fc4f6390c",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "Why do quadruped robots typically use 3 degrees of freedom for each leg?  What would be the disadvantages of 2 dof?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "Why do quadruped robots typically use 3 degrees of freedom for each leg?  What would be the disadvantages of 2 dof?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with explaining the relation of degree of freedom w.r.t. The complexity of the system.",
            "weight": 0.17142857142857143,
            "evidence": [
              "The large number of actuated degrees of freedom makes them heavier, more complex and expensive than wheeled systems.",
              "n general, the dynamic model of a robotic system is obtained through the Lagrange Energy methods. However, in a multi-degree of freedom systems, the computational complexity of the Lagrange Equations may require very long formations."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide the reasons for typical usage of 3 degrees of freedom for each leg in quadruped robots.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Our quadruped robot, StarlETH's, has four articulated legs with three actuated degrees-of-freedom (DoF) each: hip abduction/adduction (HAA), hip flexion/extension (HFE), and knee flexion/extension (KFE).",
              "The leg has got six degrees of freedom (Hip-3 DOF, Knee-1 DOF, Ankle-2 DOF), but implementing all the six DOF is difficult due to increase in cost of the project and controlling of the actuators which become complex, so in this project reduced degrees of freedom are aimed so 3 DOF per leg has been finalized.",
              "When all four legs are in contact with ground, the net distributed forces begin to match both the desired forces and the desired torques, as there are enough degrees of freedom in the system.",
              "The main benefits of the framework that we used are that it is highly modular, that it allows the various parameters to be tunned in an an intuitive way, and that it results in locomotion controllers that are robust to pushes and unexpected variations in the terrain.",
              "The merits of the quadruped configuration are, (1) walking control of a quadruped is easier than that of a biped robot, and (2) when in a posture of sitting, two \"hands\" are free to move, and allow the display of emotions, or to communicate with a human through hand motions. Thus, because each leg or hand has to be used for various purposes besides walking, we assign three degrees-offreedom (DoF) for each leg/hand.",
              "Three degrees of freedom (DOF) in quadruped robot legs offer enhanced maneuverability and versatility. This configuration allows for more complex movements, better terrain adaptation, and improved overall performance compared to simpler designs.",
              "Quadruped robots typically use 3 DOF for each leg due to the significant advantages this configuration provides in terms of mobility and functionality.",
              "The 3 DOF configuration allows quadruped robots to achieve superior performance by enabling more complex and precise movements. With this setup, robots can produce straight-line foot trajectories relative to their body, which is crucial for efficient locomotion  (3, Khudher et al., 2016). Additionally, the 3 DOF design typically incorporates hip abduction/adduction, hip flexion/extension, and knee flexion/extension, providing a wide range of motion that closely mimics the leg structure of many animals",
              "One of the key benefits of 3 DOF legs is the ability to generate 3D forces at the robot's toes in most configurations, offering a good balance between capability, weight, and power density (10, Rozen-Levy et al., 2023). This increased maneuverability allows quadruped robots to adapt to various terrains and perform complex tasks that would be challenging or impossible with fewer DOF.",
              "3 DOF enables more complex and efficient gait patterns, mimicking natural quadrupedal locomotion more closely"
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide the disadvantages of quadruped robots having 2 degrees of freedom for each leg.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Two degrees of freedom (DOF) in quadruped robot legs significantly limit mobility and control capabilities. This configuration restricts the robot's ability to navigate complex terrains and perform advanced maneuvers, making it less versatile compared to 3 DOF designs.",
              "The primary limitation of 2 DOF legs is their restricted range of motion. With only two degrees of freedom, typically one for lifting and one for swinging, these robots lack the additional maneuvering capabilities provided by a third DOF  (6, Manoonpong et al., 2020).",
              "One of the most significant drawbacks of 2 DOF legs is the inability to control body angles and lateral motion. As noted by Youm et al., \"Since our quadruped robot has only 2-d.o.f. for each leg, the body angle and lateral motion are not controllable\"  (1, Youm et al., 2008). This limitation severely restricts the robot's ability to maintain balance and navigate uneven terrain, making it less adaptable to diverse environments.",
              "Furthermore, 2 DOF legs result in reduced walking speed, higher driving torque requirements, and a larger footprint compared to their 3 DOF counterparts"
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should introduce Quadruped robots.",
            "weight": 0.08571428571428572,
            "evidence": [
              "Quadruped robots are legged mobile robots that increase their popularity in robotic and control areas due to their complex dynamic structure with high mobility in different terrain conditions compared to wheeled systems.",
              "Quadruped robots are four-legged robots designed to mimic the locomotion of animals such as dogs, cats, or horses. These robots have gained significant attention in robotics research due to their potential for navigating complex terrains and performing tasks in challenging environments."
            ]
          }
        ]
      }
    },
    "case_id": "e307068003936acfd8f82207c67e25bb",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "Can you recommend a suitable theoretical lens for qualitative research concerning robotics process automation (RPA) implementations?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "Can you recommend a suitable theoretical lens for qualitative research concerning robotics process automation (RPA) implementations?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should highlight the importance of theoretical lens for qualitative research concerning robotics process automation (RPA) implementations",
            "weight": 0.3,
            "evidence": [
              " Qualitative data will thus enable a rich understanding of incidents in relation to RPA and subsequently allow for a theoretical understanding of the process.",
              "Robotic Process Automation (RPA) has gained significant attention in recent years due to its potential to transform business processes by automating repetitive tasks. As RPA implementations continue to grow, it is essential to understand the qualitative impacts of RPA on organizations."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide information on the theoretical frameworks used for qualitative research concerning robotics process automation (RPA) implementations",
            "weight": 0.3,
            "evidence": [
              "By linking the sensemaking lens with the Normalization Process Theory (NPT), It is found that the sensemaking and sensegiving practices while implementing RPA through the four dimensions (coherence, cognitive participation, collective action, reflexive monitoring) are occurring concurrently.",
              "It has been recognized that the most frequently used perspectives on technology adoption are the Innovation Diffusion Theory (IDT), the Technology Acceptance Model (TAM), the Theory of Planned behavior (TBP), the Unified Theory of Acceptance and Use of Technology (UTAUT), and the Technology-Organization-Environment (TOE) framework",
              "Both TAM and TPB have emerged from the sociological behavior literature and share many similarities (Ajzen, 1991; Davis, 1989). The two models aim to explain why users accept or reject information technology, making behavioral intention its dependent variable. behavioral intention is in turn thought to be closely linked with actual behavior.",
              "As of now, our CSF framework mainly presents a theory of analysis and description (Gregor, 2006). It describes the phenomenon and identifies constructs (RPA success and corresponding CSF). Descriptive theories are required when limited knowledge about the phenomenon exists.",
              "As part of the preparation phase, candidate processes have to be identified that can be used for a Proof of Concept (PoC) [15, 65, 80, 86, 100]. Processes consisting of highvolume and repetitive tasks are most suitable forhttps://studenttheses.uu.nl/handle/20.500.12932/34910\"Grounded theory is a qualitative research methodology that involves the collection and analysis of data to develop a theory or conceptual framework. This approach is particularly useful for studying RPA implementations as it allows researchers to explore the experiences and perceptions of individuals involved in RPA projects. Strauss and Corbin's (1994) grounded theory approach can be applied to identify patterns and themes in RPA implementations.",
              "Thematic analysis is a qualitative research method that involves identifying and coding themes in data. This approach can be used to analyze the qualitative impacts of RPA on organizations.",
              "The first approach uses a modified function-oriented value analysis to examine RPA's potential in specific industry contexts.",
              "The second theoretical lens proposed for RPA research is the Task-Technology Fit model. Beerepoot et al. build on this model to develop a framework that matches AI capabilities with specific RPA tasks  (7, Beerepoot et al., 2021).",
              "Socio-Technical Systems (STS) theory is a well-established framework that considers both the social and technical aspects of organizational systems.",
              "Rogers' Diffusion of Innovation theory provides a framework for understanding how new technologies spread within organizations and societies."
            ]
          }
        ]
      }
    },
    "case_id": "880129a65489610c5c805e174e256072",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What considerations are important for material selection in fabricating industrial robotic grippers?  What are the advantages and disadvantages of soft vs rigid gripper materials?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What considerations are important for material selection in fabricating industrial robotic grippers?  What are the advantages and disadvantages of soft vs rigid gripper materials?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with providing the importance of grippers in industrial robots.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Because of the wide variety of objects manipulated in industrial processes, many different grippers, based on different principles, have been developed. Gripper choice or gripper design is often considered the last problem to be solved when a process is automatized. In this way the choice is often a compromise solution, or only the most common grippers are adopted to satisfy the task.",
              "To increase the flexibility of grippers, many technological avenues are possible from offering more evolved mechatronic architectures with complex kinematics as well as sensing apparatus and control to, more simply, increasing the strokes of the grippers.",
              "In the design of flexible and adaptive soft-robotic gripping systems, materials with a small Young's modulus are used in order to be risk-free in interaction with humans.",
              "When fabricating industrial robotic grippers, the selection of materials is crucial to ensure the gripper's performance, durability, and compatibility with the workpiece."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide information on material selection  in fabricating industrial robotic grippers.",
            "weight": 0.17142857142857143,
            "evidence": [
              "The object to be manipulated is a steel cube of edge length of 51mm. On the faces there are very small drilled holes, whose area covers more than 50% of the total surface available. The weight of the object is 330g. The selected grippers for the required operations are the ones belonging to the following categories: x friction two fingers; x friction jaw; x magnetic.https://www.sciencedirect.com/science/article/pii/S2212827114006945\" newer materials are being used to improve functionality of grippers, which include piezoelectric, shape memory alloys, smart fluids, carbon fiber, soft fabrics, micro electro mechanical systems, and synthetic sheets and many more. Industrial robots are commonly used in a large number of applications because they present superior performance in terms of accuracy, precision, rigidity, and most importantly speed.",
              "We showed that soft, modular and adaptive grippers by utilizing different gripping components and materials can increase both performance and flexibility; however, as we argued the challenges in these improvements, introducing softness into the robotic gripper design requires set of design and control principles compared to hard grippers, and smart materials are not capable of providing strong gripping force.https://www.researchgate.net/profile/Nilesh-Sabnis/publication/328065054_A_Review_State_of_The_Art_of_Robotic_Grippers/links/5bb5c81545851574f7f806a4/A-Review-State-of-The-Art-of-Robotic-Grippers.pdf\"Another promising material-jamming approach is a granulate-filled pillow stiffened by negative pressure [14]. The stiff surface of the pillow gripper adapts the topology of the handled object. Hence, the object is kept through form closure and the operating vacuum.",
              "The Fin Ray(r) gripper was selected due to the rigid body joints, which produce an uniform distribution of material strain through the material-bond structure. In conventional grippers with traditional articulated joints the load transmission takes place via line forces (sliding, roller and needle bearings) or via point forces through ball bearings, which lead to unnecessary additional weight on the gripper.",
              "the material should be suitable for easy processing, have a low weight, a high toughness and a high ductility as well as a high load capacity. Especially silicones and elastomers are commonly used for the production of soft-robotic grippers but are only suitable for lower payloads due to their low Young's modulus.",
              "The material for the use in heavy-duty softrobotic grippers has to have soft-properties and should offer the possibility of absorbing larger forces without damage through the surface structure of the gripper jaw. At the same time the material has to have sufficient flexibility to adjust the gripper surface to the topology of the handled object.",
              "Depending on its chemical composition and morphological structure, Polyurethanes (PU) combines the essential properties of thermoplastics, elastomers, and duromers and accomplish all soft-property criteria.",
              " Polyurethanes are assigned to the polymer class (103 Pa < Young's modulus > 107 Pa) and are among the most commonly used soft-robotic materials.",
              "Due to the increased Young's modulus of PU compared to silicones, they are well suited as material for the production of heavy-duty soft-robotic grippers. The increased Young's modulus of PU offers the possibility to absorb larger forces with the gripping jaws and at the same time enough flexibility to adapt to the handled object.",
              "Grippers have then been ranked through a Multiple Attribute Decision Making (MADM) method, in order to identify the best gripping tools for the handling of non-rigid materials.",
              "Hybrid approaches in robotic gripper design aim to combine the benefits of both soft and rigid materials, addressing the limitations of each while capitalizing on their strengths. This synergy allows for grippers that can adapt to various object shapes while maintaining the ability to handle heavier loads and perform precise manipulations.",
              "The material must withstand the mechanical stresses involved in gripping and manipulating objects without breaking or deforming.",
              "- Fatigue resistance is also essential for applications involving repetitive motions.",
              "- Lightweight materials are preferred to reduce the overall weight of the robotic arm, thereby improving energy efficiency and reducing wear and tear on joints and actuators.",
              "- The material must provide adequate friction to securely hold different objects without slipping.- Surface texture and coatings can also influence grip effectiveness.",
              " Materials should be resistant to chemicals if used in environments involving corrosive substances or solvents.",
              "The gripper must maintain functionality within the temperature range of its operating environment, including extremes."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide a comparison of soft vs rigid gripper materials.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Soft grippers have experienced a growing interest due to their considerable flexibility that allows them to grasp a variety of objects, in contrast to hard grippers, which are designed for a specific item. One of their most remarkable characteristics is the ability to manipulate soft objects without damaging them. This, together with their wide range of applications and the use of novels materials and technologies, renders them a very robust device.",
              "Traditional hard robotic grippers consist of a set of mostly rigid joints and links [3] that have to precisely adapt to the object to not damage it. Consequently, their use is limited to a specific item type. If a variety of objects needed to be manipulated, it will be very difficult, if not impossible, to grasp them with the same rigid gripper.",
              "It is also worth mentioning the fabric-based soft grippers; these pneumatic actuators present some advantages over others due to their high flexibility, stiffness customizability, and high force-to-weight ratio.",
              "the soft grippers based on soft pneumatic actuators (SPAs), which offer better compliance, and a higher degree of freedom than the rigid robot, have been actively developed, Due to the inherent compliance of soft materials, complicated controls and additional structures are not needed with soft grippers, as the proper selection of materials of various stiffness allows for simple control. Furthermore, soft grippers with SPAs are constructed from low-cost and readily-available elastomers.",
              "SPAs with a variety of forms and operating pressures, including granular jamming, and fiber-reinforced SPAs have been suggested; however, their practical application as a robotic gripper has been limited by a small fingertip force and slow actuation speed.",
              "Soft grippers may require more complex control systems and higher power inputs compared to rigid grippers.",
              "Rigid grippers are made from strong, durable materials, making them suitable for heavy-duty applications.",
              "Rigid grippers provide high precision and repeatability, making them suitable for tasks requiring high accuracy.",
              "Rigid grippers often have simpler control systems and lower power requirements compared to soft grippers.",
              "- **Limited Flexibility and Adaptability**: Rigid grippers are less adaptable to different workpiece sizes and shapes, making them less suitable for handling delicate or irregularly shaped workpieces.- **Potential for Injury or Damage**: Rigid grippers can cause injury to humans or damage to the environment if not designed or controlled properly.- **Higher Cost and Complexity**: Rigid grippers can be more expensive and complex to design and manufacture compared to soft grippers.",
              "Soft grippers can allow the gripping of parts with variation in surface geometry, but often focus on gripping alone and may not be able to bring the assembly forces required"
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should provide the importance of grip simulation before implementing it in the real-world.",
            "weight": 0.08571428571428572,
            "evidence": [
              "Due to the characterization of the material model carried out in advance, the predictive deformation in a gripping simulation will be precise enough to derive the locally applied contact force on the gripped object and to examine the maximum capable payload of the gripper. Accordingly, gripping positions can be simulated in advance, which provides information whether the gripper can be used or has to be adapted. Additionally, the maximum payload of the designed gripper can be determined by calculating the necessary pull-off force through the grip simulation."
            ]
          }
        ]
      }
    },
    "case_id": "050725a9a16e6694b491f735c63ace96",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are good benchmarks and evaluation strategies for comparing obstacle avoidance strategies in mobile robots?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are good benchmarks and evaluation strategies for comparing obstacle avoidance strategies in mobile robots?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with providing the importance of benchmarking and evaluation of obstacle avoidance strategies in mobile robots. .",
            "weight": 0.17142857142857143,
            "evidence": [
              "In domains such as processors and graphics cards, benchmarks are not only very common, but every new product is evaluated and estimated towards a whole set of benchmarks. Several organizations exist that explicitly deal with defining and applying benchmarks, such as SPEC, BAPCo, EEMBC, and many more. A major issue is the problem that companies may want to influence benchmarks in order to gain an advantage for their own products.",
              "Obstacle avoidance is a crucial capability for mobile robots, enabling them to navigate safely through environments while avoiding collisions. Evaluating and comparing different obstacle avoidance strategies is essential for advancing the field and selecting appropriate algorithms for specific applications.",
              "Obstacle avoidance is a crucial aspect of autonomous navigation in mobile robots. To ensure the effectiveness of obstacle avoidance strategies, it is essential to evaluate and compare them using standardized benchmarks and evaluation strategies.",
              "A comprehensive evaluation of obstacle avoidance strategies in mobile robots involves a mix of quantitative metrics, qualitative assessments, and thorough testing in both simulated and real-world environments. By following these structured benchmarks and evaluation strategies, researchers can ensure a rigorous and holistic assessment of the effectiveness and robustness of new algorithms."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide benchmarks to compare obstacle avoaidance strategies in mobile robots.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Bench-MR, the first open-source motion-planning benchmarking framework designed for sampling-based motion planning for nonholonomic, wheeled mobile robots. Unlike related software suites, Bench-MR is an easy-to-use and comprehensive benchmarking framework that provides a large variety of sampling-based motion-planning algorithms, extend functions, collision checkers, post-smoothing algorithms and optimization criteria.",
              "Bench-MR is a highly configurable and expandable software suite with representative state-of-the-art motion-planning and evaluation components. It helps one to gain novel insights, such asi)how some combinations of motion-planning and post-smoothing algorithms achieve better performance than asymptotically (near) optimal motion-planning algorithms orii)how changes of the obstacle density in navigation scenarios can affect the planning efficiency and the resulting path quality.",
              "The heuristic cost is determined by the distance of the current grid cell's center to the target grid cell's center, and the path cost is calculated by the distance between the centers of adjacent grid cells. The algorithm repeats the exploration by picking the cell with the lowest price (sum of path cost and heuristic cost) in the priority queue until the picking cell reaches the target (path is found) or the queue is empty (path is not found). If the path is found, the algorithm will construct the absolute path by following the path cost from the target grid cell.",
              "Arena-Bench is a benchmarking suite designed for evaluating obstacle avoidance approaches in highly dynamic environments. It provides tools to design and generate evaluation worlds, scenarios, and tasks for autonomous navigation. This suite allows for the testing, evaluation, and comparison of different motion-planning techniques in complex environment scenarios."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide evaluation metrics for comparing obstacle avoidance strategies in robots..",
            "weight": 0.17142857142857143,
            "evidence": [
              "* Cost metrics: * computation time, execution time * reaction time, cycle length of update step * time to collision, distance to collision * number of parameters to tune * variance in results",
              "* Utility metrics: * path length, distance traveled * clearance, i.e. minimum distance to obstacles * smoothness of trajectory * precision at target * distance/error from given trajectory, orientation error * scaling to 3D, in contrast to 2D * scaling concerning degrees of freedom",
              "* Reliability metrics: * mission success * number of collisions * robustness in narrow spaces, number of narrow passages traversed, or narrowness of passages * dependency on parameters (small changes in parameters should not have big influences on result) * avoiding traps or cyclic behavior * occurrence of oscillations * control loop stability",
              "The success statisticsmeasure the percentage of found, collision-free and exact paths. Whether a path is collision-free is checked with a given collision checker.",
              "The path lengthmeasures the length in meters (m) of a path in the workspace.",
              "The computation timesmeasure the time in seconds (s) required for collision checking, for extend function evaluation (namely forward integration when using forward propagation or solving the two-point boundary value problems when using steer functions), and for finding an initial path.",
              "The mean clearing distancemeasures how close a path is to obstacles (reported in meters, m).",
              "In many vision-based applications, such as scene understanding, robotic perception, and picture reduction [10,11,12,13,14], semantic segmentation using deep learning (DL) is a fundamental problem. Minae et al. [10] examined DL-based segmentation models that showed remarkable performance in visual image segmentation tests to address lack of regular datasets for evaluating object segmentation.",
              "the results of our evaluation show that the approach successfully detects and avoids static and dynamic obstacles in real time with high accuracy, efficiency, and smooth steering with low angle changes.",
              "For evaluating mobile robot local planning approaches, the MRPB 1.0 benchmark has been proposed  (122, Fang et al., 2020). It features carefully designed simulation scenarios that challenge the applicability of local planners in large-scale, partially unknown, and dynamic complex environments. MRPB 1.0 also includes three types of evaluation metrics that consider safety, efficiency, and smoothness of motions."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should provide information about the testing frameworks to evaluate obstacle avoidance strategies in mobile robots.",
            "weight": 0.08571428571428572,
            "evidence": [
              "Several testing frameworks have been developed particularly for the domains of robot motion planning. These include MoVeMa [11], Framework for Obstacle Avoidance [21] and SAMPLE [16].",
              "Simulated environments offer a controlled and reproducible setting for evaluating obstacle avoidance strategies. Some popular simulation platforms include:a) Gazebo: An open-source 3D robotics simulator [1]b) V-REP (now CoppeliaSim): A versatile robot simulation platform [2]c) MATLAB Robotics Toolbox: Provides simulation capabilities for various robotic systems [3]",
              "For evaluating mobile robot local planning approaches, the MRPB 1.0 benchmark has been proposed  (122, Fang et al., 2020). It features carefully designed simulation scenarios that challenge the applicability of local planners in large-scale, partially unknown, and dynamic complex environments. MRPB 1.0 also includes three types of evaluation metrics that consider safety, efficiency, and smoothness of motions."
            ]
          }
        ]
      }
    },
    "case_id": "eef3ee38231d9fa5ffabbec75d1c5b50",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are some biologically-inspired robot path planning algorithms?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are some biologically-inspired robot path planning algorithms?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with explaining what biologically-inspired algorithms are.",
            "weight": 0.19999999999999998,
            "evidence": [
              "In the last few years, bio-inspired optimization techniques have been widely adopted in fields such as computer science, mathematics, and biology in order to optimize solutions. Bio inspired optimization problems are usually nonlinear and restricted to multiple nonlinear constraints to tackle the problems of the traditional optimization algorithms, the recent trends tend to apply bio-inspired optimization algorithms which represent a promising approach for solving complex optimization problems.",
              "Bio-inspired optimization algorithms are those methods that are generally inspired by physical principles, evolution theory and certain behaviors of living beings to efficiently solve optimization problems in very diverse application areas.",
              "Biologically-inspired algorithms draw inspiration from natural phenomena and biological systems to solve computational and engineering problems. Path planning for robotic systems is a critical area where such algorithms have demonstrated significant potential.",
              "Biologically-inspired algorithms offer a robust, flexible, and efficient approach to robot path planning. By mimicking natural processes and biological behaviors, these algorithms provide innovative solutions for navigating complex and dynamic environments."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide explaination on the biologically-inspired robot path planning algorithms.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Grey Wolf Optimizer (GWO) is a population-based algorithm[50],[51]that mimics the hunting mechanism of grey wolves. The approach is described by five operators: the social hierarchy, encircling prey, hunting, attacking prey, and search for prey.",
              "The social hierarchy operator classifies wolves (agents) into dominated and non-dominated groups. In each iteration, the fitness function evaluates the agent positions and determines the leadership. Alpha (a), beta (b), and gamma (g) wolves are assigned as non-dominated leaders, which oblige the rest of wolves, called omega (o), to follow them.https://ieeexplore.ieee.org/abstract/document/9223657\"Whale Optimization Algorithm (WO)is a population-based metaheuristic algorithm[55]that mimics the foraging behavior of humpback whales. The WO algorithm presents three mathematical models: encircling prey, spiral bubble-net attacking mechanism, and searching for prey. The encircling prey operator assumes that the best whale position is close to the global solution, based on which the others update their positions in the best whale neighborhood",
              "PSO algorithm is a stochastic optimization method that mimics the social behavior of bird or fish swarms[56],[57]. An individual of the swarm is denoted as a particle, which is a point in the coordinate system. Each particle has a current position x and a velocity V. During the search of a solution, new points can be found by adding V to x.https://ieeexplore.ieee.org/abstract/document/9223657\"An integrated biologically inspired self-organizing map (SOM) algorithm is proposed for task assignment and path planning of an autonomous underwater vehicle (AUV) system in 3-D underwater environments with obstacle avoidance. The algorithm embeds the biologically inspired neural network (BINN) into the SOM neural networks. The task assignment and path planning aim to arrange a team of AUVs to visit all appointed target locations, while assuring obstacle avoidance without speed jump.",
              "With the rapid development of artificial intelligence-related technologies, many intelligent algorithms are used for the complete coverage path planning domain [15], such as the genetic algorithm [16], the ant colony algorithm [17], and the biologically inspired neural network algorithm.",
              "Yang et al. [18] applied the biologically inspired neural network algorithm to the complete coverage path planning domain for the first time. In their algorithm, the map is shown as a grid map, and each grid is considered as a neuron. The adjacent neurons are connected, and the robot's mobile path is chosen by calculating the neuronal activity value.",
              "Zhu et al. [19] improved the biologically inspired neural network algorithm by presenting the Glasius bionic neural network algorithm (GBNN), which reduced the calculating time of algorithm path planning.",
              "biologically inspired neural network algorithm based on Q-learning, which proceeds with path planning based on the biologically inspired neural network algorithm, and when it comes to the position change of a reachable path point, processes path optimization by adopting Q-learning to avoid falling into a locally optimal solution, thus reducing the path repetition ratio, while still ensuring complete coverage, achieving the desired path planning results.",
              "Yang et al. [8] proceeded with path point optimization selection on the above biologically inspired neural network model, and applied it to the complete coverage path planning domain. In order to generate an ordered and low repetition ratio path, the deciding path points",
              "Firstly, the grid map is built by discretizing the three-dimensional underwater environment into many equal grids. Secondly, the activity values of all AUVs in the BINN maps of each target are calculated. Then, the AUV with the highest activity value in the BINN map of the target is selected as the winning AUV for the target. Finally, the winning AUV performs path planning according to the BINN strategy.",
              "The generator searches the configuration space surrounding existing nodes in the roadmap and uses a combination of random and deterministic search methods that emulate the behaviour of octopus limbs. The strategy consists of randomly mutating the states of the links near the end-effector, and mutating the states of the links near the base of the robot toward the states of the goal configuration.",
              "Ant Colony Optimization is inspired by the foraging behavior of ants. Ants deposit a chemical substance called pheromone on the ground as they move, which influences the paths taken by other ants. Over time, shorter paths tend to have higher concentrations of pheromones.",
              "Particle Swarm Optimization mimics the social behavior of birds flocking or fish schooling. Each individual, known as a particle, adjusts its trajectory based on its own experience and the success of its neighbors.",
              "Genetic algorithms are inspired by the process of natural selection. They work by evolving a population of candidate solutions over generations using mechanisms analogous to biological evolution, such as selection, crossover, and mutation.",
              "Neural networks and reinforcement learning methods are inspired by the brain's structure and learning mechanisms. Neural networks model complex patterns through layers and neurons, while reinforcement learning focuses on learning optimal actions through trial and error.",
              " Artificial Immune System (AIS)AIS algorithms are inspired by the principles and processes of the vertebrate immune system, particularly its ability to recognize and respond to foreign entities."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should name the state-of-the-art biologicaly-inspired algorithms.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Particle swarm optimization (PSO), Genetic Bee Colony (GBC) Algorithm, Fish Swarm Algorithm (FSA), Cat Swarm Optimization (CSO), Whale Optimization Algorithm (WOA), Artificial Algae Algorithm (AAA), Elephant Search Algorithm (ESA), Cuckoo Search Optimization Algorithm (CSOA), Moth flame optimization (MFO), and Grey Wolf Optimization (GWO) algorithm."
            ]
          }
        ]
      }
    },
    "case_id": "38dea1ab1b53857ef1e9ffad2aab6691",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "Is it possible to get frequency domain information in a fenquency band beyond the limitation of nyquist theory?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "Is it possible to get frequency domain information in a fenquency band beyond the limitation of nyquist theory?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with providing informaiton about Nyquist criteria/theory.",
            "weight": 0.17142857142857143,
            "evidence": [
              "In the classic band-limited scenarios employing orthogonal pulse shapes, the bound of an inter-symbol interference (ISI)-free symbol interval is given by T0= 1/(2W), assuming that the symbols are strictly bandlimited to W Hz. This constraint enables a simple receiver structure, relying on a matched filter and a low-complexity optimal symbol-by-symbol detection algorithm.",
              "At a high signal-to-noise ratio the main sources of interference at the input to the receiver are the intersymbol interference and crosstalk; by crosstalk we mean the interference between the different waveforms. It is desirable, therefore, for the receiver to eliminate both types of interference and to minimize the remaining error due to additive noise in the channel. This constraint on the intersymbol interference and crosstalk is defined as the generalized Nyquist criterion.",
              "At this point we can restate the Nyquist criterion as it applies to broadband signals:A signal of bandwidth BW must be sampled at a rate equal to or greater than twice its bandwidth (2BW) in order to preserve all the signal information.",
              " The Nyquist theorem then states that if we were to sample this signal we would need samples with a frequency larger than twice the maximum frequency contained in the signal, that is fsample[?] 2fmax.  If this is the case, we have not lost any information during the sampling process and we could theoretically reconstruct the original signal from the sampled signal."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide information on where the Nyquest criteria is inefficient.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Wideband analog signals push contemporary analog-to-digital conversion (ADC) systems to their performance limits. In many applications, however, sampling at the Nyquist rate is inefficient because the signals of interest contain only a small number of significant frequencies relative to the band limit, although the locations of the frequencies may not be known a priori. For this type of sparse signal, other sampling strategies are possible.",
              "The Nyquist theorem imposes a fundamental limitation on the frequency content that can be accurately represented in a sampled signal. Frequencies above the Nyquist frequency (fs/2) will be aliased, meaning they will appear as lower frequencies in the sampled signal, leading to potential misinterpretation of the frequency content [2]."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide information on getting the drefequency domain knowledge in a frequency band beyond the limittion of Nyquist theory.",
            "weight": 0.17142857142857143,
            "evidence": [
              "A reduced-complexity three-stage-concatenated faster-than-Nyquist signaling (FTNS)-based transceiver architecture is proposed, which operates with the aid of soft decision (SoD) frequency-domain equalization (FDE) at the receiver. More specifically, the decoding algorithm conceived allows us to attain near-capacity performance as an explicit benefit of iterative detection, which is capable of eliminating the intersymbol interference imposed by FTNS.",
              "Let K denote the total number of frequencies in the signal, and let W denote its band limit in hertz. Simulations suggest that the random demodulator requires just O(K log(W/K)) samples per second to stably reconstruct the signal. This sampling rate is exponentially lower than the Nyquist rate of W hertz. In contrast to Nyquist sampling, one must use nonlinear methods, such as convex programming, to recover the signal from the samples taken by the random demodulator.",
              " the first proposal of the faster-than-Nyquist (FTN) concept in the 1970s [1], it has recently been rediscovered as a means of boosting a transmission rate beyond that defined by the Nyquist criterion [2], without imposing any additional bandwidth expansion.",
              "in the FTN signaling scheme transmitted symbol's interval T is typically set such that T<T0, hence more symbols are packed in the time domain than in the conventional arrangement.",
              "Compressive sensing is a signal processing technique that exploits the sparsity of signals in certain domains to reconstruct signals from fewer samples than required by the Nyquist theorem [3]. This approach can potentially recover frequency information beyond the Nyquist limit for signals that have a sparse representation in the frequency domain.",
              "Super-resolution methods aim to enhance the resolution of a signal beyond the limits imposed by the sampling rate. These techniques often rely on additional information or assumptions about the signal structure [4].",
              "In audio processing, bandwidth extension techniques are used to estimate and synthesize high-frequency components based on the available low-frequency information [5].",
              "Some techniques deliberately use aliasing to fold high-frequency components into the observable frequency range, and then use additional information or assumptions to unfold and recover the original high-frequency content [6].",
              "By using multiple sampling rates or combining information from multiple sensors, it's possible to extend the observable frequency range beyond what a single sensor operating at the Nyquist rate could achieve [7].",
              "Recent advancements in the analysis of the Estimation of Signal Parameters via Rotational Invariant Techniques (ESPRIT) algorithm have shown that it can achieve \"noisy super-resolution scaling\" beyond the Nyquist limit under certain conditions. This subspace-based signal processing technique can potentially attain an improved error scaling of $$\\mathcal{\\tilde{O}}(n^{-3/2})$$ with respect to the cutoff frequency n, even in high-noise scenarios[5].",
              "Other sub-Nyquist sampling strategies include multicoset sampling, coprime sampling, and generalized sampling  (8, Cohen et al., 2013)  (236, Govindarajan et al., 2023)  (232, Manganaro et al., 2023). These techniques often rely on multiple parallel sampling channels or specific sampling patterns to capture the signal information at sub-Nyquist rates.",
              "Multi-rate asynchronous sub-Nyquist sampling (MASS) has been proposed for wideband spectrum sensing in cognitive radio networks, offering lower sampling rates compared to previous approaches  (65, Sun et al., 2012). This method, along with other sub-Nyquist techniques, enables the processing of signals that extend beyond the Nyquist frequency, addressing practical needs in various applications  (22, Yamamoto et al., 2022)."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should provide limitations of the frequency domain information in a fenquency band beyond the limitation of nyquist theory are used.",
            "weight": 0.08571428571428572,
            "evidence": [
              " They may require additional information about the signal structure or sparsity.- The accuracy and reliability of the recovered high-frequency information may be limited.- These methods often work best for specific types of signals or under certain conditions.",
              "compressed sensing typically requires sparsity in the signal, and super-resolution techniques may have constraints on the signal-to-noise ratio or the nature of the underlying data."
            ]
          }
        ]
      }
    },
    "case_id": "d521598603fbab74a6c13d58993f8b55",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "How does the addition of XAI techniques such as SHAP or LIME impact model interpretability in complex machine learning models ?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "How does the addition of XAI techniques such as SHAP or LIME impact model interpretability in complex machine learning models ?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with introducing XAI.",
            "weight": 0.13333333333333333,
            "evidence": [
              "In general, the main aim of these interpretability techniques is to shed light and provide insights into the prediction process of the machine learning models and to be able to explain how the results from the prediction was generated.",
              "The capacity of LIME to be interpreted locally faithfully and SHAP to provide feature significance through its Shapley values was crucial in revealing the decision-making processes of these models. The focus on XAI increases the openness of our machine-learning models. It provides essential resources to examine and understand their complex workings, enabling us to take a wise and educated approach to using artificial intelligence.",
              "Model interpretations are often used in practice to extract real world insights from machine learning models. These interpretations have a wide range of applications; they can be presented as business recommendations or used to evaluate model bias. It is vital for a data scientist to choose trustworthy interpretations to drive real world impact.",
              "machine learning interpretability allows users to comprehend the results of the learning models by providing reasoning for the decisions that it has arrived at. This nature of Explainable AI(XAI) and Interpretable Machine Learning (IML) is particularly helpful in the context of AI applications pertaining to healthcare and medical diagnosis.",
              "Explainable AI (XAI) methods are used to improve the interpretability of these complex models, and in doing so improve transparency. However, the inherent fitness of these explainable methods can be hard to evaluate. In particular, methods to evaluate the fidelity of the explanation to the underlying black box require further development, especially for tabular data."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should discuss the performance of SHAP regarding the model interpretiability in complex machine learning models",
            "weight": 0.13333333333333333,
            "evidence": [
              "On average, SHAP has the smallest average time to output explanation across all data sets included in this study. For detecting the bias, SHAP and MAPLE enable the participants to better detect the bias.",
              "the SHAP summary plot aids in identifying the pivotal features crucial for the model's predictive accuracy. Features with the highest mean (|SHAP values|) are identified as pivotal for the model to make precise predictions. In scholarly contexts, the SHAP summary plot emerges as an invaluable tool for comprehending the functioning of a machine learning model and pinpointing features pivotal for accurate predictions.",
              "The SHAP beeswarm plot aids in pinpointing crucial features for accurate predictions, with features exhibiting a broader spread of SHAP values considered most important for the model.",
              "From a technical standpoint, SHAP values are derived from a meticulous formula, capturing the average alteration in the model output magnitude consequent to a modification in the value of a specific feature, with all other features held constant. This intricate formulation facilitates a granular comprehension of the nuanced contributions of individual features to the overall model predictions.",
              "In the academic milieu, the SHAP summary plot emerges as an invaluable analytical instrument, elucidating the intricacies of a machine learning model, revealing features pivotal for precision in predictions, and unraveling their respective impacts on the model output magnitude.",
              "Similar to LIME, Shapley Additive Explanations (SHAP) is a feature explanation algorithm that fits a separate additive explanation model that is easier to interpret than the original [7].",
              "\"SHAP exploits the principle that there exists a unique solution for the feature attribution values that satisfies the properties of local accuracy, missingness, and consistency.",
              "For both global and local feature explanations, XGBoost with SHAP is the best performing method in the low accuracy bucket.",
              "However, when the predictive accuracy of all models is low, XGBoost with SHAP may be the best choice to obtain trustworthy interpretations.",
              "Based on the analysis performed, as opposed to the traditional ML models, the use of SHAPley (used interchangeably with SHAP, both meaning the same) additive functions helps us to identify the output of the given classifier, observe, infer and interpret it in a way that the AI model's reasoning is valid and justified.",
              "SHAP's superiority is often attributed to its more precise interpretation mechanism and its ability to provide both local and global explanations  (132, Martino et al., 2022). This versatility makes SHAP a preferred choice in many applications, including healthcare and finance  (81, Vivek et al., 2021).",
              "SHAP is based on game theory concepts and provides a unified approach to explaining the output of any machine learning model. It calculates Shapley values, which represent the importance of each feature in a prediction [1]."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should discuss the performance of LIME regarding the model interpretiability in complex machine learning models",
            "weight": 0.13333333333333333,
            "evidence": [
              "LIME achieves the lowest performance for the identity metric. LIME achieves the highest performance for the separability metric across all data sets.",
              "The LIME technique has been introduced as a local interpretability technique that relies on the assumption that the decision boundary of a complex machine learning model is linear locally around the instance to be explained. It explains the instance of interest by fitting an interpretable model on perturbed sample around the input instance of interest. In particular, LIME generates a perturbed sample around the instance to be explained.",
              "For each instance in the perturbed sample, LIME gets the prediction from the model to be explained (the perturbed sample along with the prediction will act as the training data set for the interpretable model). Then, the technique assign weights to the instances in the new training data set according to their proximity to the instance to be explained. Finally, LIME fits an interpretable model on the new training data set.",
              "Local Interpretable Model-Agnostic Explanations (LIME) is a feature explanation algorithm that fits localized explainer models to explain how individual predictions are made [11]. The method is model agnostic; it does not depend on how the model is built, rather only on the predictions the model makes.",
              "Basically, LIME explanations are based on local surrogate models. These, surrogate models are interpretable models (like a linear model or decision tree) that are learned on the predictions of the original black box model. But instead of trying to fit a global surrogate model, LIME focuses on fitting local surrogate models to explain why single predictions were made. LIME can be used for traditional machine learning tasks and even contentbased recommendation system algorithms, as it is model-agnostic.",
              "However, LIME may have advantages in specific scenarios. Roberts et al. discovered that LIME performs better than SHAP in dense segments of datasets, while SHAP excels in sparse segments  (174, Roberts et al., 2022). This difference is attributed to the varying bias-variance characteristics of the underlying estimators used by LIME and SHAP.",
              "LIME creates a local, interpretable model around a specific prediction to explain how the model arrived at that decision. It works by perturbing the input and observing how the predictions change [2]."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should provide the limitaitons in XAI approaches.",
            "weight": 0.13333333333333333,
            "evidence": [
              "A major problem in this context is that both the quality of the interpretability techniques and trust of the machine learning model predictions are challenging to measure.",
              "Generally speaking, there is a trade-off between the performance of machine learning models and their interpretability. That is the more interpretable the model such as linear models and decision trees, the lower their performance would be compared with complex models such as deep learning models.",
              "LIME does not guarantee a fair distribution of the effect among the features, while Shapley value does.",
              "It is important to note that LIME local explanations are not additive and can not be combined into a global feature importance score. Linden et al. found that when aggregated, LIME local explanations fail to reliably represent global model behavior [13].",
              "Calculating SHAP values, especially for large datasets and complex models, can be computationally intensive, as it requires evaluating multiple permutations of feature sets.",
              "LIME's approach of generating synthetic data and training a local surrogate model for each instance can also be resource-heavy, particularly when dealing with a large number of predictions.",
              "Since LIME builds local surrogate models, the fidelity of these models to the original complex model may vary, potentially leading to inaccurate explanations in some instances.",
              "While SHAP provides a theoretically sound method, approximations or sampling methods used in practice to estimate SHAP values can sometimes lead to imprecise explanations."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should provide information on the performance of other techniques as well.",
            "weight": 0.06666666666666667,
            "evidence": [
              "MAPLE achieves the highest performance for the identity across all data sets."
            ]
          }
        ]
      }
    },
    "case_id": "5480ba91951fc42e9beb989eea40360d",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What strategies are used to improve robustness and safety of quadrotor UAVs in extreme weather conditions?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What strategies are used to improve robustness and safety of quadrotor UAVs in extreme weather conditions?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with providing the importance of robustness and safety of quadrotor UAVs.",
            "weight": 0.19999999999999998,
            "evidence": [
              "For the quadrotor helicopter, as an example of UAV systems, which is relatively a simple, affordable and easy to fly system, the accurate attitude control of it in harsh environmental conditions due to wind disturbances is an open challenge.",
              "Particularly, robustness issues can be critical for the rotorcrafts due to the complicated aerodynamic effects (which make it difficult to obtain an accurate dynamic model), such as the errors from sensors (like measurement noises) and external disturbances (like winds).",
              "In particular, robustness issues may be critical for quadcopter control since it can be subjected to undesired nonlinear dynamics and external disturbances. In order to weaken the effect of wind gusts, some authors have proposed complex control techniques to achieve stability.",
              "The satisfactory performance, safety and reliability are fundamental requirements in the QUAV system to complete the flight mission successfully. However, some specific applications will take the QUAV into hostile environments, such as strong wind.",
              "Early and reliable detection and diagnosis of various faults in the QUAV system are crucial to avoiding the tragedy of wreck. The accurate fault information will be conducive to taking remedial actions quickly to guarantee the safety operation of QUAV. Therefore, in order to improve the safety, reliability and survivability of QUAV[7], it is highly desirable to design a QUAV system with the capability to detect and diagnosis faults.",
              "Quadrotor UAVs (Unmanned Aerial Vehicles), commonly known as drones, face significant challenges when operating in extreme weather conditions like high winds, rain, snow, and temperature extremes. Ensuring robustness and safety under such conditions is crucial for their performance and reliability in practical applications."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide the solutions to improve robustness and safety of quadrotor UAVs in extreme weather conditions",
            "weight": 0.19999999999999998,
            "evidence": [
              "Firstly, a dynamical system of the quadrotor taking into account aerodynamical effects induced by lateral wind and actuator faults is considered using the Newton-Euler approach. Then, based on active disturbance rejection control (ADRC), the fault tolerant controller is proposed to recover faulty system and reject perturbations.",
              "The developed controller takes wind gusts, actuator faults and measurement noises as total perturbations which are estimated by improved extended state observer (ESO) and compensated by nonlinear feedback control law. So, the developed robust fault tolerant controller can successfully accomplish the tracking of the desired output values.",
              " a novel robust fault tolerant control strategy based on ADRC is successfully applied to stabilize an unmanned quadrotor aircraft subjecting to actuator faults and wind gusts. An improved ESO is designed to estimate the plant dynamics and disturbances, while a nonlinear feedback control law is developed to actively suppress disturbances and accommodate actuator faults. With the accurate estimation of the plant dynamics and disturbances by ESO, the developed controller can successfully drive the outputs of quadrotor aircraft to the desired values.",
              "The controller is developed combining two parts; one dedicated to stabilize the closed-loop system and the second one for dealing and estimating external disturbances as well unknown nonlinearities inherent to the real system's operations. For bounding the energy used by the system during a mission and, without losing its robustness properties, the quadratic problem formulation is used considering the actuators system constraints. The resulting optimal bounded control scheme improves considerably the stability and robustness of the closed-loop system and at the same time bounds the motor control inputs.",
              "Feedback linearization approach with anti windup scheme is designed as autonomous landing controller. Anti windup basically controls the integrator component accumulation in the autopilot design and restricts the output within saturation limits.",
              "Flare controller is designed with integrator and antiwindup scheme is used to handle the controller output within saturation limits. Landing is primarily a longitudinal mode operation but due to disturbances and coupling lateral and directional modes also gets activated.",
              "pitch angle, roll angle and yaw angle nonlinear dynamic equations are linearized to obtain the control commands in terms of elevator deflection, aileron deflection and rudder deflections. Similarly total velocity of aerial vehicle being an important parameter is controlled using thrust command. A first order linearized model of velocity is used to obtain thrust control command.",
              "An optimal bounded robust control algorithm was developed in this paper. This controller was conceived to be robust with respect to uncertain dynamics and external and unknown perturbations. Its structure contains two parts, one for stabilizing the aerial vehicle close to the ideal conditions (small angles) and the second one for estimating and compensating unknown dynamics. For improving robustness and avoid saturating the actuators of the system, the obtained controller of this scheme was bounded solving a quadratic problem in the control inputs and taking into account the motors constraints.",
              "First, the dynamic model of a QUAV taking into account actuator faults and external disturbances is constructed. Then, treating the actuator faults and external disturbances as augmented system states, an adaptive augmented state Kalman filter (AASKF), is developed without the need of make the assumption that the exact stochastic information of actuator faults and external disturbances are available. Next, in order to reduce the computational load of AASKF, an adaptive three-stage Kalman filter (AThSKF) is proposed by decoupling the AASKF into three sub-filters. The AThSKF-based FDD scheme can not only detect and isolate actuator faults but also estimate the magnitudes even if the QUAV suffers from the external disturbances.",
              "The dynamic model of the quadrotor is first obtained using Newton-Euler equations. Then, considering the underactuated and the strongly coupled characteristics of the quadrotor system, a nonlinear adaptive sliding mode control (ASMC) scheme is proposed. Meanwhile, additional adaptive laws are designed to estimate all the parameters of the quadrotor system, which in principle are difficult to be measured directly and accurately. Furthermore, to guarantee the asymptotic stability of the closed-loop system, the upper bound of the fully unknown external disturbance is estimated and adopted as the switching gain of the ASMC.",
              "The robustness of the ASMC schemes developed in [30, 31] to external disturbance and uncertainty of physical parameters has been verified by simulation experiments. In [32], a robust ASMC approach is developed for attitude and altitude tracking of a four-rotor system under the simultaneous effect of parametric uncertainties and consistent external disturbance. This ASMC can only deal with external disturbance with known upper bound.",
              "One of the foundational strategies is using materials that can withstand extreme temperatures and resist corrosion due to moisture.",
              "**Model Predictive Control (MPC):** This approach predicts future states and adjusts controls accordingly.- **Adaptive Control:** These systems adjust their parameters in real-time based on sensor feedback to maintain stability.",
              "Effective for obstacle detection in poor visibility.- **Inertial Measurement Units (IMUs):** Help maintain orientation and position even when GPS signals are weak.",
              "Combining data from multiple sensors can help correct errors and provide a more comprehensive understanding of the UAV's environment.",
              "Adaptive Terminal Sliding Mode Control**: This strategy, based on characteristic modeling, improves attitude control accuracy and robustness by handling nonlinear dynamics and unmodeled disturbances in UAVs[1].",
              "Adaptive Robust Dynamic Surface Integral Sliding Mode Control (ADSISMC)**: This approach combines dynamic surface and integral sliding mode control to improve trajectory tracking under parametric uncertainties and external disturbances[5].",
              " Using Inertial Measurement Units (IMUs) designed to operate accurately in extreme temperatures and high vibration environments [9].",
              "Integrating optical flow sensors for improved position estimation in low-visibility conditions [10].",
              "Kalman Filtering: Applying advanced Kalman filtering techniques to fuse data from various sensors, providing more robust state estimation [11].- GPS-INS Integration: Tightly coupling GPS and Inertial Navigation System (INS) data to maintain accurate positioning even when GPS signals are compromised [12].",
              " Fuzzy logic-based approaches, such as the robust fuzzy backstepping sliding mode controller (RFBSMC), have shown high performance in terms of stability, trajectory tracking, and robustness against external disturbances and parameter uncertainties  (7, Khebbache et al., 2013). Similarly, fuzzy PID control methods based on ant colony algorithms have demonstrated effectiveness in reducing the influence of perturbations and stabilizing motion patterns under wind disturbance conditions  (80, Tang, 2024).",
              "The integration of deep neural networks (DNNs) with robust linear quadratic regulators (RLQR) has improved flight performance during trajectory tracking, particularly when subjected to wind gusts  (83, Benevides et al., 2024). Additionally, the combination of neural networks with interval type-2 fuzzy logic control and sliding mode control (NNIT2FSMC) has shown promise in alleviating chattering effects and maintaining robustness against external disturbances  (15, Bouguerra et al., 2017).",
              "The integration of artificial intelligence techniques with disturbance observer-based feedback linearization has proven effective in improving disturbance approximation and compensation, resulting in more robust flight control under various conditions  (19, Lazim et al., 2018). Active disturbance rejection control (ADRC) strategies, particularly when combined with fuzzy logic, have shown stronger stability and robustness compared to classical ADRC and PID control methods  (74, Liu et al., 2023).",
              "The use of remote wind measurements from LiDAR systems has enabled the development of wind-preview-based Model Predictive Controllers (MPC) for improved disturbance rejection  (67, Whidborne et al., 2023)."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide limitations associated with designing the robust and safe solutions for quadrotor UAVs considering extreme weather conditions.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Furthermore, as a consumable hardware device, a QUAV is often equipped with low-cost and poor-quality components. These situations may lead to system failures and decrease the safety of QUAV."
            ]
          }
        ]
      }
    },
    "case_id": "79eda5689229572a789e612010246d44",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "Are there papers that use different formats of Q&A with the user to clarify intent and compose more complicated prompts to LLM?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "Are there papers that use different formats of Q&A with the user to clarify intent and compose more complicated prompts to LLM?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with providing relevant paper and a short text on what the authors are proposing in terms of using different formats of Q&A with the user to clarify intent and compose more complicated prompts to LLM",
            "weight": 0.24,
            "evidence": [
              "To harness the abilities of LLMs, while still ensuring a high level of precision towards the intent of the learners,this paper proposes an approach to utilize knowledge graphs (KG) as a source of factual context, for LLM prompts, reducing the risk of model hallucinations, and safeguarding against wrong or imprecise information, while maintaining an application-intended learning context. We utilize the semantic relations in the knowledge graph to offer curated knowledge about learning recommendations. With domain-experts in the loop, we design the explanation as a textual template, which is filled and completed by the LLM.Abu-Rasheed, H., Weber, C., & Fathi, M. (2024). Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations.arXiv preprint arXiv:2403.03008.",
              "In this paper, we study the power of LLMs to build task-oriented chatbots, resulting in lighter specifications - no intent definition required - and more natural conversations than in intent-based approaches. To this end, we propose a lightweight domain-specific language based on YAML to specify chatbots using modules of different types (e.g., menus, question-answering, data gathering). These specifications are compiled into structured LLM prompts that use the ReAct framework to inform our runtime how to interpret the user input and coordinate the tasks that the chatbot must perform.Sanchez Cuadrado, J., Perez-Soler, S., Guerra, E., & De Lara, J. (2024, July). Automating the Development of Task-oriented LLM-based Chatbots. InProceedings of the 6th ACM Conference on Conversational User Interfaces(pp. 1-10).",
              "we identified four key phases in the human-LLM interaction flow--planning, facilitating, iterating, and testing--to precisely understand the dynamics of this process. Additionally, we have developed a taxonomy of four primary interaction modes: Mode 1: Standard Prompting, Mode 2: User Interface, Mode 3: Context-based, and Mode 4: Agent Facilitator. This taxonomy was further enriched using the \"5W1H\" guideline method, which involved a detailed examination of definitions, participant roles (Who), the phases that happened (When), human objectives and LLM abilities (What), and the mechanics of each interaction mode (How). We anticipate this taxonomy will contribute to the future design and evaluation of human-LLM interaction.Gao, J., Gebreegziabher, S. A., Choo, K. T. W., Li, T. J. J., Perrault, S. T., & Malone, T. W. (2024, May). A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration. InExtended Abstracts of the CHI Conference on Human Factors in Computing Systems(pp. 1-11).",
              "The proposed work's novelty stems from the method for generating purpose-driven user intent taxonomies with strong validation. This method not only helps remove methodological and practical bottlenecks from intent-focused research, but also provides a new framework for generating, validating, and applying other kinds of taxonomies in a scalable and adaptable way with reasonable human effort.Shah, C., White, R. W., Andersen, R., Buscher, G., Counts, S., Das, S. S. S., ... & Yang, L. (2023). Using large language models to generate, validate, and apply user intent taxonomies.arXiv preprint arXiv:2309.13063.",
              "There are methodologies that adopt hierarchical models wherein a user's simple initial queries are expanded into more complex ones through interactive layers. For example, Clark et al. (2022) explored how hierarchical prompt engineering can build complex queries incrementally through multiple Q&A rounds to leverage the full capabilities of LLMs [3].",
              "In the paper by Zamani et al. (2022), collaborative techniques were explored where users and systems work together to refine search intents via continuous Q&A loops. The approach enabled the creation of specific and complex task queries that significantly improved the quality of the outputs generated by LLMs [4].",
              "In educational technology, systems utilizing interactive Q&A formats have been explored to help students clarify their learning goals and generate personalized study plans. An example is the work by Price et al. (2021) on a tutoring system that uses multiple rounds of questions to better align with student intentions and provide tailored educational content [6].",
              "Some approaches focus on decomposing complex problems. Hauer et al. suggest using an orchestrating LLM to interact with users and break down vague problems into more manageable components  (9, Hauer et al., 2024). This method allows the LLM to ask follow-up questions and gain a deeper understanding of user requirements.",
              "[(2, Kuhn et al., 2022) | n_citations: 19 | CLAM: Selective Clarification for Ambiguous Questions with Generative Language Models ]:We introduce CLAM: a framework for getting language models to selectively ask for clarification about ambiguous user questions... we show that we can prompt language models to detect whether a given question is ambiguous, generate an appropriate clarifying question to ask the user, and give a final answer after receiving clarification... This lets us automatically evaluate multi-turn clarification dialogues."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide limitations associated with using different formats of Q&A with the user to clarify intent and compose more complicated prompts to LLM",
            "weight": 0.24,
            "evidence": [
              "One of the challenges in these interactive systems is dealing with ambiguous or incomplete user inputs. Future research may focus on improving the robustness of these systems through advanced natural language understanding (NLU) techniques.",
              "Ensuring that these interactive Q&A systems can scale efficiently while maintaining high performance remains an active area of research. Techniques such as contextual compression and real-time feedback loops are currently being developed to address these challenges."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should begin with providing the limitations of basic LLMs.",
            "weight": 0.12,
            "evidence": [
              "Through multiple experiments on two widely-used datasets in the field of conversational recommendation, we highlight several issues with the current evaluation methods for user simulators based on LLMs: (1) Data leakage, which occurs in conversational history and the user simulator's replies, results in inflated evaluation results. (2) The success of CRS recommendations depends more on the availability and quality of conversational history than on the responses from user simulators. (3) Controlling the output of the user simulator through a single prompt template proves challenging.",
              "Recently, the advances in generative artificial intelligence fostered by Large Language Models (LLMs) have enabled a new range of open-domain chatbots, like ChatGPT, able to converse fluently on any topic. However, they are general-purpose, and therefore not directly usable to solve specialised tasks reliably.",
              "To understand user intents from log data, we need a way to label them with meaningful categories that capture their diversity and dynamics. Existing methods rely on manual or machine-learned labeling, which are either expensive or inflexible for large and dynamic datasets."
            ]
          }
        ]
      }
    },
    "case_id": "e03c49cccc971cf3ae67556554b4666b",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What theories from psychology are testable on a platform like mturk but haven't yet been studied in the HCI literature?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What theories from psychology are testable on a platform like mturk but haven't yet been studied in the HCI literature?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with introducing MTurk.",
            "weight": 0.3,
            "evidence": [
              "Amazon.com's Mechanical Turk (MTurk) is an online, web-based platform that started in 2005 as a service to allow researchers to \"crowdsource\" labor-intensive tasks for workers registered on the site to complete for compensation.",
              "MTurk has rapidly become a source of subjects for experimental research and survey data for academic work, as its representativeness, speed, and low cost appeal to researchers."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide theories from psychology which are testable on a platform like mturk but haven't yet been studied in the HCI literature",
            "weight": 0.3,
            "evidence": [
              "Across six studies conducted online using MTurk samples, we observed no effect of control manipulations on conspiracy theory beliefs, while replicating previously reported correlational evidence of their association. The results suggest that conspiracy beliefs are not suitable for compensating for threats to control.",
              "It is important to extend research beyond these samples to examine workplace phenomena that are specific to special populations. We contribute to this argument by noting the particular usefulness that MTurk can provide for sampling from hard-to-reach populations, which we characterize as groups that are in the numerical minority in terms of nationwide representation.",
              "Amazon Mechanical Turk is an increasingly popular data source in the organizational psychology research community.",
              "Amazon's Mechanical Turk (MTurk) is one of the more widely used eLancing (or crowdsourcing) options in the behavioral sciences and, as we show below, it is increasingly used within the organizational psychology research community.",
              "The potential role of brief online studies in changing the types of research and theories likely to evolve is examined in the context of earlier changes in theory and methods in social and personality psychology, changes that favored low-difficulty, high-volume studies. An evolutionary metaphor suggests that the current publication environment of social and personality psychology is a highly competitive one, and that academic survival and reproduction processes (getting a job, tenure/promotion, grants, awards, good graduate students) can result in the extinction of important research domains. Tracking the prevalence of brief online studies, exemplified by studies using Amazon Mechanical Turk, in three top journals (Journal of Personality and Social Psychology, Personality and Social Psychology Bulletin, Journal of Experimental Social Psychology) reveals a dramatic increase in their frequency and proportion.",
              "Trauma researchers have recently begun using Amazon's Mechanical Turk (MTurk) as a data collection platform that is both time- and cost-efficient.",
              "Motive Disposition Theory (MDT) is a psychological theory that has been largely overlooked in HCI research. It offers potential insights into individual differences in motives and the interplay between implicit and explicit motivations.",
              "MDT is a prominent motivation theory that focuses on human needs and offers several unique perspectives that could enrich HCI research.",
              "Psychological Reactance TheoryThis theory suggests that when people feel their freedom is threatened, they react to restore that freedom. In an MTurk context, researchers could examine how different task constraints or reward structures affect workers' sense of autonomy and their subsequent task performance or engagement.",
              "Construal Level TheoryThis theory proposes that psychological distance influences individuals' thoughts and behavior. On MTurk, researchers could investigate how the perceived distance between workers and requesters (in terms of social, temporal, or spatial distance) affects task performance, engagement, or decision-making processes.",
              "Self-Determination TheoryWhile aspects of this theory have been studied in HCI, there's potential for more in-depth exploration on MTurk. Researchers could examine how different task designs or feedback mechanisms on the platform influence workers' intrinsic motivation, competence, and relatedness needs.",
              "Cognitive Load TheoryThis theory could be applied to study how different task presentations or instructions on MTurk affect workers' cognitive load and subsequent performance. Researchers could explore optimal ways to design tasks that balance cognitive demands with efficiency and accuracy.",
              "Social Information Processing TheoryIn the context of MTurk, this theory could be used to examine how workers form impressions of requesters or other workers based on limited online interactions, and how these impressions influence their work behavior or platform engagement.",
              "Regulatory Focus TheoryThis theory distinguishes between promotion focus (oriented towards gains) and prevention focus (oriented towards avoiding losses). On MTurk, researchers could investigate how framing tasks or rewards in terms of gains or losses affects worker motivation and performance.",
              "Terror Management TheoryWhile less directly applicable, this theory could be used to explore how subtle reminders of mortality affect workers' task choices, performance, or engagement on the platform."
            ]
          }
        ]
      }
    },
    "case_id": "d28ae3b3ee305bf7a2f3a4fc3ed68331",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What performance metrics are typically used for evaluating differential privacy systems?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What performance metrics are typically used for evaluating differential privacy systems?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with introducing differential privacy systems.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Differential privacy (DP) defines privacy protection by promising quantified indistinguishability between individuals who consent to share their privacy-sensitive information and those who do not. DP aims to deliver this promise by including well-crafted elements of random noise in the published data, and thus there is an inherent tradeoff between the degree of privacy protection and the ability to utilize the protected data.",
              "Differential privacy allows us to quantify cumulative privacy loss as data are analyzed and re-analyzed, shared, and linked",
              "Differential privacy (DP) is a mathematical framework for privacy-preserving data analysis. Evaluating DP systems involves assessing both their privacy guarantees and utility. The following sections outline the key performance metrics used in this evaluation process."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide the performance metrics which are typically used for evaluating differential privacy systems.",
            "weight": 0.19999999999999998,
            "evidence": [
              "A compendium of results, colloquially known as the Fundamental Law of Information Recovery,  tells  us  that  overly  accurate  answers  to  too  many  questions  can  destroy  any reasonable  notion  of  privacy.",
              "To understand the process of choosing in practice, we conducted an empirical study of institutions known to be utilizing some form of differential privacy in 2016.",
              "Paths of privacy loss. What are the paths of privacy loss?  What are the uses ofthe data during which information regarding one person can affect the experience of, or otherwise be exposed to,  another person?",
              "Epsilon per datum. The pathways describe the avenues for privacy leakage, that is, the uses of the data that leak information; the granularity describes the unit of information that is protected.",
              "Burn  rate. Many  analyses,  such  as  trend/popularity  monitoring,  are  carried  out continually.  Data involved in these analyses may maintain their importance or may be down- weighted as they age.",
              "Privacy loss allowed before retirement. Are data \"retired\" after they have reached a certain degree of exposure (e.g., worst-case or expected cumulative privacy loss?)  What is the threshold for retirement?",
              "For the Netflix dataset, we train the recommender system on the qualifying set and evaluate it on the test set released for the Netflix Challenge. In the experiments reported in the rest of this section, we focus on the predictive accuracy of the recommendations. Hence, we use the Root Mean Square Error (RMSE) metric to measure the accuracy of the predicted ratings.",
              "we compare the results of the privacy-preserving MF approach to two other privacy preserving recommendation approaches: private version of the GE baseline and private k-nearest neighbors (kNN) recommendation algorithm.",
              "we proposed four differentially-private approaches to MF and evaluated three of them: in input perturbation, the data was protected by introducing noise prior to the data analysis, while in two other investigated approaches, differentially private variants of ALS and gradient descent were employed to guarantee privacy.",
              "we use FLEX to evaluate the impact of differential privacy on 9862 real-world statistical queries in our dataset.",
              "To compute elastic sensitivity we built an analysis framework for SQL queries based on the Presto parser [9], with additional logic to resolve aliases and a framework to perform abstract interpretation-based dataflow analyses on the query tree.",
              "To investigate FLEX's support for the wide range of SQL features in real-world queries, we ran FLEX's elastic sensitivity analysis on the queries in our experiment dataset. We recorded the number of errors and classified each error according to its type.",
              "Specifically, we measure the noise introduced to query results based on whether or not the query uses join and what percentage of the data is accessed by the query.",
              "For each value of  in the set {0.1, 1, 10} (keeping d fixed at n[?]eln n), we computed the median error of each query, as in the previous experiment.",
              "Since wPINQ programs are implemented in C#, we are unable to run wPINQ directly on our SQL query dataset. Instead we compare the utility between the two mechanisms for a selected set of representative queries.",
              "The Count, Sum and Average functions capture many of the calculations utilized in ITS, and can be evaluated accurately with differential privacy, enabling, e.g., Scenario 1. However, Max and Min are also valuable functions (e.g., evaluate the speed of the slowest and fastest vehicles in a road section), but have high global sensitivity. Consequently, applying the Laplace mechanism as in Theorem 2.1 to evaluate these functions would provide useless results",
              "Since di|erential privacy maintains composability, it is possible to monitor the overall privacy loss (a worst-case evaluation) and bound it.",
              "In a nutshell, this mechanism achieves heterogeneous differential privacy by manipulating the sensitivity of the function using a linear transformation on the input domain. Finally, we evaluate on real datasets the impact of the proposed mechanism with respect to a semantic clustering task.",
              "We have also described a generic mechanism achieving HDP called the Stretching Mechanism, which protects at the same 19 time the items of the profile of user and the privacy vector representing his privacy expectations across items of the profile. We applied this mechanism for the computation of the cosine similarity and evaluate its impact on a distributed semantic clustering task by using the recall as a measure of utility.",
              "The most fundamental metric in differential privacy is epsilon (e), also known as the privacy budget or privacy loss parameter.- Lower e values indicate stronger privacy guarantees.- Typically, e values between 0.1 and 10 are considered practical for real-world applications [1].",
              "In (e, d)-differential privacy:- d represents the probability of violating the e-differential privacy guarantee.- Typically, d should be very small (e.g., d < 1/n, where n is the number of records in the dataset) [2].",
              "Privacy loss can be measured using various techniques:- Max divergence- KL divergence- Renyi differential privacy [3]",
              "Accuracy measures how close the differentially private results are to the true results:- Mean Absolute Error (MAE)- Mean Squared Error (MSE)- Root Mean Squared Error (RMSE)",
              "For specific types of queries:- L1 error: sum of absolute differences between true and noisy answers- L2 error: Euclidean distance between true and noisy answers [4]",
              "For classification tasks, the F1 score balances precision and recall:F1 = 2 * (precision * recall) / (precision + recall)",
              "**Privacy accounting methods**: Techniques to track cumulative privacy loss over multiple queries or iterations[13].",
              "Privacy Leakage: Some evaluations measure privacy leakage, often in the context of specific attacks like membership inference. This can be quantified as the difference between true positive and false positive rates of such attacks  (61, Lu et al., 2022).",
              "The utility of a differentially private system refers to the degree to which the modified data remains useful for analysis. Utility metrics are essential to ensure that privacy-preserving transformations do not excessively degrade the data's value.",
              "**AUC-ROC for Classification Models**: Evaluates the performance of machine learning models trained on differentially private data, particularly useful in binary classification tasks."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide the importance of using these performance metrics for evaluating differential privacy systems.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Evaluating differential privacy systems necessitates a comprehensive understanding of multiple performance metrics that collectively inform us about the privacy guarantees, utility, efficiency, scalability, and robustness of the system. By analyzing these metrics, practitioners can better design and implement differential privacy systems that balance security, practicality, and user needs."
            ]
          }
        ]
      }
    },
    "case_id": "f5d0906eb95bf278d7c5b55a1de0644c",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What data analysis tool are the most accurate for an appraisal of a mixed methodology research study?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What data analysis tool are the most accurate for an appraisal of a mixed methodology research study?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should provide a background on mixed methodology research study.",
            "weight": 0.19999999999999998,
            "evidence": [
              "There are three recognised methods for conducting research: quantitative, qualitative and mixed methods. Mixed methods research encourages researchers to use multiple approaches to collecting and analysing data within a single study, recognising the limitations of using a single method.",
              "The MMAT was initially developed in 2006 based on a literature review on systematic reviews combining qualitative and quantitative evidence. It was subject to pilot and interrater reliability testing.:https://content.iospress.com/articles/education-for-information/efi180221\"Mixed methods systematic reviews provide a more complete basis for complex decision-making than that currently offered by single method reviews, thereby maximizing their usefulness to clinical and policy decision-makers.",
              "Mixed methods (MM) involve combining qualitative (QUAL) and quantitative (QUAN) methods in program evaluation, primary research, and literature review."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide the analysis tools which are accurate for an appraisal of mixed methodology research study.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Mixed Methods Appraisal Tool is a tool for evaluating primary mixed methods studies based on five evaluation criteria: 1) justification for adopting mixed methods methodology; 2) integration between the quantitative and qualitative components; 3) interpretation of integrated findings of quantitative and qualitative data; 4) presentation of divergences between quantitative and qualitative results; and 5) compliance with the methodological rigor of each individual approach in mixed methods research.",
              "After the integration of qualitative and quantitative results, the findings need to be interpreted based on the similarities and divergences (disagreements, discrepancies or dissonances). A successful integration should result in a view closer to the integral of the phenomenon of interest, rather than the sum of the parts (findings) from each arm in isolation.",
              "when there are no disagreements between quantitative and qualitative results, the reviewer rates \"Yes\". However, if disagreements arise, to rate this criterion as \"Yes\", a clear explanation is needed as to how such disagreements are handled and interpreted.",
              "To evaluate the quantitative arm, use MMAT Part I: study types 2 (randomized controlled trails), 3 (quantitative non-randomized) or 4 (quantitative descriptive), accordingly and use study type 1 for the qualitative arm. Both arms need to meet completely the criteria of scientific rigor to be considered of good quality for the mixed methods study. When both arms are rated different for the level of quality, the overall quality of the mixed methods study must be assigned to the lower rating.",
              "Table of Specifications (ToS) for estimating content validity, both theoretically and in practice. A ToS is operationally defined by a set of procedures that attempts to align a set of items, tasks, or evidence with a set of concepts that are to be assessed. The development and the logic of the ToS require the presentation of transparent evidence to increase the trustworthiness of the validity estimates by maintaining an audit trail.",
              "a ToS is a set of procedures that attempts to align a set of items, tasks, or evidence with a set of concepts that are to be assessed. That is, it is a versatile procedure designed to achieve alignment between a particular concept or construct and its assessment instrument.",
              "One means by which qualitative and quantitative data can be integrated during analysis is to transform one data type into the other to allow for statistical or thematic analysis of both data types together. ",
              " Typology Development--The analysis of one data type yields a typology (or set of substantive categories) that is then used as a framework applied in analyzing the contrasting data type.",
              " Extreme Case Analysis--\"Extreme cases\" identified from the analysis of one data type and pursued via (additional data collection and) analysis of data of the other type, with the intent of testing and refining the initial explanation for the extreme cases.",
              "Data Consolidation/Merging--The joint review of both data types to create new or consolidated variables or data sets, which can be expressed in either quantitative or qualitative form. These consolidated variables or data sets are then typically used in further analyses.",
              "Quantitizing refers to a process by which qualitative data are treated with quantitative techniques to transform them into quantitative data. The researcher must first reduce verbal or visual data (e.g., from interviews, observations, artifacts, or documents) into items, constructs, or variables that are intended to mean only one thing and that can, therefore, be represented numerically.",
              "The promise of the nested research design is that both LNA and SNA can inform each other to the extent that the analytic payoff is greater than the sum of the parts. Not only is the information gleaned complementary, but also each step of the analysis provides direction for approaching the next step. Most prominently, LNA provides insights about rival explanations and helps to motivate case selection strategies for SNA, whereas SNA helps to improve the quality of measurement instruments and model specifications used in the LNA.",
              "When appraising a mixed methodology study, it's crucial to consider:a) The research design (concurrent, sequential, or transformative)b) The relative weight given to qualitative and quantitative datac) The integration point of the two methodologiesd) The purpose of mixing methods (triangulation, complementarity, development, initiation, or expansion)",
              "The most accurate data analysis tools for appraising a mixed methodology research study depend on the specific nature of the study and the researcher's needs. However, integrated platforms like MAXQDA and Dedoose, combined with specialized qualitative (NVivo, ATLAS.ti) and quantitative (SPSS, R) tools, generally provide the most comprehensive and accurate analysis capabilities.",
              "The concept of multi-method evaluations suggests that researchers can employ a diverse set of analytical tools, each suited to different aspects of their data. For instance, a study might use statistical software for quantitative analysis, qualitative coding software for textual data, and visualization tools for presenting integrated results. This approach can provide a more nuanced and comprehensive understanding of the research subject (LLM MEMORY, 2024).",
              "One notable approach that has gained traction is the quantitizing mixed methods approach, exemplified by usage feature analysis in linguistics. This method provides a flexible and replicable framework for analyzing qualitative data in a systematic quantitative manner.",
              "GRATIS is an open-access software designed to analyze and visualize the chronological evolution of qualitative data. It uses dynamic network visualizations and data mining tools to provide a rich, time-based analysis of textual data without extensive data preparation. This tool is particularly useful for studies that need to maintain the contextual integrity of qualitative data over time, making it a robust choice for mixed methods research that includes longitudinal qualitative data.",
              "The DARC framework integrates computational science and social science methods to enhance risk communication. It uses natural language processing (NLP) techniques to ensure the validity, reliability, and precision of messages. This framework is modular and can be adapted to various research contexts, making it a versatile tool for mixed methods studies that require precise communication and risk assessment",
              "For mixed methods research, AI tools can enhance both qualitative and quantitative analysis by providing deeper insights and more accurate predictions.",
              " **NVivo**: Widely used for qualitative data analysis, NVivo supports the integration of qualitative and quantitative data, allowing researchers to manage, analyze, and visualize data from multiple sources.",
              "- **MAXQDA**: Similar to NVivo, MAXQDA is designed for mixed methods research and offers tools for coding, analyzing, and visualizing qualitative and quantitative data.- **R and Python**: These programming languages offer extensive libraries for both qualitative and quantitative data analysis, including text mining, statistical analysis, and machine learning."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide limitations and challenges in data analysis tool used for an appraisal of a mixed methodology research study.",
            "weight": 0.19999999999999998,
            "evidence": [
              "\"Mixed methods research typically involves the integration of quantitative and qualitative methodologies to provide a comprehensive analysis of complex phenomena. However, the scope of data analysis tools for mixed methods research extends beyond this traditional dichotomy. According to Bauer, multi-method evaluations can encompass a broader range of combinations, including multiple quantitative methods, multiple qualitative methods, or a mix of both."
            ]
          }
        ]
      }
    },
    "case_id": "db4211ada9ca179cddb052bf63e732a1",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "How is artificial intelligence being utilized to enhance the diagnosis and treatment of sleep apnea?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "How is artificial intelligence being utilized to enhance the diagnosis and treatment of sleep apnea?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with introducing sleep apnea.",
            "weight": 0.13333333333333333,
            "evidence": [
              "Apnea is a sleep disorder that stops or reduces airflow for a short time during sleep. Sleep apnea may last for a few seconds and happen for many while sleeping. This reduction in breathing is associated with loud snoring, which may awaken the person with a feeling of suffocation.",
              "Obstructive sleep apnea (OSA) is a clinical condition characterized by a cessation of airfow due to upper airway obstruction with simultaneous respiratory efort present.",
              "Artificial Intelligence (AI) is increasingly being utilized to enhance the diagnosis and treatment of sleep apnea, a common sleep disorder characterized by repeated interruptions in breathing during sleep."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide the information on utilizing artificial intelligence to enhance the diagnosis of sleep apnea.",
            "weight": 0.13333333333333333,
            "evidence": [
              "So far, a variety of methods have been introduced by researchers to diagnose sleep apnea, among which the polysomnography (PSG) method is known to be the best. Analysis of PSG signals is very complicated. Many studies have been conducted on the automatic diagnosis of sleep apnea from biological signals using artificial intelligence (AI), including machine learning (ML) and deep learning (DL) methods.",
              "The preprocessing is an important step for the biological signals used in the CADS based on AI and are divided into two category of low-level and high-level techniques. The lower-level methods in preprocessing the biological signals include steps like noise removal, baseline correction, segmentation, and normalization (Shoeibi et al., 2022). These methods by improving the performance of CADS based on AI play an important part in the diagnosis of sleep apnea.",
              "In addition, researchers use a number of advanced preprocessing methods to increase the performance, which are called high-level preprocessing. In the CADS based on ML, the high-level preprocessing techniques often include the methods in the domains of frequency or time-frequency.",
              "The high-level preprocessing techniques in the time-frequency domain are include empirical mode decomposition (EMD) (Rilling et al., 2003), discrete wavelet transform (DWT) (Shensa, 1992), continues wavelet transform (CWT) (Aguiar-Conraria & Soares, 2014), wavelet coefficients thresholding (WCT) (Abramovich & Benjamini, 1996), and bivariate fast and adaptive EMD (FAEMD) (Thirumalaisamy & Ansell, 2018), which are used in sleep apnea detection.",
              "The feature extraction is the most important section in the CADS based on AI for sleep apnea detection, and these methods are divided into four categories in ML: time-domain, frequency-domain, time-frequency domain, and nonlinear.",
              "SAMOA, belongs to this category of help tools, being an automatic SAS diagnostic system that incorporates both conventional programming and artificial intelligence techniques.",
              "In its initial phase, a set of pneumological and electrophysiological signals are analyzed separately in order to identify and extract their principal features. Data segmentation processes are then applied to each signal in order to obtain a sequence of intervals of variable duration. In this way a temporal reference framework based on points is replaced by one based on intervals, with each interval symbolically labeled in accordance with previously established clinical criteria for the domain.",
              "In the initial phase the most relevant characteristics of the signal are extracted and a segmentation based on significant time intervals of variable length is carried out. The intermediate phase consists of assigning suitable labels to these intervals and combining this symbolic information with contextual information in order to build more complex structures that will identify clinically significant events. Finally, at a level close to knowledge, a rule-based system brings together all the events information available and produces a set of conclusions.",
              "A phenotyping approach can provide further insight into OSA severity and physiology and is an important step in personalizing medicine. Machine learning (ML) can be applied to identify phenotypes or previous unidentifed patterns and complement the development of alternative metrics to quantify and describe OSA, rather than solely by AHI.",
              "A study also employed supervised ML (decision tree learner) to develop a predictive algorithm for OSA endotypes from PSG and clinical data [13]. Such knowledge of the endotypes, upper-airway collapsibility, arousal threshold, loop gain and pharyngeal muscle responsiveness is essential to inform treatment avenues.",
              "CNNs are widely used for sleep apnea detection, analyzing various physiological signals. For instance, 1D-CNNs have been applied to single-lead ECG signals  (30, Gandhi et al., 2023), while LeNet-5 architecture has been used for automatic feature extraction from RR intervals  (72, Lu et al., 2019). MobileNet V1, a lightweight CNN, has also been employed for efficient OSA detection  (218, Hemrajani et al., 2023).",
              " RNNs, particularly Long Short-Term Memory (LSTM) networks, are used to capture temporal dependencies in sleep data. A deep RNN framework has been developed for automated feature extraction and apnea event detection from respiratory signals  (91, Penzel et al., 2020).",
              "Combinations of different neural network architectures have shown promising results. For example, a dense recurrent convolutional neural network (DRCNN) has been constructed using multiple dense convolutional units followed by a bidirectional LSTM layer  (3, Pourbabaee et al., 2019).",
              "Some approaches utilize multiple physiological signals simultaneously. Bernardini et al. proposed a deep learning framework based on CNNs that uses both ECG and oxygen saturation data  (8, Bernardini et al., 2021).",
              "Novel approaches like SlAction use infrared videos to detect OSA in daily sleep environments, employing machine learning techniques to analyze human motions during sleep."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide the information on utilizing artificial intelligence to enhance the treatment of sleep apnea",
            "weight": 0.13333333333333333,
            "evidence": [
              "\"Artificial intelligence has the capacity to improve the treatment of OSA through predicting outcomes of treatment options, evaluating the treatment the patient is currently utilizing and increasing understanding of the mechanisms that contribute to OSA disease process and physiology. Implementing AI in guiding treatment decisions allows patients to connect with treatment methods that would be most effective on an individual basis.",
              "A study utilized ML to compare the compliance with CPAP therapy of patients with OSA at different points of treatment by building classifiers. The trial showed that month 3 was the time-point with the most accurate classifier of 84% in cross-validation and testing. Four variables (headaches, psychological symptoms, arterial hypertension and EuroQol visual analog scale) were reported relevant for prediction of CPAP compliance at each time point. Epworth and average nighttime hours were found the be important in prediction at months 1 and 3.",
              "Another study analyzing prediction of obstruction sites in OSA patients based on snoring sound parameters found the accuracies ranged from 60.4 to 92.2%. The study additionally concluded that snoring sound analysis does not seem to be a viable diagnostic modality for treatment selection.",
              "Adenotonsillectomy (AT) is the main treatment approach for pediatric OSA. A study demonstrated how ML can be used to predict surgical candidates for pediatric OSA in order to help avoid unnecessary surgery in children with OSAS. An unsupervised ML technique, the K-means clustering method, was used to stratify patients into two groups depending on physiological and neurophysiological symptoms.",
              "Tis predictive model suggested individuals at high risk for OSA showed extensive activation of immune cells and pathways and higher expression of these genes which decreased after CPAP treatment. Te information provided by ML in this setting can improve the identifcation of people with high risk of OSA as well as insight into CPAP treatment individual beneft.",
              "Another study also used ML to identify biomarkers of the presence and severity of OSA, PAI-I, tPA and sE-Selectin, and demonstrated that they reduce with CPAP treatment and therefore may provide a measure of treatment response and guide preventative cardiovascular management for those patients identifed as higher risk.",
              "ML and analysis of blood-based biomarkers can provide insight into the treatment of OSA and play a role in personalized management. A study highlighted 4 metabolites-signatures that provide an accuracy of 0.98 for OSA detection, and demonstrated a signifcant modulation of plasma metabolites previously altered by OSA following 6 months of CPAP therapy.",
              "Using ML to increase the understanding of OSA physiology and etiology, particularly the location of upper airway collapse, can subsequently improve treatment selection and outcome. Te ML approach with a model using linear discriminants to analyze audio signal was demonstrated to have fair accuracy in discriminating tongue and non-tongue collapse with overall accuracy of 81% and 64% accuracy for all sites of collapse classes.",
              "Continuous Positive Airway Pressure (CPAP) is a common treatment for sleep apnea. AI algorithms are being used to analyze data from CPAP machines to optimize pressure settings and improve treatment efficacy [6].",
              "AI-powered smart devices and wearables are being designed to monitor sleep patterns, detect apnea events, and provide real-time interventions, such as positional therapy prompts or stimulation to prevent airway collapse [8]."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should provide the list of datasets which can be used to train a model which would provide sleep apnea diagnosis and treatment.",
            "weight": 0.13333333333333333,
            "evidence": [
              " College Dublin sleep apnea database (UCD): The UCD dataset contains 25 PSG signals from adult cases with apnea disorder (Goldberger et al., 2000). The cases were selected in a six-month period from the cases visiting the sleep disorders hospital of St Vincent University for the diagnosis of apnea, OSA, or primary snoring. The number of cases was 25, out of which 21 were male, and four were female. In this dataset, there are various data: ECG, EMG, EOG, EEG, Thermistor, ribcage movements, abdomen movements, finger SpO2, snoring (tracheal microphone), and body position.",
              "Apnea-ECG database: This dataset includes different signals from 70 cases which is equally divided for training and test (Kemp et al., 2000; Penzel et al., 2000). The signals recording was done in 7-10 h. Each signals recording includes an ECG signal, a set of interpretations of apnea by a human expert (according to the simultaneous recording of respiration and signal), and a set of QRS interpretations. In addition, eight records are accompanied by four extra signals, which include respiration signals from the chest, abdomen, nose airflow, and SpO2.",
              "Sleep-EDF database expanded: In this dataset, PSG recording has been performed from 198 cases, which includes EOG (horizontal), EEG (Fpz-Cz and Pz-Oz), EMG, and event marker. In addition, some of the recordings also have rectal body temperature and oro-nasal respiration.",
              " Sleep heart health study PSG database: The Sleep Heart Health Study (SHHS) has been performed by National Heart, Lung, and Blood Institute to investigate the cardiovascular consequences caused by respiration disorders in sleeping (Quan et al., 1997). The performed PSG recording has various signals: EOG, EMG, thoracic and abdominal excursions, nasal-oral airflow, finger-tip pulse oximetry, ECG, heart rate, body position, and ambient light.",
              "MIT-BIH polysomnographic database: The MIT-BIH dataset is a set of several physiological signals during sleep which is recorded in Boston's Beth Israel Hospital Sleep Laboratory (Ichimaru & Moody, 1999). This dataset includes more than 80 h of 4-channel, 6-channel, and 7-channel PSG recordings, each of which has EEG, ECG, and respiration signals.",
              " PhysioNet/CinC 2018 challenge: The biological signals of this challenge are collected by Computational Clinical Neurophysiology Laboratory (CCNL), Massachusetts General Hospital (MGH), and the Clinical Data Animation Laboratory (CDAC). This dataset includes 1985 cases that were under surveillance in the MGH sleep laboratory. The sleep steps have been interpreted by the experts in MGH according to AASM instructions. Various physiological signals like ECG, EMG, EOG, EEG, and SpO2 have been recorded from the cases and are placed in this dataset.",
              "MrOS sleep study: MrOS is a study regarding osteoporotic fractures among males. The sleep study study aims to understand the relationship between sleep disorders and falling, fracture, mortality, and cardiovascular diseases."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should provide the overall treatment of sleep apnea.",
            "weight": 0.06666666666666667,
            "evidence": [
              "The first-line and most common treatment for obstructive sleep apnea is nasal continuous positive airway pressure, which serves as a pneumatic splint to stabilize the upper airway and is effective when used with appropriate adherence. Continuous positive airway pressure compliance rates remain significantly low despite machine improvements and compliance intervention. Other treatment options include oral appliances, myofunctional therapy, and surgery."
            ]
          }
        ]
      }
    },
    "case_id": "e09a30179e0b587d57edb17317ae3288",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What is a good Ontology semantic similarity measure that considers multiple inheritance cases of concepts?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What is a good Ontology semantic similarity measure that considers multiple inheritance cases of concepts?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with providing a background on ontology semantic similarity.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Computation of semantic similarity between words for text understanding is a vital issue in many applications such as word sense disambiguation, document categorization, and information retrieval.",
              "Ontologies, as abstract description systems for domain-specific knowledge composition, hence receive more and more attention in computational biology and bioinformatics. Particularly, many applications relying on domain ontologies require quantitative measures of relationships between terms in the ontologies, making it indispensable to develop computational methods for the derivation of ontology-based semantic similarity between terms.",
              "The knowledge in ontologies may be used to constrain the search for solutions to an optimization problem and thereby finding a solution faster, finding a better solution or finding a solution that is generalized better.",
              "In many biomedical and computer science applications it is useful to determine how similar two concepts are. Measures that compute similarity between concepts are semantic similarity measures, and semantic similarity measures have received renewed interest recently with the development of novel methods based on representation learning. Semantic similarity measures are used to compare words and terms in natural language texts, entities represented in graphs and knowledge graphs and ontology classes based on the knowledge within the ontologies.",
              "Semantic similarity measures exploit knowledge sources as the base to perform the estimations. In recent years, ontologies have grown in interest thanks to global initiatives such as the Semantic Web, offering an structured knowledge representation. Thanks to the possibilities that ontologies enable regarding semantic interpretation of terms many ontology-based similarity measures have been developed.",
              "Semantic similarity measures are vital in various applications, such as information retrieval, natural language processing (NLP), and bioinformatics. These measures assess how similar two concepts are within an ontology, often represented as a structured form of knowledge such as a taxonomy or graph."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide information on good Ontology semantic similarity measure that considers multiple inheritance cases of concepts",
            "weight": 0.19999999999999998,
            "evidence": [
              "The common specificity feature considers the depth of the Least Common Subsumer (LCS) of two concepts and the depth of the ontology to obtain more semantic evidence.",
              "In a view of an independent domain, in order to get high accuracy, most path-based measures rely on large and general purpose taxonomy. Usually, researchers may choose WordNet to apply these measures because of its perfect structure. However, the coverage of biomedical terms in WordNet is so limited that the accuracy of similarity assessments for medical terms is poor. So Pedersen et al. [11], Al-Mubaid and Nguyen [24], and Batet et al. [25] adopted these measures to the biomedical domain by exploiting SNOMED CT as the input ontology.",
              "Note that there exists the case of multiple inheritance in which concepts may be subsumed by several superconcepts in the largest and most widely used knowledge source such as SNOMED CT, MeSH in the UMLS, or WordNet.",
              "In the experiments, we use SNOMED CT, a large and detailed ontology with multiple inheritance between concepts, as input ontology. The experimental results show that our measure has rather high correlation values with respect to both physicians and coders and the measure outperforms most approaches based on taxonomical structure and IC and context vectors.",
              "exclusively inherited shared information (EISI) that captured the information shared by two terms based on an intuitive observation on the multiple inheritance relationships among the terms in the GO graph. EISI was derived from the information content of the exclusively inherited common ancestors (EICAs), which were screened from the common ancestors according to the attribute of their direct children.",
              "The promising features of EISI are the following: (1) it provides a more effective way to characterize the semantic relationship between two GO terms by taking into account multiple common ancestors related, and (2) can quickly detect all EICAs with time complexity ofO(n), which is much more efficient than other methods based on disjunctive common ancestors.",
              "To address the problem caused by multiple inheritance, Couto et al. employed the concept of disjunctive common ancestors and defined a graph-based similarity measure (GraSM) (Couto et al., 2007), where the information two terms share was derived from all their disjunctive common ancestors by taking the average of their information content.",
              "They later updated GraSM and proposed a new method, dubbed Disjunctive Shared Information (DiShIn) (Couto and Silva, 2011), to address the computational complexity problem caused by its recursive definition for disjunctive common ancestors and the problem caused by parallel interpretations shared by two terms.",
              "Because ontology can provide explicit concept specification, we establish an ontology-based semantic similarity computation model. This model quantifies the semantic similarity between concepts considering the semantic distance, concept level and the overlapping degree between sets of the hypernyms and hyponyms. The association between the semantic distance and concept level is also presented so that the number of parameters is reduced.",
              "constructing fuzzy ontologies from fuzzy EER models may be valuable to the development of fuzzy ontologies, and in turn the constructed fuzzy ontologies may be useful for reasoning about the fuzzy EER models.",
              "A Measure Combining Superconcepts and Common Specificity**:   This measure integrates both the superconcepts of the evaluated concepts and their common specificity feature. It considers the depth of the Least Common Subsumer (LCS) and the overall depth of the ontology to provide more semantic evidence. By taking into account all superconcepts of the evaluated concepts, it effectively handles multiple inheritance scenarios.",
              " The traditional Wu & Palmer measure is extended to consider all subsumed concepts (ASC) rather than just the LCS. This extension is particularly useful in large and complex taxonomies with multiple overlapping hierarchies and extensive use of multiple inheritance. By including all relevant superconcepts, this measure captures more explicit knowledge, leading to improved similarity values compared to the original Wu & Palmer measure.",
              "Ontology-based semantic similarity measures typically rely on the \"is-a\" relations found in the underlying taxonomy or ontology  (66, Jiang et al., 2020). These measures can be broadly classified into three categories: path-based, feature-based, and information content (IC) based approaches  (50, McInnes et al., 2016). Path-based methods often utilize graph-based features such as the shortest path length between concepts and the position of their lowest common ancestors to capture semantic similarity  (66, Jiang et al., 2020)."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide the challenges in multiple inheritance.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Multiple inheritance in ontologies occurs when a concept has more than one parent concept. This presents challenges for traditional semantic similarity measures, as they often assume a single inheritance hierarchy. Multiple inheritance can lead to:a) Ambiguity in concept relationshipsb) Increased complexity in similarity calculationsc) Potential overestimation or underestimation of similarity scores"
            ]
          }
        ]
      }
    },
    "case_id": "ef77a4f716eda4a6c980c9d7a56c5bb4",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are the latest techniques for ensuring the reliability of a P2P storage system?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are the latest techniques for ensuring the reliability of a P2P storage system?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with introducing the P2P system.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Peer-to-peer (P2P) storage is a promising technology to provide users with cheap and online persistence.",
              "Peer-to-peer (P2P) storage systems have received a lot of attention in recent years. This interest arises from the idea of aggregating idle disk resources from desktop computers to build decentralized and collaborative storage systems. Such systems encourage users to share local disk resources to obtain online storage capacity."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide recent techniques to ensure reliability of P2P storage system.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Redundancy schemes based on data coding techniques are a common solution for P2P storage systems. Unlike fixedrate codes like Reed-Solomon or Tornado Codes where the maximum number of redundant blocks is initially determined, rate-less erasure codes can generate an unlimited number of redundant blocks.",
              "a storage system using a rate-less erasure code that splits data objects into m redundant blocks. After it, n redundant blocks are generated and each one stored to a different node. Thanks to the redundancy scheme, the storage system ensures that original objects can be always recovered by gathering m out of the n redundant blocks.",
              "when nodes abandon the system, redundancy is reduced and this availability could be threatened. Repair algorithms are the responsible to cope with these departures and ensure that the number of redundant blocks is always kept close to n. Traditionally, these repair algorithms used reactive schemes that triggered the reparation lost blocks as soon as node disconnections were detected.",
              "other reactive schemes differentiated between permanent and transient failures, being able to reintegrate data from temporally unavailable nodes and reducing the total number of repairs.",
              "Unlike reactive algorithms, proactive algorithms schedule the creation of new redundant blocks at a constant rate independently of the actual node departures. By estimating the churn characteristics and setting the repair rate consequently, proactive repair can maintain the number of online nodes close to n avoiding bandwidth spikes.",
              "Under the keep-avail strategy, the proactive repair rate RA is set to ensure that the mean number of online nodes is equal to the targeted nodes, E(Lon) = n. Using Expression (1), they obtained that RA= (an)/u.",
              "Under the keep-stored strategy, the proactive repair rate RSis set to ensure that the mean number of nodes in the system is equal to the targeted nodes, E(Lglobal) = n. Since E(Lglobal) = E(Lon+Loff), using the Cramer's theorem we can obtain the required rate RSable to maintain this targeted number of nodes in the system, RS = (an)/(a(u [?] 1) + 1).",
              "Blockchain technology offers new opportunities for P2P because of its salient features, including decentralization, reliability, stability, and controllability.",
              "Optimization can be conducted taking into account economic aspects, stability and reliability criteria, and the availability of energy, as well as to maximize interests or benefits [20].",
              "the effect of network tariffs on peak load reduction was investigated using a mixed-integer linear programming (MILP) based optimization model developed in a neighborhood with 30 different consumers with flexible loads. The developed multi-energy management system in [15] reduced costs by allowing prosumers to choose from a variety of energy sources and storage methods.",
              "Themis, an accountable P2P cloud storage scheme with smart contracts on Ethereum.",
              "First, cloud data integrity verification is decentralized and implemented by miners on blockchain without any trusted third party. Second, by carefully setting up the reward and punishment mechanism within a smart storage contract, all rational nodes will participate in the storage service following an accountable rule. Third, based on reliable information published on the blockchain, users are free to choose appropriate storage servicers who want to share idle storage, making storage service decentralized and flexible. Fourth, compared with the existing related systems, by adopting a payment at maturity method, the malicious behavior of breaking the contract after the servicer obtains some revenue is prevented, and the availability of user data within the specified period is enhanced.",
              "A pub-sub model is used where storage servicers first publish storage contracts on the blockchain to declare their service, and then users choose an appropriate servicer according to the contract information. Under normal circumstances, data transmission is carried out off-chain. When a dispute arises, i.e., the request sent by a user has no reply or the response is incorrect, the participants request the storage contract to make a ruling.",
              "As the incentive layer of IPFS, Filecoin guarantees the security and stability of the storage process in IPFS. Filecoin consists of three types of entities: storage miners who are responsible for mining and storage; search miners who provide search capabilities; users that store and query data.",
              "Sia is a decentralized cloud storage system similar to Bitcoin, aiming to compete with existing storage solutions in P2P and enterprise areas. There are three types of entities in a Sia system: miners, storage servicers and users. Miners receive rewards based on the PoW mechanism. Storage servicers earn revenues by renting their hard disks. Users obtain the data storage service through the system.",
              "The Sia system automatically generates a verification request for the data storage status at a fixed frequency, requiring storage servicers to provide proof of the correct storage of user data.",
              "the node-based scalable model for Blockchain storage systems (SMBSS) is proposed, which enables a node to have the basic functions with a low-cost storage system while encouraging nodes with adequate storage resources to store more data in order to improve data availability and ultimately break through the storage performance bottleneck and fundamentally solve the scalability issue of Blockchain storage systems.",
              "Replication and redundancy are fundamental techniques used in P2P storage systems to ensure data reliability and availability. These methods involve distributing multiple copies of data across different nodes to mitigate the impact of node failures or user attrition.",
              "To solve this problem and to increase the reliability of the stored data, multiple storage nodes can be networked together to redundantly store the data, thus forming a distributed data storage system.",
              "A notable implementation of DHTs in P2P storage is the Cooperative File System (CFS), which utilizes a distributed hash lookup algorithm called DHash  (6, Stoica et al., 2001). DHash enhances system reliability through several mechanisms:1. Fine-grained block distribution and caching for improved load balancing2. Replication for increased robustness3. Server selection techniques to reduce latency",
              "To further enhance data security and efficiency in blockchain-based P2P storage systems, some solutions incorporate additional techniques. For instance, Huffman compression can be used for file size optimization, while RSA encryption can be applied to ensure data security  (194, Dhingra et al., 2022). These methods contribute to more efficient storage utilization and improved data protection in the distributed environment.",
              "One significant advancement in ensuring data integrity is the concept of multiple-replica provable data possession (MR-PDP), which provides strong evidence that multiple copies of data are actually stored in the system  (32, Khan et al., 2008). This approach helps verify the presence and integrity of replicated data across untrusted storage nodes.",
              "Recent advancements include the development of BFT-DSN, which combines storage-weighted Byzantine Fault Tolerant (BFT) consensus with erasure coding, homomorphic fingerprints, and weighted threshold signatures for decentralized verification  (206, Ranjan et al., 2024). This approach aims to improve Byzantine resilience in decentralized storage networks while maintaining competitive performance in terms of storage cost and latency.",
              "Although still in early stages, quantum key distribution techniques are being explored to provide unconditionally secure key exchange for P2P storage systems [10]."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide challenges associated with P2P storage systems.",
            "weight": 0.17142857142857143,
            "evidence": [
              "However, due the instability of these infrastructures, P2P storage systems must introduce redundancy in order to guarantee a reliable storage service. Besides, they need data repair algorithms to maintain this redundancy in front of permanent node departures. To ensure that such repairs can always be run, existing P2P storage systems aim to maintain 100% data availability. Unfortunately, this solution seems to overkill in preventing data loses, introducing network and data overheads.",
              "these reactive schemes suffer from network spikes caused by data repair processes.",
              "The full redundancy of Blockchain data is intended to guarantee the decentralization characteristic, but brings high storage pressures on nodes and causes the scalability issue of Blockchain storage systems, which is detrimental to decentralization."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should provide the applications of P2P storage systems.",
            "weight": 0.08571428571428572,
            "evidence": [
              "Peer-to-peer (P2P) energy trading in the power distribution system is one of the most effective ways to increase the influence of renewable energy from decentralized generations (DG). As DERs become more popular, traditional energy consumers become customers who can both consume and generate energy [7]."
            ]
          }
        ]
      }
    },
    "case_id": "e55f39332cdc48743ea977d0ec4d0e3e",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are existing methods to elicit user intents when an automation encounters ambiguity, especially in failure cases?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are existing methods to elicit user intents when an automation encounters ambiguity, especially in failure cases?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should provide the methods to extract user intents when an automation encounters ambiguity, especially in failure cases.",
            "weight": 0.3,
            "evidence": [
              "TaskLint is our system for helping task authors to identify ambiguities before the task is posted (i.e., proactive). Like lint tools for code, it identifes possible problems during the process of authoring the instructions. Like grammar checkers and other computer-assisted writing tools, it uses a wide range of NLP methods, drawing on many existing tools.",
              "We present TaskLint, a system to automatically detect problems with task instructions. Leveraging a diverse set of existing NLP tools, TaskLint identifes words and sentences that might foretell worker confusion. This is analogous to static analysis tools for code (\"linters\"), which detect possible features in code that might indicate the presence of bugs.",
              "The study had two stages. In Phase 1, a novice requester created instructions for several task scenarios (na\"ive), used our tool to detect ambiguities in the task instructions, and modifed the instructions based on the feedback from the tool (TaskLint). In Phase 2, we posted their instructions to Amazon Mechanical Turk (AMT) and measured the performance of tasks created (na\"ive, TaskLint).",
              "Failures due to ambiguity occurred when there were multiple reasonable interpretations of a request, and the response was misaligned with what the user intended while still accurately answering the question.",
              "People generally expect to have functional conversations with voice assistants. The lack of social conversations may reduce users' ability to build trust in their voice assistants. Indeed, past research has shown that users trust embodied conversational agents more when they engage in small talk, although this varies by user personality type and level of embodiment of the agent.",
              "Broadly, past research has evaluated how various factors such as accuracy and errors affect people's trust in algorithms.",
              "We reference Herbert Clark's grounding model for human communication, which relies on four different levels to achieve mutual understanding: channel, signal, intention, and conversation.",
              "If users are repeatedly unable to repair failures with voice assistant, this weakens their trust and causes them to reduce their scope of commands to simple tasks with low risk of failure.",
              "In particular, we find that failures due to spurious triggers and ambiguity are less detrimental to user trust than failures due to incorrect action execution, missed triggers, or overcapture.",
              "Robust evidence across two experiments demonstrates that users attribute successful service outcomes internally, while robot-induced service failures are blamed on the firm (and not the robot), confirming the well-known self-serving bias.",
              "While this external attributional shift occurs regardless of the robot design (i.e., it is the same for warm vs. competent robots), the findings imply that service recovery minimizes the undesirable external shift and that this effect is particularly pronounced for warm robots. For practitioners, this implies prioritizing service robots with a warm design for maximizing user retention for either type of service outcome (i.e., success, failure, and failure with recovery).",
              "service recovery through human handover mitigates the deleterious externalization of attribution by shifting back attribution internally and getting users to take their share of responsibility for failures that are created during their interactions with robots.",
              "Specifically, users internalize responsibility for an outcome if the service robot has warm (vs. competent) design features.",
              "While users are indifferent to the type of robot in case of a non-resolved failure, service robot design alters how users attribute responsibility for a recovered failure in that they are more forgiving toward robots with warm design.",
              "varying levels of human involvement in repair strategies depending on the risk factors involved. In high-risk situations, such as those pertaining to safety, users suggested designing protocols that include a preliminary interactive checking process with the user before the robot initiates an automated repair.",
              "Unclear or inaccurate instructions: Users may struggle to articulate their intentions clearly, especially when dealing with complex or unfamiliar tasks. This can lead to ambiguous or imprecise instructions that automated systems find difficult to interpret accurately  (29, Li et al., 2022).",
              "Unexpressed or evolving intentions: Users may not always know exactly what they want at the outset of an interaction. Their intentions can be initially vague or may evolve as they engage with the system, requiring a more dynamic approach to intent recognition  (24, Tanaka et al., 2021).",
              "Complex multi-turn interactions: In real-world settings, determining a user's true intent often requires multiple rounds of dialogue. This is particularly true when dealing with intricate or nuanced requests that cannot be fully captured in a single interaction  (36, Hao et al., 2023).",
              "Changing context and requirements: User intentions can change over time due to shifting contexts or evolving needs. This dynamism poses challenges for context-aware systems that may be designed to provide standardized services based on initial assumptions about user intent  (7, Oyama et al., 2011).",
              " Providing users with better context about what the system knows and how well it can interpolate or extrapolate helps users refine their own intents in complex settings. This method can reveal subtle low-level inconsistencies in desired task requirements that might otherwise lead to undesirable behaviors.",
              "Open-Ended Clarification Requests: To prevent user frustration, systems can generate open-ended requests that explicitly mention two or three of the most likely interpretations  (16, Jackson et al., 2020).",
              "For visual inputs with ambiguity or uncertainty, systems can involve users in resolving these issues, combining automated planning and execution monitoring to track task states and recover from mistakes  (21, Mohomed et al., 2020).",
              "When user intent expressions contain semantic ambiguity, AI/ML techniques become necessary to infer the intended meaning. This approach bridges the gap between expressed and intended intents  (33, Glassman, 2023).",
              "Systems can be designed to identify instances of high aleatoric uncertainty, where user intent is inherently ambiguous, and low epistemic uncertainty, where the system's knowledge is insufficient. This classification helps in determining when and how to seek clarification  (31, Choi et al., 2023).",
              "User interactions with control interfaces can be modeled as probabilistic goal-directed actions. This approach allows systems to reason about user intentions with adjustable rationality, accounting for uncertainty in user inputs  (11, Jain et al., 2019).",
              "XAI methods can help users understand the system's reasoning, which can be crucial in resolving ambiguities:a) Decision Tree Visualization: Showing the logic path that led to a particular interpretation.b) Confidence Scores: Providing transparency about the system's certainty in its understanding.c) Counterfactual Explanations: Showing how different inputs would change the system's interpretation."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide limitations of some of the techniques.",
            "weight": 0.3,
            "evidence": [
              "The performance of TaskLint depends on the accuracy of the NLP tools we use in detecting ambiguities. Most of the NLP tools we used can detect ambiguities occurring only at the word level or at the sentence level.",
              "When faced with ambiguities in a task, humans may intuit what the requester was expecting based on context, common sense, and prior experience with similar tasks. When we post tasks with ambiguities, we may get the correct result even without using any mechanism to disambiguate. Hence, it is very diffcult to show that any new method to disambiguate will produce better accuracy of results signifcantly.",
              "Each method has its strengths and limitations, and an effective system often combines multiple approaches to achieve the best results."
            ]
          }
        ]
      }
    },
    "case_id": "de3b24bf31962c6107002d487f2bab5e",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are the leading techniques for person-following robot navigation (which must track the person and potentially re-identify if occluded)?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are the leading techniques for person-following robot navigation (which must track the person and potentially re-identify if occluded)?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with providing challenges associated with person-following robots.",
            "weight": 0.3,
            "evidence": [
              "Human following is a crucial feature of humanrobot interaction, yet it poses numerous challenges to mobile agents in real-world scenarios. Some major hurdles are that the target person may be in a crowd, obstructed by others, or facing away from the agent.",
              "Robot person following (RPF) is a crucial capability in human-robot interaction (HRI) applications, allowing a robot to persistently follow a designated person. In practical RPF scenarios, the person often be occluded by other objects or people. Consequently, it is necessary to re-identify the person when he/she re-appears within the robot's field of view.",
              "Previous person re-identification (ReID) approaches to person following rely on offline-trained features and short-term experiences. Such an approach i) has a limited capacity to generalize across scenarios; and ii) often fails to re-identify the person when his re-appearance is out of the learned domain represented by the short-term experiences.",
              "Monocular person following (MPF) is a capability that supports many useful applications of a mobile robot. However, existing MPF solutions are not completely satisfactory. Firstly, they often fail to track the target at a close distance either because they are based on visual servo or they need the observation of the full body by the robot. Secondly, their target Re-IDentification (Re-ID) abilities are weak in cases of target appearance change and highly similar appearance of distracting people.",
              "Following a specific user is a desired or even required capability for service robots in many human-robot collaborative applications. However, most existing person-following robots follow people without knowledge of who it is following."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide the techniques used for person-following robot navigation which track the person and re-identify if occluded.",
            "weight": 0.3,
            "evidence": [
              "In this framework, we combine Convolutional Channel Features (CCF) and online boosting to construct a classifier of a target person to be followed. It allows us to take advantage of deep neural network-based feature representation and adapt the person classifier to the specific target person depending on circumstances.",
              "\"a novel person re-identification module composed of three parts: a 360-degree visual registration, a neural-based person re-identification using human faces and torsos, and a motion tracker that records and predicts the target person's future position.",
              "Body detection model. For every frame, our system first runs a body and pose detector that provides the body bounding boxes and the poses of all humans in the image.",
              "Motion Tracker. We use a standard open-source Kalman filter motion tracker [3][27] 2 which tracks the position of the detected body bounding boxes. Each tracked bounding box is assigned a consistent ID across frames allowing tracking sort of the target even with noisy detections or when the target is occasionally not detected for a few frames.",
              "Face and Torso Identification Models. To distinguish the targeted individual from others, we utilize three additional neural-based machine learning models: a face detector that provides face bounding boxes in the current frame; a face embedding model that provides a face embedding on a cropped face bounding box; and a torso embedding model that provides a torso embedding on a cropped torso bounding box.",
              "Thanks to the fish-eye camera and visual servoing, it is rare to lose the person when it walks to the side of the mobile agent. Nevertheless, this still might happen, and occlusions might also lead to the system losing its target. In this case, the system enters into search mode. When the system is using the RGBD camera, the last known position of the target is kept as the local goal for a short period of time (e.g. 2 seconds).",
              "After that, or when using the fisheye camera, our mobile agent will stop moving forward and rotate in the direction where the target was last seen. This allows our mobile agent to have a better chance at re-finding the target.",
              "e a ReID framework for RPF that leverages long-term experiences. The experiences are maintained by a loss-guided keyframe selection strategy, to enable online continual learning of the appearance model.",
              "Existing RPF systems can be achieved through two steps: identify and follow. In the identify step, the system performs tracking and possibly ReID to locate the target person, while the follow step involves planning and executing the control of the robot to maintain the desired relative position with the target person.",
              "Specifically, the ReID module learns an appearance model of the target person when he can be correctly identified from tracked people. Later, if and when a long-time occlusion occurs, this appearance model is utilized to re-identify the target person among all the tracked people.",
              "First, we extract ReID features for the tracked people with a feature extractor. Given an image I and a person's bounding box B, we extract his image patch, denoted as M. Subsequently, we learn a feature extractor f by a standard CNN, which extracts local features from M. To represent partially visible human bodies, we further transform these local features into features associated with the body parts",
              "We learn a target classifier g using short-term experiences, denoted by mstand defined as the most recent observed pairs of image patches and labels {M, y}. These pairs are replayed from a short-term memory S, representing the latest knowledge about the target person.",
              "To leverage long-term experiences, we establish a longterm memory denoted as L, responsible for storing valuable samples, i.e., pairs of image patches and labels. When presented with a new sample, the memory manager employs a keyframe selection strategy to decide whether to add this sample to the memory buffer. Once the buffer reaches its capacity, memory consolidation takes effect to create space by purging certain samples.",
              " The algorithm optimizes the appearance model upon successful identification of the target person, identification recognized when the target id exists within the tracked individuals and the target confidence s surpasses the threshold dsw. When the target person is lost, the algorithm re-identifies him from all observed individuals. An individual is considered as the target person if his estimated confidence has surpassed a threshold dsw for consecutive zreidframes.",
              "To remove the assumption of full-body observation, we propose a width-based tracking module, which relies on the target width, which can be observed even at a close distance. For handling issues related to appearance variation, we use a global CNN (convolutional neural network) descriptor to represent the target and a ridge regression model to learn a target appearance model online. We adopt a sampling strategy for online classifier learning, in which both long-term and short-term samples are involved.",
              "In every frame, the classifier would predict the score of the target. If the score is lower than a threshold dswitch, then the system will turn to Re-ID state for judging an id-switch is happening. If the target id is lost, it will also lead to Re-ID state. The candidate will be judged as the target if its predicted score is larger than a threshold didin Nidconsecutive frames.",
              "d method uses a Sequential Nearest Neighbour with Thresholding Selection algorithm we devised to fuse together an anonymous person tracker and a face recogniser.",
              "Our proposed algorithm uses a Sequential Nearest Neighbour with Thresholding Selection algorithm to fuse together data from an anonymous person tracker and a face recogniser to enable identification and tracking of each surrounding person. This allows robots to track and follow specific users, as required in may service robot applications.",
              "A prototype using an RGB-D camera as the sole input sensor combines instance image segmentation and matrix calculations for dynamic path planning. This approach fuses visual and depth information for scene understanding and path estimation with reduced computational resources[1].",
              "This method uses a single RGB-D camera to track multiple people by combining skeletal tracking with shadow data. When direct skeletal tracking is not possible due to occlusion, shadow information is used to maintain tracking[2][7].",
              "A robust visual tracking approach for person-following tasks uses multiple templates:- A Scene Analysis Module (SAM) identifies the real target and similar distractors.- Positive templates are collected based on tracking confidence.- Negative templates are gathered from recognized distractors.",
              "This advanced technique fuses pose information into both video human detection and human association procedures:- Pose-guided person location prediction exploits temporal information to compensate for missing detections.- A hierarchical pose-guided graph convolutional network (PoseGCN) is used for appearance discrimination, exploiting human structural relations to improve person representation[5].",
              "One common approach is the use of feature-based tracking algorithms. For instance, some systems employ point-based features like Speeded Up Robust Features (SURF) to detect humans under challenging conditions such as variations in illumination, pose changes, and partial occlusions  (2, Gupta et al., 2017). Other methods utilize Histogram of Oriented Gradients (HOG) in combination with Support Vector Machines (SVM) for human detection  (4, Win et al., 2017).",
              "One common approach involves the use of feature-based tracking algorithms combined with machine learning models. For instance, some systems employ convolutional neural networks (CNNs) trained for human pose detection, which allow for robust tracking even during long-term occlusions  (3, Welsh, 2017). These models can be further enhanced by incorporating identity embeddings, enabling the system to maintain tracking across consecutive frames despite temporary visual obstructions.",
              "The Extended Kalman Filter (EKF) remains a popular tool for person tracking, especially when dealing with mobile platforms and temporary occlusions. EKF-based algorithms can consider the movement of both the robot and the target, allowing for accurate tracking even when the person is briefly occluded by others  (35, Miura et al., 2009)  (38, Miura et al., 2010).",
              "A novel system for human following using a differential robot, including an accurate 3-D human position tracking module and a novel planning strategy that ensures safety and dynamic feasibility, is proposed. The authors utilise a combination of gimbal camera and LiDAR for long-term accurate human detection."
            ]
          }
        ]
      }
    },
    "case_id": "3d8c315aed4cc104f2ad61f4deeda9c1",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are different techniques in Non-linear, Non-stationary signal processing? Which one is much effective in view of Geophysical signals?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are different techniques in Non-linear, Non-stationary signal processing? Which one is much effective in view of Geophysical signals?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should introduce non-linear and non-stationary signals.",
            "weight": 0.15,
            "evidence": [
              "Nonlinear  signal  processing  has  gained  increasing  attention  in  various  applications,  such  as biomedical signal analysis, fault diagnosis, and image processing[1]. Nonlinear signals are characterized by their  complex  and  non-stationary  nature,  making  their  analysis  and  interpretation  a  challenging  task[2]. Traditional  signal  processing  techniques  such  as  Fourier  analysis  and  wavelet  transform  are  inadequate  for nonlinear signals due to their linear and stationary assumptions.",
              "Non-linear, non-stationary signal processing techniques have evolved to address the limitations of traditional linear methods. These techniques include neural network-based approaches, time-frequency analysis, and decomposition-based methods, which are better suited for handling complex real-world signals."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide techniques in non-linear, non-stationary signal processing.",
            "weight": 0.15,
            "evidence": [
              "the selected non-linear techniques were estimation of the maximal Lyapunov exponent, correlation dimension and calculating sample entropy.",
              "Among the non-linear signal processing techniques the maximal Lyapunov exponent and the correlation dimension are prime candidates for dynamic analysis of biological signals [1]. The maximal Lyapunov exponent and the correlation dimension are both properties of non-linear systems. Their calculation is based on a phase space, a construct which demonstrates the changes of the dynamical variables of the system.\"https://link.springer.com/article/10.1007/s11517-008-0350-",
              "The sample entropy, another technique for dynamic analysis of biological signals, is a measure of regularity of finite length time series and estimates the extent to which the data did not arise from a random process.",
              "Stein et al.[18]starting from the phase space, implemented an algorithm that uses non-linear predictive forecasting for the RR series, predicting its future behaviour for a few beats by observing other sufficiently similar trajectories in phase space[48]. Given the RRiinterval the next interval RRi+ 1is predicted as the weighed mean of the RR intervals following the three nearest neighbours (found according to Euclidean distance).",
              "Approximate entropy (ApEn), reflects the regularity of time series[51]. It measures the logarithmic likelihood that runs of patterns, which are close to each other will remain close in the next incremental comparisons.",
              "An index of regularity, defined as the degree of recurrence of a pattern in a signal, can be defined by means of the conditional entropy (CE). CE represents the amount of information carried by the most recent sample of a seriesxwhen its pastL[?] 1 samples.",
              "Ensemble empirical mode decomposition (EEMD) and complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN) algorithms have been proposed for the analysis of nonlinear and non-stationary signals.",
              "EEMD is prone to mode mixing and requires a large number of trials to achieve accurate results. On the other hand, CEEMDAN overcomes the mode mixing issue and provides more accurate results with fewer trials or computation time.",
              "EEMD generates multiple noise-added versions of the input signal and applies EMD to each version to obtain a set of IMFs[7]. CEEMDAN further improves upon EEMD by adapting the added noise based on the local characteristics of the signal.",
              "CEEMDAN improves on EEMD by introducing the concept of complementary ensemble. In addition to adding white noise to the original signal, CEEMDAN adds an ensemble of signals obtained by subtracting a  low-pass  filtered  version  of  the  original  signal  from  the  original  signal.",
              "We quantify the contributions of the acoustic fltering and the neural network, respectively, for a spoken digit recognition task using four frequency decomposition methods with diferent non-linear characters: Lyon's ear cochleagram, MFCC flter, linear spectrogram, and Spectro HP.https://arxiv.org/pdf/1906.02812\"Traveltime inversion methods [14] are based on a linear approximation of the forward model, while seismic full-waveform inversion (FWI) addresses the full non-linear problem, leading to superior inversion accuracy and resolution [15]."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide techniques in non-stationary signal processing.",
            "weight": 0.15,
            "evidence": [
              "Ensemble empirical mode decomposition (EEMD) and complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN) algorithms have been proposed for the analysis of nonlinear and non-stationary signals.",
              "Empirical  mode  decomposition (EMD) is a powerful technique for decomposing a non-linear and non-stationary signal into a set of intrinsic mode functions (IMFs) that capture the underlying oscillatory modes of the signal.",
              "EEMD generates multiple noise-added versions of the input signal and applies EMD to each version to obtain a set of IMFs[7]. CEEMDAN further improves upon EEMD by adapting the added noise based on the local characteristics of the signal.",
              "As an effective signal separation method of non-stationary signal, empirical mode decomposition (EMD) has been widely used in the data or time series analysis of many engineering fields.",
              "LMD, ITD and LCD methods try to improve the construction of mean curve in EMD and they maybe work for some or a part of signals, but it is different to verify the superiority of them to EMD for all signals in theory.",
              "an improved empirical wavelet transform is proposed. This method combines the advantages of piecewise cubic Hermite interpolating polynomial (PCHIP) and the EWT, and is named PCHIP-EWT. The main idea of the proposed method is to select useful sub-bands from the spectrum envelope. The proposed method selects the maximum points of the spectrum to reconstruct the spectrum envelope on the basis of PCHIP.",
              "The LMD iteration process uses local means and magnitude to decompose the data instead of the cubic spline which is used in the EMD. And the LMD has been proven to be more effective than the EMD in analyzing amplitude and frequency modulated signals.\"\u000b\"Jain and Pachori[7]proposed a new iterative approach called eigenvalue decomposition (EVD) and proved that the EVD is neither affected by the ratio of their mean frequencies nor by their relative amplitudes.",
              "an improved VMD approach that combines VMD and detrended fluctuation analysis (DFA) named DFA-VMD. The noisy and non-stationary signals are first broken down by the VMD and then reconstructed by the DFA. Although the DFA-VMD has the same computational complexity and time complexity with the EMD, it performs better than the EMD in denoising and discrete wavelet threshold filtering.",
              "the time-variant local autocorrelated polynomial (TVLAP) model in the state space is proposed to model the dynamics of a non-stationary stochastic process (i.e., a signal or a time series), through which the model-based signal processing methods could be utilized to denoise, to correct the outliers/dropouts, and/or to identify anomalies contained in the measurements."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should justify which among non-linear and non-stationary is more effective in view of Geophysical signals.",
            "weight": 0.15,
            "evidence": [
              "The Kalman-Bucy method is here analized and applied to the solution of a specific filtering problem to increase the signal message/noise ratio. The method is a time domain treatment of a geophysical process classified as stochastic non-stationary. The derivation of the estimator is based on the relationship between the Kalman-Bucy and Wiener approaches for linear systems.",
              "A seismic signal represents the transient response of the Earth to excitation due to natural phenomena, such as eathquakes, or due to artificial sources as used in geophysical exploration, and it results in non-stationaty signals in noise.",
              "Two types of mathematical methods for data treatment can be used to represent a seismic signal: deterministic and stochastic. The deterministic method consists of using physical theories of wave propagation involving solutions of integral and differential equations satisfying contour and initial conditions. The stochastic method takes the statistical description of time series for the expressions of dynamic laws as statistical facts.",
              "Over the years, geophysical signal analysis techniques have steadily progressed from the use of basic integral transforms to present-day nonlinear signal processing techniques such as wavelet analysis, fractal and multifractal analysis, and empirical mode decomposition technique, which have found their forays in geosciences, paving the way for effectively unravelling the hidden information from the nonlinear and nonstationary geophysical data.",
              "It is based on Empirical Mode Decomposition (EMD) developed to analyze non-stationary and non-linear processes, which adaptively projects a signal on a basis of empirical AM/FM functions called Intrinsic Modulation Functions (IMFs)",
              "studies showed how EMD analysis is suitable for processing non-stationary and non-linear signals in low SNR conditions, enabling a better exploitation of current and past satellite altimetry missions.",
              "However, the wavelet-based versions of these techniques, respectively denoted as W-MMSE, W-Kalman and W-ANN, rather than the original techniques themselves, have been found to possess better capabilities in forecasting the highly nonlinear geophysical data.",
              "Other non-linear methodologies applied to complex geophysical time series include detrended fluctuation analysis, chaos theory, and wavelet analysis  (96, Ray et al., 2021).",
              "ANNs have been found to be effective in identifying the complex behaviour of most geophysical data which, by their very nature, exhibits extreme variability... and have the ability to analyse non-stationary geophysical data like wavelet transforms.",
              "Comparative studies have shown that some decomposition methods outperform others in specific applications. For instance, VMD has been found to be superior to Wavelet Transform (WT) and Ensemble EMD (EEMD) in Ground Penetrating Radar (GPR) signal denoising  (45, Xu et al., 2017).",
              "For seismic event detection, established methods like the Short-Time Average/Long-Time Average (STA/LTA) algorithm remain widely used  (138, Zainab et al., 2023). However, newer approaches, such as the Akaike Information Criterion (AIC), have also been developed for seismic phase picking  (126, Li et al., 2022)."
            ]
          }
        ]
      }
    },
    "case_id": "ca229869d669f7985df9e6b2280c7349",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are the most important practical applications of dextrous, in-hand robotic manipulation?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are the most important practical applications of dextrous, in-hand robotic manipulation?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with providing a short note on dexterous, in-hand robotic manipulation.",
            "weight": 0.3,
            "evidence": [
              "Dexterous multi-fingered hands can accomplish fine manipulation behaviors that are infeasible with simple robotic grippers.",
              "Our dexterous hand is a fundmanetal human feature that distinguishes us from other animals by enabling us to go beyond grasping to support sophisticated in-hand object manipulation.",
              "Our key innovation is the use of a tendon-driven ball joint as a basis for an articulated thumb. The design innovation enables our under-actuated hand to perform complex in-hand object manipulation such as passing a ball between the fingers or even writing text messages on a smartphone with the thumb's end-point while holding the phone in the palm of the same hand.",
              "To achieve advanced in-hand manipulation tasks, robotic hands are required to be equipped with distributed tactile sensing that can continuously provide information about the magnitude and direction of forces at all contact points between them and the objects they are interacting with.",
              "Adaptive robot hands have changed the way we approach and think of robot grasping and manipulation. Traditionally, pinch, fingertip grasping and dexterous, in-hand manipulation tasks were executed with fully actuated, rigid robot hands and relied on analytic methods, computation of the hand object Jacobians and extensive numerical simulations for deriving optimal and minimal effort grasps.",
              "Dexterous, in-hand robotic manipulation has several important practical applications across various industries and research fields.",
              "Dextrous robotic hands come in various designs, from multi-fingered anthropomorphic hands to specialized grippers with active components. These hands aim to achieve human-like dexterity and manipulation capabilities for diverse tasks."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer sould present the most important practical applications of dextrous, in-hand robotic manipulation.",
            "weight": 0.3,
            "evidence": [
              "heuristic finger gait which enables continuous object rotation for a wide variety of object shapes and sizes.",
              "we demonstrate the utility of our dexterous soft robotic hand in three real-world cases: unscrewing the cap of a jar, orienting food items for packaging, and gravity compensation during grasping.",
              "The resulting policy exhibits unprecedented levels of dexterity and naturally discovers grasp types found in humans, such as the tripod, prismatic, and tip pinch grasps, and displays contact-rich, dynamic behaviors such as finger gaiting, multi-finger coordination, the controlled use of gravity, and coordinated application of translational and torsional forces to the object.",
              "For precision grasps, our policy tends to use the little finger instead of the index or middle finger. This may be because the little finger of the Shadow Dexterous Hand has an extra DoF compared with the index, middle, and ring fingers, making it more dexterous.",
              "We demonstrate our approach on the RBO Hand 2, with learned motor skills for turning a valve, manipulating an abacus, and grasping.",
              "we propose to learn in-hand robotic manipulation tasks from human demonstrations, using Dynamical Movement Primitives (DMPs), and to reproduce them with a robust compliant controller based on the Virtual Springs Framework (VSF), that employs real-time feedback of the contact forces measured on the robot fingertips. With this solution, the generalization capabilities of DMPs can be transferred successfully to the dexterous in-hand manipulation problem: we demonstrate this by presenting real-world experiments of in-hand translation and rotation of unknown objects.",
              "Dexterous robotic hands equipped with advanced sensing capabilities can recognize and manipulate a wide variety of objects with different shapes, sizes, and materials[1]. This is particularly useful in manufacturing, logistics, and household robotics where robots need to handle diverse objects.",
              "Soft robotic hands offer a promising solution for manipulating delicate or fragile objects without causing damage[3]. This technology has applications in fields such as food processing, healthcare, and handling of sensitive materials in manufacturing.",
              "By learning from human demonstrations, robotic systems can acquire complex manipulation skills, making them more adaptable and easier to program for various tasks[3]. This capability is valuable in industries where robots need to work alongside humans or perform tasks that require human-like dexterity.",
              "EMG-based decoding of hand movements can be applied to develop more intuitive and functional prosthetic hands or assistive devices for individuals with disabilities[5].",
              "Dexterous manipulation techniques can be used to create more immersive and realistic interactions in virtual and augmented reality environments[5]. This has applications in training simulations, remote operation, and entertainment.",
              "Robotic hands with dynamically reconfigurable tactile sensors can adapt to different object shapes and surface conditions, enabling more reliable grasping and manipulation in varied environments[6]. This is particularly useful in unstructured settings like homes or disaster response scenarios.",
              "Advanced control algorithms allow robotic hands to perform complex in-hand manipulations, such as reorienting objects without releasing them[7]. This capability is valuable in assembly lines, packaging, and other industrial applications where objects need to be precisely positioned.",
              "Combining multiple sensory modalities (vision, proprioception, and haptics) enables more robust and adaptable manipulation in real-world scenarios where some sensory information may be occluded or unreliable[8]. This approach enhances the versatility of robotic systems in complex environments.",
              "Robots can assist in surgeries with high precision, performing tasks like suturing or handling delicate tissues",
              "Robots can pick fruits and vegetables without causing damage, and sort them based on size and ripeness.",
              "Robots can assist in tasks such as pruning, grafting, and inspecting plants for diseases or pests.",
              "Advancements in control algorithms, such as Soft Actor-Critic (SAC), show promise for improving dextrous manipulation in various industrial robotics applications. These include robot arms, automated assembly lines, warehousing, and logistics operations  (206, Zhang et al., 2024)."
            ]
          }
        ]
      }
    },
    "case_id": "d4e24c4f015d55b91350ce6267a7af5e",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What interfaces have researchers developed for helping people optimize LLMs for a task, and what are the biggest remaining user problems?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What interfaces have researchers developed for helping people optimize LLMs for a task, and what are the biggest remaining user problems?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should provide the role of interfaces to optlmize LLMs.",
            "weight": 0.19999999999999998,
            "evidence": [
              "The interface should be more than an assistant or a butler, but instead a secretary, actively working with the user to discover emergent interaction schemes on the fly.",
              " These interfaces aim to make LLMs more accessible and effective for various applications.",
              "Researchers have developed various interfaces to help users fine-tune and optimize LLMs more effectively. These interfaces aim to bridge the gap between the technical complexities of LLMs and the practical needs of users across different domains."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide the interfaces developed by researchers for helping people optimize LLMS for a task.",
            "weight": 0.19999999999999998,
            "evidence": [
              "A user mostly ignorant to the underlying tools/systems should be able to work with a LAUI to discover an emergent workflow. Contrary to the conventional way of designing an explorable GUI to teach the user a predefined set of ways to use the system, in the ideal LAUI, the LLM agent is initialized to be proficient with the system, proactively studies the user and his/her needs, and proposes new interaction schemes to the user.",
              " Flute X GPT, a concrete example using an LLM agent, a prompt manager, and a flutetutoring multi-modal software-hardware system to facilitate the complex, real-time user experience of learning to play the flute.",
              "AutoDroid, a mobile task automation system capable of handling arbitrary tasks on any Android application without manual efforts. The key insight is to combine the commonsense knowledge of LLMs and domain-specific knowledge of apps through automated dynamic analysis. The main components include a functionality-aware UI representation method that bridges the UI with the LLM, exploration-based memory injection techniques that augment the app-specific domain knowledge of LLM, and a multi-granularity query optimization module that reduces the cost of model inference.",
              "We integrate AutoDroid with off-the-shelf LLMs including online GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a new benchmark for memory-augmented Android task automation with 158 common tasks.",
              "OpenAGI, an open-source AGI research and development platform designed for solving multi-step, real-world tasks. Specifically, OpenAGI uses a dual strategy, integrating standard benchmark tasks for benchmarking and evaluation, and open-ended tasks including more expandable models, tools, plugins, or APIs for creative problem-solving. Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models.",
              "GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process.",
              "Describe, Explain, Plan and Select (DEPS), an interactive planning approach based on Large Language Models (LLMs). Our approach helps with better error correction from the feedback during the long-haul planning, while also bringing the sense of proximity via goalSelector, a learnable module that ranks parallel sub-goals based on the estimated steps of completion and improves the original plan accordingly. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances.",
              "Researchers have developed LLM-based text-to-SQL interfaces to improve database querying. These systems aim to generate accurate SQL from natural language questions, making database interactions more intuitive for users[1].",
              "Tools like TauchiGPT_V2 have been created to assist in academic research. These offline agent-based AI tools leverage LLMs to optimize standard research operations, potentially revolutionizing the research process[2].",
              "LLM-based interfaces have been developed to optimize novelty in top-k recommendations. These systems use reinforcement learning and LLMs to provide feedback for novel items, addressing the challenge of recommending items without user feedback data[3].",
              "The IO Navigator (ION) framework uses LLMs to analyze I/O traces of applications and provide diagnoses of potential I/O issues in High-Performance Computing systems. This tool aims to make complex I/O optimization more accessible to scientists[4].",
              "Interfaces like MiniGPT-4 have been developed to enhance vision-language understanding. These systems align visual encoders with advanced LLMs to enable capabilities such as detailed image description generation and website creation from hand-drawn drafts[7].",
              "For more technical users, CLI tools have been created to offer greater control and automation in the optimization process.",
              "Researchers from Tsinghua University and Microsoft Research Asia developed OpenPrompt, an open-source framework for prompt-learning paradigms. It provides a user-friendly interface for prompt-based learning and optimization of LLMs [1].",
              "The team at BigScience created PromptSource, a toolkit for creating, sharing, and using natural language prompts. It offers a streamlined interface for prompt engineering and dataset creation [2].",
              "Although not exclusively for optimization, LangChain provides a framework for developing applications with LLMs, including tools for prompt optimization and chaining [3]."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide the biggest remaining user problems to optimize LLMs for a task.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Despite its utility, the memory footprint of the KV cache swells with the enlargement of the model dimensions and lengths of generation sequences, imposing significant demands on device memory capacity.",
              ", LLM inference outcomes do not differ even cache memory is reduced. Nevertheless, such lossless merging heavily relies on assigning merge ratios to reflect the attention proportion of tokens being evicted and those being merged. Such a ratio pertinent to future generation steps is unpredictable, thereby rendering the efficacy of merging difficult to ascertain.",
              "Despite the exceptional performance of Large Language Models (LLMs), the substantial volume of key-value (KV) pairs cached during inference presents a barrier to their efficient deployment.",
              " It is argued that incremental improvement of such LLMs is not a viable approach to working toward human-level AGI, in practical terms given realizable amounts of compute resources. This does not imply there is nothing to learn about human-level AGI from studying and experimenting with LLMs, nor that LLMs cannot form significant parts of human-level AGI architectures that also incorporate other ideas.",
              "The AGIEval corpus is differently focused (more heterogeneous rather than purely science-focused) but also presents a massive challenge for current LLMs. How far toward this level of complex multi-step reasoning we can get within the \"Transformer as hub\" architecture remains to be seen. The more advanced the problems get, the more unique each problem is and the more abstraction is needed to figure out what knowledge from prior problems and readings to generalize to deal with the current problem.",
              " The crux of this challenge lies in the development of an efficacious external filter system capable of extracting the most pertinent information to address the queries at hand.",
              "There are instances when LLMs may fail to gen7 erate an appropriate response due to a mismatch between the local document and the user query. In such cases, LLMs might return a response like \"The text provided is not related to the query\". By employing sentiment analysis, the system can discern this as a negative sentiment from the LLM and consequently escalate the query to a higher level of assistance automatically.",
              "Another limitation arises due to the restrictive context window sizes of the LLMs, which are typically 4k or 8k tokens. This constraint can result in the system's failure to adequately address complex queries that necessitate a broader understanding of the context or the synthesis of information from multiple documents.",
              "Integrating LLMs with sensitive data, especially in healthcare and other regulated industries, remains a significant challenge due to privacy concerns and regulations[6].",
              "Despite advancements, there's still a lack of transparency in how LLMs make decisions, making it challenging for users to trust and interpret the results[5].",
              "Enabling LLMs to reliably follow multi-step instructions and accomplish complex goals articulated in natural language remains difficult[12].",
              "Optimizing LLMs to better align with user intents without the need for expensive model retraining is an area of active research[15]."
            ]
          }
        ]
      }
    },
    "case_id": "7670af632f0932d5ed20c34e1c9f01d2",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "With respect to signal processing which operator is more sensitive to noise detection, first order or second order and why?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "With respect to signal processing which operator is more sensitive to noise detection, first order or second order and why?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with explaining the noise detection using first order operator.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Sobel algorithm based on first derivative convolution, analyses derivatives and computes the gradient of the image intensity at every point, and then it gives the direction to increase the image intensity at each point from light to dark. It plots the edges at the points where the gradient is highest.",
              "the detection of weak signals in additive noise described by the first-order moving average (FOMA) of an impulsive process is considered. Specifically, decision regions of the maximum likelihood (ML) and suboptimum ML (S-ML) detectors are derived in the FOMA noise model, and specific examples of the ML and S-ML decision regions are obtained. The ML and S-ML detectors are employed in the antipodal signaling system and compared in terms of bit error rate in an impulsive noise environment.",
              "First-order operators, also known as gradient operators, measure the rate of change or slope of a signal. They are based on the first derivative of the signal.",
              "First-order derivatives are moderately sensitive to noise. Noise often manifests as high-frequency components, and the first derivative amplifies these high-frequency components."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should explain the noise detection using second order operator.",
            "weight": 0.17142857142857143,
            "evidence": [
              "However, for second-order motion patterns (contrast-modulated noise), thresholds for identifying the direction of motion are consistently higher (performance is worse) than those for identifying spatial structure.",
              "We estimate cumulants of nonGaussian processes in the presence of unknown deterministic and/or Gaussian signals, which allows either parametric or nonparametric estimation of the covariance of the nonGaussian noise. Our approach is to augment existing second-order detection methods using cumulants. We propose solutions for detection of deterministic signals based on matched filters and the generalized likelihood ratio test which incorporate cumulants, where the resulting solutions are valid under either detection hypotheses.",
              "The second criteria involves calculating the second-order difference between two adjacent difference values, i.e. L(a)- L(a-1). Similarly to second-order derivative for a function, this calculation helps pacify the contrast between noise-free pixels from smooth and edgy regions while still separates them from the impulsive noise values. The third criteria considers the rate at which the differences change, i.e.  L(a)/L(a-1) for L(a-1) > 0. A sudden jump with a large enough factor could also indicate a possible noise behavior.https://www.sciencedirect.com/science/article/pii/S0165168407000576\"Some pixels have extreme intensity values that remain in the second round to be restored. In the second round, the FON pixels may already be restored depending on the second-order neighborhood (SON) and third-order neighborhood (TON) pixels. This means, in the second round, the process depends on SON and TON to estimate the original intensity value of the noisy pixel.",
              "Second-order operators are based on the second derivative of the signal. They measure the rate of change of the gradient, effectively detecting rapid changes in the signal's slope."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should compare first and second order operations with respect to sensitivity to noise detection.",
            "weight": 0.17142857142857143,
            "evidence": [
              "either first-order and second-order patterns are encoded by separate mechanisms with different properties, or dynamic noise selectively impairs (\"masks\") sensitivity to second-order motion direction but not orientation. The former predicts the two thresholds should remain distinct for second-order patterns, irrespective of the temporal structure (static vs. dynamic) of the noise carrier. The latter predicts direction thresholds should be higher than orientation thresholds, for both second-order and first-order motion patterns, when dynamic (but not static) noise is present.",
              "the original intensity value of the noisy pixels will be estimated using the median value of the non-extreme intensity pixels of the first-order neighborhood pixels. In case all the pixels in the first-order neighborhood are having extreme intensity pixels, the median value of the non-extreme intensity pixels in the second-order neighborhood pixels will be used to estimate the original intensity of the noisy pixel.",
              "The 2-order differential is much higher than the 1st order differential of the high frequency component of the signal, and the weakening of the very low frequency signal is also stronger than the 1st order differential.",
              "Through observation, it is easy to find that the first-order differential Roberts, Sobel and Prewitt operators extract relatively few edge information and the edge continuity is poor; Laplacian operator, Canny operator and traditional fractional differential method extract the edge information is richer;",
              "The second derivative amplifies high-frequency components more than the first derivative. Noise in signals often manifests as high-frequency fluctuations. Therefore, second-order operators tend to enhance these noise components more significantly than first-order operators.",
              "First-order operators respond to gradual changes in the signal, producing a constant output for a linear change. Second-order operators, on the other hand, respond to abrupt changes in the gradient, making them more sensitive to sudden variations that could be indicative of noise.",
              "Second-order operators have the property of zero-crossing at edges, which makes them particularly useful for edge detection. However, this property also makes them more susceptible to detecting noise, as noise can create false edges or zero-crossings.",
              "Consider a simple noisy signal. When you compute its first derivative, any small fluctuations (noise) in the signal become more pronounced. Hence, the first-order operator can detect noise, but not as strongly as second-order operators.",
              "Using the same noisy signal, when you compute its second derivative, the resulting signal will have even more pronounced noise spikes than the first derivative. This makes second-order operators more sensitive to small fluctuations in the signal."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should provide the characteristics of first and second order operators in signal processing.",
            "weight": 0.08571428571428572,
            "evidence": [
              "An edge is a rapid change in the pixel intensity of the image. It comprises the critical characteristics and important features of an image. Such rapid changes are detected by using first and second order derivatives.",
              "\"One of the primary applications of higher order statistics has been for detection and estimation of nonGaussian signals in Gaussian noise of unknown covariance."
            ]
          }
        ]
      }
    },
    "case_id": "9931fdf53f47af01cd84e446d5676292",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are some principles for designing just-in-time interventions in a programming scenario?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are some principles for designing just-in-time interventions in a programming scenario?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The anser should begin with introducing just-in-time interventions.",
            "weight": 0.19999999999999998,
            "evidence": [
              "The just-in-time adaptive intervention (JITAI) is an intervention design aiming to provide the right type/amount of support, at the right time, by adapting to an individual's changing internal and contextual state.",
              "Just-in-time adaptive intervention (JITAI) is an intervention design framework that could be delivered via mobile app to facilitate in-the-moment monitoring of triggers for lapsing, and deliver personalized coping strategies to the user to prevent lapses from occurring.",
              "The concept of Just-In-Time Programming (JITP) further emphasizes the importance of continuous and proactive support."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide information on principles for designing just-in-time interventions in a programming scenario.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Multiple components are used to construct the foundation to design these interventions: decision points, intervention options, tailoring variables, and decision rules.",
              "JITAIs are a type of intervention design in which skill building (e.g., coping strategies, decision-making, planning behavior), emotional support (e.g., encouragement, empathy), and instrumental support (e.g., feedback, reminders) occur in an adaptive manner to facilitate support in the exact moment of need.",
              "Many extant JITAIs rely on decision rules that are grounded in comprehensive theoretical models and typically rely on a series of conditional statements (e.g., If smoking urge>[threshold], then recommend urge surfing intervention).",
              "Machine learning methods can employ historical data to model general response patterns that predict the proximal outcome (e.g., lapses) [36]. These models can be used in multiple phases of JITAI development, such as when initially exploring previously collected data to narrow down a subset of salient tailoring variables (e.g., variable selection).",
              "If the algorithm predicts that a lapse will occur, then an additional variable selection model is utilized to determine the likelihood of risk. To make the final decision, predictions from both the classification and variable selection models are taken into consideration.",
              "Machine learning can be a potent tool to enhance the effectiveness of a JITAI. However, researchers should consider the ultimate goal of the JITAI before deciding to implement a machine learning solution. Utilizing machine learning methods would be contraindicated when enough theoretical and empirical evidence is available to construct an effective decision rule a-priori.",
              "During the development process of a JITAI, it is crucial to decide what key intervention components are needed to affect the desired intervention outcome and what information should be used to tailor the delivery of each component to participants over time.",
              "Just-in-time interventions should be sensitive to the programmer's current context, including:a) The specific task or problem they are working onb) Their level of expertisec) The programming language and environment being usedd) Recent actions or errors",
              "The timing of interventions is crucial. They should be triggered:a) When the programmer is likely to need assistanceb) Before frustration sets inc) At natural break points in the coding process",
              "By incorporating these real-time feedback mechanisms, just-in-time interventions in programming scenarios can provide developers with valuable insights and support throughout the coding process, potentially leading to higher quality software and a more efficient development workflow.",
              "Context-awareness and accessibility are crucial principles for designing effective just-in-time interventions in programming scenarios. Integrated Development Environment (IDE) tools have traditionally offered contextualized and easily accessible support for developers.",
              "Reducing cognitive load is a critical principle in designing effective just-in-time interventions for programming scenarios. As software development becomes increasingly complex, tools that can alleviate the mental burden on developers are becoming essential.",
              "Integrated Development Environments (IDEs) are a primary target for integrating just-in-time interventions. Plugins can be developed to provide real-time feedback on code quality and suggest improvements as developers write code.",
              "To maximize effectiveness, just-in-time interventions should aim to integrate seamlessly across various development tools and platforms used in the software development lifecycle."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide challenges involved in designing just-in-time interventions in a programming scenario.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Developers of JITAIs should be mindful of employing machine learning techniques because the statistical and practical application can be time-consuming efforts.",
              "Statistically, there are a myriad of (1) methods to handle missing data, (2) cross-validation procedures, (3) model parameters to optimize, (4) combinations of models to test, and (5) variable extraction procedures. Therefore, there appear to be a functionally infinite number of models to examine, and there may be no such thing as a \"right\" model."
            ]
          }
        ]
      }
    },
    "case_id": "798f4cf8735e057c9956945ced665a8c",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are the most important open challenges in Federated Learning?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are the most important open challenges in Federated Learning?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with introducing Federated Learning.",
            "weight": 0.3,
            "evidence": [
              "Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized.",
              "The term Federated Learning was coined as recently as 2016 to describe a machine learning setting where multiple entities collaborate in solving a machine learning problem, under the coordination of a central server or service provider. Each client's raw data is stored locally and not exchanged or transferred; instead, focused updates intended for immediate aggregation are used to achieve the learning objective.",
              "It utilizes the on-device processing power and untapped private data by performing the model training in a decentralized manner and keeping the data where it is generated.",
              "There are three kinds of federated learning, i.e., horizontal, vertical, and transfer federated learning. Types of federated learning are based on client-server and peer-peer architecture. Federated learning has potential to serve in various applications such as healthcare, transportation, mobile network, etc.",
              "Federated Learning (FL) is a decentralized approach to machine learning where multiple clients (e.g., mobile devices, edge nodes) collaboratively train a model under the coordination of a central server, but without sharing their raw data. This approach aims to enhance data privacy and reduce communication costs.",
              "Federated Learning (FL) is a promising approach to machine learning that allows training models on distributed datasets without centralized data collection."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide most important open challenges in Federated Learning.",
            "weight": 0.3,
            "evidence": [
              "While the applications are many, there are several challenges associated with federated learning. The challenges can be broadly classified into two: training-related challenges and security challenges.",
              "Training related challenges encompass the communication overhead during multiple training iterations, heterogeneity of the devices participating in the learning and heterogeneity of data used for training.",
              "security challenges include the privacy and security threats due to the presence of adversaries ranging from malicious clients in the local device to a malicious user who has only a black-box access to the model.",
              "In FL, although the private data does not leave the device, it might be still possible for an adversary or a curious observer to learn the presence of a data point used for training in the local models.",
              "Communication overheads is one of the major bottlenecks in federated learning.",
              "The heterogeneity of the systems in the network as well as the non-identically distributed data from the devices affect the performance of the FL model.",
              "Like any machine learning model, Federated Learning models are also prone to attacks. The attacks can be introduced by a compromised central server or compromised local devices in the learning framework or by any participant in the FL workflow.",
              "Communication is a critical bottleneck in federated networks [5] which, when coupled with privacy concerns over sending raw data, necessitates that data generated on each device remain local. Indeed, federated networks potentially comprise a massive number of devices, e.g., millions of smartphones, and communication in the network can be slower than local computation by many orders of magnitude due to limited resources such as bandwidth, energy, and power.",
              "Moreover, the number of data points across devices may vary significantly, and there may be an underlying statistical structure present that captures the relationship among devices and their associated distributions [42]. This data-generation paradigm violates frequently used independent and identically distributed (i.i.d.) assumptions in distributed optimization and may add complexity in terms of problem modeling, theoretical analysis, and the empirical evaluation of solutions.",
              "In federated environments, the low participation of devices, nonidentically distributed local data, and local updating schemes pose novel challenges to these model-compression approaches.",
              "research on current FL mainly faces three bottlenecks: privacy and security threats, heterogeneity challenges, and huge communication overhead of FL.",
              "However, even under the protection of the FL security mechanism, there are still various types of attacks that can be against the FL system, thereby destroying the reliability of the FL system and threatening the data privacy of the participants.",
              "Several core challenges, such as privacy, security, communication cost, system and statistical heterogeneity, architecture, and aggregation algorithm designs, vary by domain and specific use cases.",
              ". In fact, by exposing more data effectively, ML can provide better data pattern differentiation. However, managing the large-scale data to maintain the efficiency and scalability of the ML algorithms has obviously been a challenge.",
              " In addition, in wireless networks the data is produced by and distributed over billions of devices.1 This necessitates the need for exploring learning solutions that can efficiently handle distributed datasets.",
              "These challenges are mainly related to the security, privacy, and performance of the current federated algorithm, as well as its important considerations in wireless settings.",
              "Similar to almost every decentralized algorithm, one of the essential considerations of federated learning is the convergence of the algorithm under limited communication and computation resources.",
              "Furthermore, considerations such as optimum number of local learners to participate in the global update, grouping of the local learners, and frequency of local updates and global aggregation, which induce trade-off between model performance and resource preservation, are application-dependent and worth investigation.",
              "How do we enable federated learning for a massive number of heterogeneous devices under strict wireless resource constraints?",
              "Although sparsification-enabled federated learning can reduce communication resource consumption, it might not be sufficient alone for a large number of devices and limited communication resources",
              "A malicious user can send the wrong local learning model parameters to the aggregation server to slow the federated learning convergence rate. In some cases, the global federated learning model might not converge due to the presence of a malicious user. On the other hand, it is necessary to verify the end-devices' updates before considering them for global aggregation.https://ieeexplore.ieee.org/abstract/document/9460016\"Scaling federated learning systems to millions or billions of devices presents challenges in both the orchestration of such systems and the sheer volume of data and model updates involved.",
              "Ensuring that the federated model performs fairly across all clients and doesn't introduce or amplify biases is a critical challenge, especially given the diverse nature of client data[7].",
              "Balancing the need for a global model with the desire for personalized models that cater to individual client needs is an area of active research[3].",
              "As FL models become more complex, ensuring their interpretability and explainability becomes increasingly important, especially in sensitive domains like healthcare[12][15].",
              "Federated Learning presents numerous open challenges that span communication, privacy, system design, and algorithmic aspects.",
              "The non-IID nature of data in federated settings poses a substantial challenge to the learning accuracy of FL algorithms  (48, He et al., 2021). It can result in significant skewness across devices or locations, making it difficult to achieve a globally optimal model."
            ]
          }
        ]
      }
    },
    "case_id": "64ef9b9e4c220dd1a2f6115b2a9e242a",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are metrics to mesures the quality of human robot communication?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are metrics to mesures the quality of human robot communication?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with providing the importance of the quality of human robot communication.",
            "weight": 0.3,
            "evidence": [
              "Proficiency self-assessment (PSA) is the ability of a robot to predict, estimate, or measure how well it can perform a task in a given context and environment. Human-robot teaming benefits not only from identifying a set of practicable metrics for PSA but also from developing metrics that evaluate how the robot communicates its proficiency to a human, how the human understands the communication, and how the robot perceives the human.",
              "interaction quality metrics in human-robot communication encompass a wide range of objective and subjective measures. These metrics aim to capture the nuanced aspects of human-robot interactions, from nonverbal behaviors and engagement to satisfaction and collaborative performance, providing a comprehensive assessment of the communication quality between humans and robots."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide the metrics to mesures the quality of human robot communication.",
            "weight": 0.3,
            "evidence": [
              "the common metrics group evaluates the overall performance of HRI systems. This group of metrics has three sub-groups: i) system performance or team performance, which describes how well the robots and humans perform in a team composition; ii) robot performance, which describes the degree of awareness that robots have about humans and the environment, as well as their autonomy; and iii) operator performance, which lists a set of factors that can impact how well humans perform when using HRI systems.",
              "Time behavior measures indicates the response and processing times that a human, robot, or a combination of humans and robots requires to perform its functions, a sub-task, or a complete task. Examples of these metrics are human idle time, algorithm processing time, collaboration time, and task completion time.",
              "Process measures are an aggregation of facts generated from the start to the end of a task or sub-task as well as cost-related, workspace design, safety, or product quality-related elements. Examples of these metrics are the number of errors and the number of assembles reached.",
              "Physiological measures are values obtained from body measures that help to understand the current state of the human (e.g., acceleration of human joints and heart rate).",
              "Human-Robot physical measures are values obtained from sensors that indicate the current state of the interaction (e.g., the distance between the human and the robot).",
              " This proficiency-based human-robot interaction (HRI) use case can occur before, during, or after the performance of a task. This article presents a set of metrics for this use case, driven by a four-stage cyclical interaction flow: (1) robot self-assessment of proficiency (RSA), (2) robot communication of proficiency to the human (RCP), (3) human understanding of proficiency (HUP), and (4) robot perception of the human's intentions, values, and assessments (RPH).",
              "Robots that can self-assess their abilities to perform tasks can potentially improve human-robot interaction (HRI).",
              "Endsley's three levels of situation awareness (SA)--perception, comprehension, and projection [40]--are widely used throughout HRI research and are particularly relevant to metrics definition for robot self-assessment of proficiency.",
              "The robot will use one or more modalities to communicate its proficiency to the human. The inherent limitations of the communication modality will affect what kind of proficiency measures can be communicated and what aspects of the communication the human is expected to understand (i.e., inputs to the HUP stage).",
              "A visualization display consisting of graphics, charts, or images displayed on a monitor (e.g., visualizations of input-output relationships of a neural network [172] or heat maps to convey image saliency [22, 65]) can be rich in information and can be transmitted instantaneously.",
              " These could include text as part of a visualization (e.g., highlighting text that indicates what component of the robot system is being used to perform a task [165]), a historical log, or sentences via natural language generation (e.g., narrating robot experiences during task execution [126], summarizing explanations based on situation criteria [172]).",
              "Subjective measures are those that are traditionally self-reported by human participants, and center around the participants' opinions toward or about the robots they are evaluating. While these measures may be influenced by external factors, they most directly capture the individuals' thoughts and preferences. In contrast, objective measures tend to be independent of the user's feeling, and focus on quantifiable values such as interface utility, timing, and task performance.",
              "The top five most frequently used subjective measures included evaluations capturing the human participants' directed emotion toward robot (13.30%), usability (12.25%), trust (12.13%), interface attribution (11.27%), and personal preference (10.65%). The objective measures were principally focused on task performance (28.51%) and the utilization of interfaces.",
              "As the team works, from an ecological viewpoint, individual team members each need to \"get\" information from the shared work environment, and manipulate that same work environment such that the outputs of each task's \"set\" new states in the environment.",
              "Similar to the sharing of information, a team of human and robot workers also share physical aspects of the work environment. Interaction between the team members can be characterized by physical coherency, defined as the degree to which agents work on separate physical elements of the work environment (e.g., artifacts). The consequence of low coherency meaning more need to share physical elements amongst the agents. Thus, high physical coherency implies each agent has its own demarcated part of the physical workspace to work on (e.g., their own sets of tools), thus reducing the required physical interaction with other agents.",
              "The capabilities of each team member affect the team's required interaction and associated teamwork. This metric addresses the teamwork required when the capabilities of multiple team members are required to complete work. The metric describes the extent to which the functions and tasks allocated to each agent is within their ability to perform independently.",
              "Evaluation of a team requires understanding which team members have authority over parts of the work (i.e., those who have authority or are assigned authority, to execute actions that aid the team in meeting its goals) and which team members are responsible for the outcome of work (i.e., those who are accountable for the results, from a legal and procedural perspective).",
              "\"Team cognitive coherence\" reflects the degree to which higher abstractions of the work are shared between agents, such as the goals, the intent, and evaluation.",
              "Several HCI evaluation techniques have been adopted and adapted to assess the efficiency and intuitiveness of HRI designs, such as Goal Directed Task Analysis (Adams, 2005), and Situation Awareness Global Assessment Technique (Endsley, 1988).",
              "GOMS has also been adapted to HRI (Drury et al., 2007) as a method to evaluate and predict efficiency of interfaces in HRI.",
              " In (Young et al., 2011) the authors propose three perspectives on social interaction in HRI: 1. Visceral factors of interaction, such as sadness, joy, frustration, and fear. 2. Social mechanisms including body language, and cultural norms. 3. Long-term social structures such as social relationships. These perspectives are suggested as basis for evaluation of human interaction experiences in HRI.",
              "Proxemics: This metric evaluates the physical distance and positioning between humans and robots during interaction[2]. It can provide insights into the comfort level and social dynamics of the communication.",
              "Behavior analysis: Automated classifiers can be used to discriminate between different types of human behavior when interacting with robots, such as attempts at social interaction versus disengagement[2].",
              "User experience evaluation: Systematic literature reviews have been conducted to assess various aspects of user experience in human-robot interaction[5]. This can include measures of usability, satisfaction, and overall interaction quality.",
              "Video sequencing tasks: The Human-Robot Interaction Video Sequencing Task (HRIVST) evaluates the legibility of a robot's behavior by asking participants to order short videos to form a logical sequence of the robot's actions[9]. This provides insights into how well humans can understand and predict robot behavior.",
              "Transparency measures: In teleoperation scenarios, various weight metrics can serve as indicators of teleoperation transparency and demonstration quality[7]. These can help assess the effectiveness of remote robot learning from human demonstrations.",
              "Empathy scales: Adapted questionnaires have been developed to evaluate the perceived empathy of artificial agents, including robots[13]. These can measure the emotional and social aspects of human-robot communication.",
              "Task performance metrics: For specific applications, task-related performance measures can be used. For example, in baseball umpiring, fans' evaluations of robot umpires can be assessed based on decision-making authority and embedded expertise[14].",
              "Safety metrics: For physical human-robot interaction, metrics like Mean Reflected Mass (MRM) can be used to assess and optimize robot postures for safe communication and collaboration[15].",
              "Confidence-weighted learning models: In remote learning scenarios, network-aware confidence weighting strategies can be employed to enhance the quality of demonstrations and subsequent robot learning[7].",
              "Gesture-based communication evaluation: Metrics can be developed to assess how well humans and robots communicate through nonverbal cues, particularly gestures used to convey plans or spatial information[8].",
              "Linguistic behaviors such as question types, topic of conversation, and sentence form, as well as non-verbal elements such as facial expressions, gestures, and eye-gaze of the conversational partner, are important factors to effectively initiate and maintain conversation through turn-taking between humans.",
              "Fluency is an important metric in Human-Robot Interaction (HRI) that describes the coordination with which humans and robots collaborate on a task.",
              "Task Completion Rate: This metric measures the percentage of tasks successfully completed when a human and robot communicate to achieve a goal. A higher completion rate indicates better communication quality [1].",
              "Time Efficiency: The time taken to complete a task or exchange information can be an indicator of communication quality. Shorter times generally suggest more efficient communication [2].",
              "Error Rate: This metric quantifies the number of misunderstandings or miscommunications during human-robot interaction. A lower error rate implies better communication quality [3].",
              "Trust and Confidence: Measures of how much users trust the robot and feel confident in its ability to understand and respond appropriately [7]."
            ]
          }
        ]
      }
    },
    "case_id": "89477b47d76d0cc8eb59b1b7938bb988",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "The Nyquist-Shannon theorem provides an upper bound for the sampling period when designing a Kalman filter. Leaving apart the computational cost, are there any other reasons, e.g., noise-related issues, to set a lower bound for the sampling period? And, if so, is there an optimal value between these bounds?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "The Nyquist-Shannon theorem provides an upper bound for the sampling period when designing a Kalman filter. Leaving apart the computational cost, are there any other reasons, e.g., noise-related issues, to set a lower bound for the sampling period? And, if so, is there an optimal value between these bounds?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with providing the reason behind setting the lower bound of sampling period while designing Kalman filter.",
            "weight": 0.24,
            "evidence": [
              "A key result in this field is the theorem of Duffin and Schaeffer [15], which states that if the density of the sequence of sample points is strictly larger than the Nyquist density, then stable reconstruction is possible.",
              "For use of real-time predicted data for control purposes, these data can now be filtered using the Kalman filter for smooth transition from predicted to actual data.",
              "A prominent advantage of the bound (15) is that it involves quantities that are either known (s2) or computed (Pk). Whereas s is a predetermined tuning parameter, the estimation error covariance Pk, which is computed at every step, decreases rapidly as k - [?] (in the sense that Pk[?] Pk+1> 0).",
              "Embedding the Krylov solver method within the CSKF/Kalman filter measurement update stage dramatically reduces the computational overhead, in particular, when the state dimension is prohibitively large.",
              "It's important to note that while adhering to this theorem prevents information loss due to undersampling, it does not guarantee optimal filter performance. Other factors, such as computational resources and system dynamics, may influence the choice of sampling period within this upper bound. The Nyquist-Shannon theorem provides a crucial starting point for determining an appropriate sampling rate, but practical considerations often lead to sampling at rates higher than this minimum requirement to improve estimation accuracy and robustness.",
              "While the Nyquist-Shannon theorem sets an upper bound, there are also reasons to consider a lower bound for the sampling period in Kalman filter design. Extremely short sampling periods can lead to computational inefficiencies and potential degradation of filter performance.",
              "While the Nyquist-Shannon theorem establishes an upper bound for the sampling period, there are important considerations for setting a lower bound as well. One key factor is the relationship between the sampling period and the dynamics of the system being estimated.",
              "Moreover, the choice of sampling period can significantly impact the filter's ability to handle noise and maintain stability. Karameh et al. demonstrate that in the context of continuous-discrete cubature Kalman filters (CD-CKF), very short sampling intervals relative to the system dynamics can actually lead to a marginal deterioration in performance when estimating hidden dynamics.",
              "One of the primary reasons to set a lower bound on the sampling period is related to noise:a) Measurement Noise: Higher sampling rates can lead to increased measurement noise. As the time between samples decreases, the signal-to-noise ratio (SNR) may decrease, making it more difficult to distinguish between the true signal and noise [2].b) Quantization Noise: In digital systems, faster sampling can lead to increased quantization noise, especially if the analog-to-digital converter (ADC) resolution is limited [3].",
              "While the Nyquist-Shannon theorem provides an upper bound for the sampling period in Kalman filter design, there are indeed several reasons to consider a lower bound as well. Noise-related issues, model mismatch, numerical stability, and sensor limitations all play a role in determining an appropriate minimum sampling period. The optimal sampling rate lies between these bounds and depends on various factors including system dynamics, noise characteristics, and application-specific requirements."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide the optimum value between the upper and lower bounds for the sampling period while designing Kalman filter.",
            "weight": 0.24,
            "evidence": [
              "The proposed entropy bounds (50) and (52) consist of two (right-hand-side) ingredients. The first one can readily be identified as the information entropy associated with the estimation error covariance of an ML or LS estimator working in ideal settings. It yet accounts for a correction term lmin(C) regulating the signal to noise ratio due to the observation model (17). By saying \"ideal settings\" we essentially refer to a hypothetical situation in which the support of the sparse parameter vector ai is perfectly known.",
              "The obtained bounds establish the relation between the complexity of the AR process and the attainable estimation accuracy through the use of a novel measure of complexity.",
              "In all the following computations the time interval is=0.001 s and the mode truncation order is six. The choice of this sampling interval meets the Nyquist-Shannon sampling theorem where 0.001 is less than half of the period 0.0024.",
              "These findings suggest that an optimal sampling period likely exists between the upper and lower bounds, balancing the need for accurate state estimation with computational efficiency and the filter's ability to effectively handle system dynamics and noise. The specific optimal value would depend on the particular system characteristics, noise properties, and computational constraints of the application at hand",
              "On the other hand, and because the CD-CKF integrates the impact of noise within the continuous dynamics during the time-update (prediction) step, the filter performance deteriorates marginally in estimating the hidden dynamics for sampling intervals that are quite large (dt = 4-8 ms) in relation to the speed of such dynamics (membrane time constant~20 ms).",
              "Determining this optimal rate often involves a combination of theoretical analysis, simulation studies, and practical considerations."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should provide the upper bound in Kalman filter designing compared to the Shennon-Nyquist theorem.",
            "weight": 0.12,
            "evidence": [
              "The compressed sensing (CS) theory, as another technique for estimating sparse signals [28, 29], is beneficial to deal with possible degeneration of covariance matrices of regression vectors (i.e., insufficient excitation), especially when the regression vectors are high-dimensional but sparse. It guarantees the recovery of high-dimensional signals from fewer observations than the Nyquist/Shannon sampling principle considers necessary.",
              "Based on the augmented stochastic system, a multirate Kalman Filter (Smyth and Wu (2007)) can be designed, which predicts at the fast rate (Tf) and corrects at a slow rate (Ts), i.e., correction only occur at the instants when ys(k) is available."
            ]
          }
        ]
      }
    },
    "case_id": "0650cf7bfe45d90ba3e1bb72427bf04a",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What publicly available datasets are typically used for evaluating type inference systems in python?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What publicly available datasets are typically used for evaluating type inference systems in python?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with providing background on type inference in pyhton.",
            "weight": 0.15,
            "evidence": [
              "Type inference refers to the process of automatically determining the data type of an expression within a programming language. In Python, which is dynamically typed, this determination takes place at runtime. To address potential ambiguities, developers can utilize type annotations, which explicitly specifies the expected data types of variables or function returns.",
              "Python type inference is challenging in practice. Due to its dynamic properties and extensive dependencies on third-party libraries without type annotations, the performance of traditional static analysis techniques is limited. Although semantics in source code can help manifest intended usage for variables (thus help infer types), they are usually ignored by existing tools.",
              "Type inference systems in Python are designed to deduce variable types to facilitate tasks such as debugging, optimization, and providing auto-completion in Integrated Development Environments (IDEs). Evaluating the accuracy and performance of these systems requires robust and varied datasets."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide publicly available datasets that are typically used for evaluating type inference systems in python.",
            "weight": 0.15,
            "evidence": [
              "We generate our annotated dataset with enriched data based on inference results from PySonar2 and perform data cleaning to enhance quality, which itself is a significant contribution, because high quality labeled data is critical for deep learning. Our dataset is collected from 4,577 popular Python projects on GitHub with 54,928,617 Lines Of Code (LoC) from 320,402 source files. It contains 77,089,946 annotations, which is large enough for most Python types research."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide challenges in evaluating type inference systems in python.",
            "weight": 0.15,
            "evidence": [
              "ManyTypes4Py, a large Python dataset for machine learning (ML)-based type inference. The dataset contains a total of 5,382 Python projects with more than 869K type annotations. Duplicate source code files were removed to eliminate the negative effect of the duplication bias. To facilitate training and evaluation of ML models, the dataset was split into training, validation and test sets by files.",
              "Python type inference is challenging in practice. Due to its dynamic properties and extensive dependencies on third-party libraries without type annotations, the performance of traditional static analysis techniques is limited. Although semantics in source code can help manifest intended usage for variables (thus help infer types), they are usually ignored by existing tools.",
              "One challenge is that it is difficult to collect a high-quality human-labeled training dataset for this purpose.",
              "typePY is our source code dataset collected from 4,577 top-star GitHub repositories. probPY stands for the dataset published in Xu et al. [13], and typeshed is a human-labeled dataset containing only the annotations for function parameters and return values [14].",
              "Unlike previous work, we trained and evaluated our model on a type-checked dataset and used mean reciprocal rank (MRR) to reflect the performance perceived by users.",
              "A type-checked dataset with 5.1K Python projects and 1.2M type annotations. Invalid type annotations are removed from both training and evaluation.",
              " In total, there are more than 882K functions with around 1.5M arguments. Also, the dataset has more than 2.1M variable declarations. Of which, 48% have type annotations.",
              "Our dataset is based on 1.3 million Python programs on GitHub, which we gathered from Google's public datasets of Python programs from BigQuery with a query to Google BigQuery that focused on Python repositories that more than watch one event in the last year. The query was issued in August 2019, but reflected a snapshot of GitHub from March 20, 2019, by Google.",
              "TypeEvalPy contains 154 code snippets with 845 type annotations across 18 categories that target various Python features. The framework manages the execution of containerized tools, transforms inferred types into a standardized format, and produces meaningful metrics for assessment.",
              "Current literature primarily assesses the performance of such type inference tools based on large-scale real-world benchmark datasets, notablyType4Py,HiTyper, andTypilus(Allamanis et al.,2020). On the contrary, open-sourced solutions only rely on specifically-designed test cases.",
              "Typilus Dataset: Accompanies the Typilus model and contains 600 Python projects",
              "Python-150K: Published in 2016, this dataset includes 8,422 Python projects.",
              "Many researchers in the field of machine learning-based type inference for Python often create and present their own datasets tailored to their specific methods.",
              "CodeSearchNet-Python (CSN-Python): Originating from the CodeSearchNet corpus, this dataset consists of approximately 450,000 real-world Python methods. It has been used for various code-related tasks, including method name prediction.",
              "PY150: This benchmarked dataset contains 150,000 Python program files in Abstract Syntax Tree (AST) formats. It is typically split into 100,000 files for training and 50,000 for testing.",
              "The Python Package Index (PyPI) is a vast repository of Python projects. Researchers often scrape and curate a collection of PyPI projects to create datasets for analysis. These datasets offer diverse coding styles and usages of Python's dynamic features, which makes them useful for evaluating type inference systems."
            ]
          },
          {
            "name": "most_important_item_3",
            "criterion": "The answer should provide the tools used for type inference in python.",
            "weight": 0.15,
            "evidence": [
              "In response to this challenge, both industry and academia have developed type inference tools and static type checkers. Examples from industry includePyright(pyr,[n.d.]) andPytype(Pyt,2023), while academic contributions featureType4Py(Mir et al.,2022) andHiTyper(Peng et al.,2022).",
              "TypeEvalPy, a type inference evaluation framework for Python bundled with a micro-benchmark that covers all the Python language constructs of Python 3.10.",
              "PYInfer, an end-to-end learning-based type inference tool that automatically generates type annotations for Python variables."
            ]
          }
        ]
      }
    },
    "case_id": "98debf018d6ed82c9f7297d0e59681b0",
    "annotator": "Annotator 2 Assignments",
    "agreement": true
  },
  {
    "initial_prompt": "What are leading methods for generating hard examples of the boolean satisfiability problem, and what are their strengths and weaknesses?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are leading methods for generating hard examples of the boolean satisfiability problem, and what are their strengths and weaknesses?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with introducing boolean satisfiability.",
            "weight": 0.17142857142857143,
            "evidence": [
              "A satisfying assignment for a formula is an assignment of the variables such that the formula evaluates to 1. It simultaneously satisfies the constraints imposed by all the operators in the formula. Such an assignment may not always exist.",
              "Boolean Satisfiability Problem (SAT) is a quintessential NP-complete problem, pivotal in theoretical computer science and practical applications like verification and artificial intelligence. Generating hard instances of SAT is crucial for evaluating algorithms, benchmarking SAT solvers, and studying phase transitions."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide theleading methods for generating hard examples of the boolean satisfiability problem.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Another motivation for SAT instance generation derives from the training process of machine learning, which requires a large number of instances while keeping a balance between classes in the dataset.",
              "Non-ML research on pseudo-industrial SAT instance generation is mainly based on structural measures and probabilistic methods. For example Giraldez-Cru and Levg[83] proposed the community attachment (CA) model to generate new instances of given modularity GiraldezCru and Levg[84] leveraged the notions of locality and popularity to portray the scale-free structure of real-world instances in their popularity-similarity (PS) model.",
              "SATGEN[85] is a generative model trained in an unsupervised way. Different from previous models that parameterize certain graph measures, it uses a GAN to implicitly capture graph-based features of SAT instances. Specifically, real formulae are transformed into LIG as input to GAN for training. The learned model then generates new LIGs which are recovered to CNF formulae by a greedy hill-climbing approach.",
              "The subsequent work G2SAT[86] improves SATGEN and focuses on bipartite LCG representation. It proposed a novel node-merging (node-splitting) algorithm to generate bipartite graphs from (to) a forest. The key component of G2SAT is a GCN-based classifier that decides node pairs for merging. Specifically, G2SAT adopted GraphSAGE[88] , where node embeddings are updated by previous embeddings of itself and neighbors.",
              "Overall, there are three variants of G2SAT, namely GCN2S, EGNN2S, and ECC2S. First, GCN2S replaced GraphSAGE with graph convolution in [42] for node embedding. On top of GCN2S, the most distinguishing modification of [87] is that it takes edge features into consideration. Instead of using LCGs, EGNN2S and ECC2S encode CNF formulae as signed VCGs, so that the edges between variables and clauses indicate whether the literal is positive or negative.",
              "Most SAT solvers work with a restricted representation of formulas in conjunctive normal form (CNF), defined as follows. A literal l is either a positive or a negative occurrence of a variable (for example, x or -x). A clause, c, is the OR of a set of literals, such as (l1[?] l2[?] l3... [?] ln). A CNF formula is the AND of a set of clauses, such as (c1[?] c2[?] c3 [?] cm)",
              "Random k-SAT involves generating formulas by randomly selecting clauses, each consisting of k literals, from a given set of variables.",
              " Geometrical and geo-regular generators: These methods use geometric probability distributions to select variables, potentially generating instances more similar to industrial problems (30, Ansotegui et al., 2008).",
              "0-hidden algorithm: This approach can generate hard K-SAT instances with fine-grained control over hardness against local search strategies  (70, Jiang et al., 2023).",
              "Random regular graph transformations: This method converts random regular graphs into systems of linear equations, followed by clausification, resulting in hard satisfiable instances (25, J\"arvisalo et al., 2006).",
              "W2SAT framework: This approach learns intrinsic structures from real-world/industrial instances to generate SAT formulas  (71, Yu et al., 2023).",
              "No-Triangle SAT: This algorithm produces hard instances by fixing arity and number of variables while varying the number of clauses (53, O'Sullivan et al., 2019).",
              "WnDGen: Generates weakly nondecisive clause sets, which can be difficult for state-of-the-art SAT solvers  (43, Kusper et al., 2013).",
              "Frustrated-loop inspired method: Generates weighted MAX-2-SAT instances with known solutions, useful for evaluating MAX-SAT solvers  (54, Ventra et al., 2019).",
              "We provide two generation methods of k-SAT instances, called geometrical and the geo-regular, as generalizations of the uniform and regular k-CNF generators. Both are based on the use of a geometric probability distribution to select variables.",
              "As a solution we introduce a very simple algorithm, called WnDGen, which generates weakly nondecisive clause sets.",
              "To evaluate a MAX-SAT solver, it is convenient to generate hard MAX-SAT instances with known solutions. Here, we propose a method of generating weighted MAX-2-SAT instances inspired by the frustrated-loop algorithm used by the quantum annealing community.",
              "Graph-based methods: These approaches, such as those using high-girth bipartite incidence graphs (29, Ansotegui et al., 2008) and digraph-based generators (57, Biro et al., 2020), can produce instances that are particularly challenging for certain types of SAT solvers. For example, minimal unsatisfiable SAT instances generated from strong digraphs are considered among the hardest unsatisfiable clause sets.",
              "Cryptographically hard instances are generated using cryptographic techniques, such as cryptographic hash functions or encryption algorithms. These instances are designed to be computationally hard to solve, making them ideal for testing the robustness of SAT solvers."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide the advantages and disadvantages of method generating hard examples of the boolean satisfiability problem.",
            "weight": 0.17142857142857143,
            "evidence": [
              "The restriction to CNF is an active choice made by SAT solvers as it enables their underlying algorithms. Further, this is not a limitation in terms of the formulas that can be handled. Indeed, with the addition of new auxiliary variables; it is easy to translate any formula into CNF with only a linear increase in size. However, this representation is not used exclusively and there has been recent success with solvers for non-clausal representations (for example, NFLSAT)",
              "Encodings have been useful in translating problems from a wide range of domains to SAT, for example, scheduling basketball games, planning in artificial intelligence, validating software models, routing field programmable gate arrays, and synthesizing consistent network configurations. This makes SAT solvers powerful engines for solving constraint satisfaction problems. However, SAT solvers are not always the best engines--there are many cases where specialized techniques work better for various constraint problems, including graph coloring.",
              "For theoretical interests, numerous combinatorial problems can be expressed in propositional formulae and solved by running a SAT solver[2] , e.g., graph coloring[3] , vertex cover[4] and clique detection[5] . It also serves as a useful tool for automated theorem proving, one typical case of which is the resolution of Keller's conjecture[6] . Moreover, there are plenty of industrial applications of SAT solving, such as bounded model checking, configuration management, and equivalence checking in circuit design.",
              "Therefore, an effective tool for instance generation helps with data augmentation and improves training.",
              "An obvious disadvantage of SATGEN is that an LIG does not uniquely map to a SAT formula; thus, the model cannot differentiate between different instances with the same LIG representation. This information loss also leads to extra post-processing and imposed constraints in the recovering stage.",
              "Strengths of k-SAT- **Parameter Control**: Allows fine-tuning through parameters like the ratio of clauses to variables (r = m/n), which influences the problem's hardness.- **Study of Phase Transitions**: Near the critical threshold (~4.26 for 3-SAT), instances of SAT exhibit a sharp jump in difficulty, providing a natural source of hard instances.### Weaknesses of k-SAT- **Unpredictability**: While near the critical threshold can yield hard instances, the problem's hardness isn't guaranteed.- **Scalability**: Beyond certain sizes and clause lengths, the generation process may become impractically slow."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should provide a small text on boolean satisfiability problem solver.",
            "weight": 0.08571428571428572,
            "evidence": [
              "Most practically successful SAT solvers are based on an approach called systematic search.",
              "The search space is a tree with each vertex representing a variable and the out edges representing the two decision choices for this variable. For a formula with n variables, there are 2nleaves in the tree. Each path from the root to a leaf corresponds to a possible assignment to the n variables. The formula may evaluate to 1 or 0 at a leaf (colored green and red respectively).",
              "Systematic search, as the name implies, systematically searches the tree and tries to find a green leaf or prove that none exists.",
              "Given a CNF formula, the DPLL algorithm first heuristically chooses an unassigned variable and assigns it a value: either 1 or 0. This is called branching or the decision step. The solver then tries to deduce the consequences of the variable assignment using deduction rules.",
              "Specifically, Popescu et al.[15] focused on a broader scope of constraint-solving problems. Another survey[16] is the closest to ours, where the author reviewed machine learning methods in solving SAT and quantified SAT (QSAT), especially for automated theorem proving.",
              "Based on its taxonomy, this survey discusses three primary patterns for this combination for SAT solving: 1) Standalone SAT solvers with pure machine learning methods; 2) Replacing some key components of existing conflict-driven clause learning (CDCL) solvers with learning-directed heuristics; 3) Modifying the local search solvers with learning-aided modules."
            ]
          }
        ]
      }
    },
    "case_id": "7b42d39d9f8fdb3a8ec1dd783d66d6c2",
    "annotator": "Annotator 2 Assignments",
    "agreement": true
  },
  {
    "initial_prompt": "What is unique k-sat and what theoretical results are known about it?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What is unique k-sat and what theoretical results are known about it?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with introducing k-sat.",
            "weight": 0.24,
            "evidence": [
              "Thek-SATproblem is to determine if a givenk-CNF has a satisfying assignment. It is a celebrated open question as to whether it requires exponential time to solvek-SAT fork[?] 3. Here exponential time means 2dnfor somed>0.",
              "Unique k-SAT is a variation of the classical k-SAT (k-Satisfiability) problem, which itself is a well-known problem in computer science and combinatorial optimization.",
              "In the k-SAT problem, you are given a boolean formula in conjunctive normal form (CNF), where each clause contains exactly k literals, and you need to determine if there exists an assignment to the variables that satisfies the entire formula.",
              " In Unique k-SAT, the additional constraint is that the boolean formula has at most one satisfying assignment."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide theoretical results associated with k-sat.",
            "weight": 0.24,
            "evidence": [
              "Our main technical result is an Isolation Lemma fork-CNFs, which shows that a given satisfiablek-CNF can be efficiently probabilistically reduced to a uniquely satisfiablek-CNF, with non-trivial, albeitexponentially small, success probability.",
              ", it is consistent with the Valiant-Vazirani result that Unique k-SAT is solvable in O(2 [?]n) time, but general k-SAT requires 2O(n)time, where n is the number of variables in the formula. This is because, when combined with the reduction from Formula SAT to k-SAT, the Valiant-Vazirani reduction squares the number of variables in the formula. Due to the variable squaring, this reduction is not very useful if the best algorithm for Unique k-SAT runs in time O(2kn), for some 0 < k < 1, which is the common working hypothesis.",
              "Perhaps the most prominent challenge in this context is that of pinning down the randomk-SAT thresholdrk--SAT. Here we prove thatrk--SAT= 2kln 2--1/2 (1 + ln 2) +ok(1), which matches the 1RSB prediction up to theok(1) error term. The proof directly employs ideas from the 1RSB cavity method, such as the notion of covers (relaxed satisfying assignments) and bits of the Survey Propagation calculations.",
              "fork[?] 3,k-SAT requires exponential time complexity, we show that the complexity ofk-SAT increases askincreases. More precisely, fork[?] 3, definesk=inf{d: there exists 2dnalgorithm for solvingk-SAT}. Define ETH (Exponential-Time Hypothesis) fork-SAT as follows: fork[?] 3,sk>0.",
              "we show thatskis increasing infinitely often assuming ETH fork-SAT. Lets[?]be the limit ofsk. We will in fact show thatsk[?] (1[?]d/k)s[?]for some constantd>0. We prove this result by bringing together the ideas ofcritical clausesand theSparsification Lemmato reduce the satisfiability of ak-CNF to the satisfiability of a disjunction of 2enk'-CNFs in fewer variables for somek'[?] kand arbitrarily smalle>0.",
              "The second moment method can be used to to determine the random k-SAT threshold within a factor of 2.",
              "It gives extraordinarily tight bounds for random NAE k-SAT, determining the threshold for that problem within a small additive constant.",
              "we develop a newasymmetric second moment methodthat allows us to tackle this issue head on for the first time in the theory of random CSPs. This technique enables us to compute the k-SAT threshold up to an additive ln2-1/2+O(1/k) ~0.19.",
              "One striking consequence of this result is that a coarse threshold for a random graph property can only happen when the value of the critical edge probability is a rational power of n. As an application of the main theorem we settle the question of the existence of a sharp threshold for the satisfiability of a random k-CNF formula.",
              "Letbe a uniformly distributed randomk-SAT formula withnvariables andmclauses. We present a polynomial time algorithm that finds a satisfying assignment ofwith high probability for constraint densities/<(1[?])[?]2[?]ln[?]()/, where-0.https://epubs.siam.org/doi/abs/10.1137/09076516X\"we initiate the study ofk-SAT instances of bounded diameter. The diameter of an ordered CNF formula is defined as the maximum difference between the index of the first and the last occurrence of a variable.",
              "We show that under highly parallel and efficient transformations, diameter and path-width are equal up to a constant factor.",
              "the computational complexity of SAT, MAX-SAT, #SAT grows smoothly with the diameter (as a function of the number of variables).",
              "there exists a sequencetk= O(k)such that forr [?] 2kln 2 - tk,Fk(n,rn)is satisfiable with probability1-o(1).Our technique yields an explicit lower bound for everykwhich fork > 3improves upon all previously known bounds. For example, whenk=10our lower bound is 704.94 while the upper bound is 708.94.https://dl.acm.org/doi/abs/10.1145/780542.780577\"It is known that the general k-SAT problem is NP-complete for k [?] 3[^1]. Since Unique k-SAT is a special case of k-SAT, it also remains within NP. However, Unique k-SAT adds the condition of uniqueness, which introduces unique complexity considerations.",
              "Unique k-SAT has been shown to be PP-complete (where PP stands for Probabilistic Polynomial time)[^3]. PP-completeness is stronger than NP-completeness in the sense that PP-complete problems are considered to be at least as hard as the hardest problems in NP and more as they deal with probabilistic computations.",
              "Randomized reductions, as suggested by the Valiant-Vazirani theorem, play an important role in solving Unique k-SAT by converting it to a simpler problem that can then be tackled using probabilistic methods[^2].",
              "A central focus of theoretical research on k-SAT problems, including Unique k-SAT, is the study of random instances and their phase transitions. The random k-SAT model is particularly important for understanding the average-case complexity and serving as a benchmark for satisfiability algorithms.",
              "Unique k-SAT is in the complexity class UP (Unambiguous Polynomial time) for all k.- For k [?] 3, Unique k-SAT is UP-complete.",
              "For Unique 3-SAT, there exists a deterministic algorithm running in time O(1.3071^n), where n is the number of variables.",
              "For Unique 4-SAT, the best known deterministic algorithm runs in time O(1.4704^n).",
              "The study of Unique k-SAT is closely tied to derandomization theory. If we could efficiently derandomize the Valiant-Vazirani reduction, it would have significant implications for the relationship between P and NP."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should provide the applications of k-sat.",
            "weight": 0.12,
            "evidence": [
              "Unique k-SAT plays a role in the security analysis of cryptographic protocols where unique solutions often translate to single keys or configurations[^7].",
              " Unique k-SAT complexities contribute to the study of proof systems and derandomization techniques[^2].",
              "The problem is also relevant in AI, especially in constraint satisfaction problems where ensuring uniqueness of solutions can be crucial.",
              "The study of Unique k-SAT has also revealed interesting connections to statistical physics. Concepts such as replica symmetry breaking and the proliferation of metastable states have been applied to analyze random k-SAT problems."
            ]
          }
        ]
      }
    },
    "case_id": "a006865cdd1db1cfc34bfbec067d47e4",
    "annotator": "Annotator 2 Assignments",
    "agreement": true
  },
  {
    "initial_prompt": "In recommendation systems, how are new methods that optimize diversity typically evaluated?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "In recommendation systems, how are new methods that optimize diversity typically evaluated?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with introducing recommendation systems.",
            "weight": 0.24,
            "evidence": [
              "Recommender systems (RSs) are effective solutions to help users find what they need in the current information overload. Based on item descriptions and/or available ratings supplied by users, RSs aim at selecting among unseen items those that may be of interest for a specific user. RSs have been proven successful in various fields including e-commerce, movie, music, travel, etc.",
              "The recommendation system is used to process a large amount of data to recommend new item to users, which are achieved using the many developed algorithms.",
              "Recommendation systems support users and developers of various computer and software systems to overcome information overload, perform information discovery tasks, and approximate computation, among others."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide information on evaluation of new methods that optimizes diversity in recommendation systems.",
            "weight": 0.24,
            "evidence": [
              " (1) none of the tested recommender systems, even the most recent ones, provides items with levels of diversity that suit user profiles (R2< 0.2); (2) the classic postprocessing diversification approach may lead to over-diversification compared to users' diversity needs and (3) the diversity adjustment that accounts for user profiles has more benefits (greater R2and smaller accuracy loss).",
              "The performance evaluation of these recommendation approaches are typically driven by their ability to predict accurately user- item interactions, i.e. by the accuracy criterion.",
              "Hence, it seems reasonable to compare and analyze the behaviors of these RSs from both the absolute and the relative diversity perspectives. For a specific user, the absolute diversity refers to the amount of diversity in her/his recommendation list while the relative diversity represents the extent to which the absolute diversity of the recommendations would fit the user's diversity needs.",
              "Compared with the random search and the grid search methods which are generally time-consuming, BO is an iterative procedure that takes into account the past choices made in order to consider the next set of hyper-parameters to evaluate, thus reducing largely the computation time.",
              "Precision represents the number of relevant items among the  recommended items for a given user .",
              "Recall refers to the proportion of items liked by the user  that are included in her/his recommended list.",
              "F1-measure is the harmonic mean of precision and recall i.e. 2[?]Precision[?]Recall/ Precision+Recall.",
              "Mean Average Precision (MAP) represents the mean of the Average Precision (AP) measures over all users. Unlike precision and recall, which do not consider positions of the relevant items, the AP measure is a ranked precision metric that places emphasis on highly ranked relevant items.",
              " represents the average of the intra-list diversity (ILD) values over all users. We also call this measure the absolute diversity as opposed to the next measure which evaluates the diversity of the recommendation lists in relation to the diversity of the items present in user profiles.",
              "R2(ILD, ILD), denoted as the relative diversity, is the coefficient of determination of the ILD values (i.e. diversities of items recommended to users by a RS) vs. ILDvalues (i.e. diversities of corresponding users' profile items), throughout the entire user set). This metric measures how well the diversity of recommendations provided by a RS meets its users' expectations in terms of diversity.",
              "The similarity function (, ) is crucial for measuring the ILD of an item list (i.e. for both absolute and relative diversities). The similarity measurement between two items could be based on the user-item rating matrix. This has the advantage of always being directly feasible.",
              "To measure the performance of R(*, th), we consider a timeline point of view. Thus, the training dataset D will include all the items assessed by the user up to a given moment t, those that the user liked and those that did not.",
              "On the other hand, to measure the performance of R with respect to the diversity of the items recommended for all users, we may use the aggregate diversity  AggDiv(R(*, th)) = |  [?]  R(u, th) : u [?] T | .",
              "If the items to be tested represent an unbalanced distribution, RMSE and MAE can be used in averaged form, depending on the evaluation (e.g., per-user or peritem). If the RMSE of each item can be calculated separately, then the average of all calculated RMSEs represents the average RMSE of the recommendation system.",
              "To measure diversity in a recommendation list, an alternative approach is to compute the distance of each item from the rest of the list and average the result to obtain a diversity score. For such an average, however, a random recommender may also produce diverse recommendations. Therefore, this needs to be accompanied by some measure of precision. Plotting precision-diversity curves helps in selecting the algorithm with the dominating curve.",
              "Having correctness metrics combined with diversity has an added advantage, as correctness metrics do not take into account the entire recommendation list. Instead, they consider the correctness of individual items.",
              " Q-statistics can be used to find diversity between two recommender algorithms. Q-statistics are based on a modified confusion matrix, confronting two classifiers as correctly classified versus incorrectly classified. As a result, the confusion matrix displays the overlap of those itemsets. Q-statistic measures are then defined to combine the elements in the modified confusion matrix, ultimately arriving at a measure for the diversity of the two recommender algorithms.",
              "Lathia et al. [40] introduced a measure of diversity for recommendations in two lists of varying lengths. In their approach, given two sets L1and L2, the items of L2that are not in L1are first determined as their set theoretic difference. Then, the diversity between the two lists (at depth N) is defined as the size of their set theoretic difference over N. This way, diversity returns 0 if the two lists are the same, and 1 if the two lists are completely different at depth N.",
              "Involves A/B testing or sample testing to evaluate the performance of the recommendation algorithm in real-world scenarios. This method provides more accurate results but is resource-intensive.",
              "Combines offline and online evaluation methods to leverage the strengths of both approaches.",
              "Coverage measures the proportion of items from the entire catalog that appear in recommendations across all users.Formula: Coverage = |Recommended Items| / |Total Items in Catalog|",
              "The Gini coefficient measures the inequality in item popularity distribution within recommendations.",
              "When evaluating new diversity-optimizing methods, researchers typically compare their performance against:1. Baseline methods (e.g., popularity-based recommendations)2. State-of-the-art diversity-aware algorithms3. The same algorithm without diversity optimization",
              "Entropy: Evaluates the uncertainty or randomness in the recommendation distribution."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should highlight the importance of diversity in recommendation systems.",
            "weight": 0.12,
            "evidence": [
              "Diversity in recommendation systems is used to avoid the overfitting problem as well as excellent skill, which provides a recommendation based on increasing the quality of user experiences.",
              "Diversity could be also considered to be the opposite of similarity. If items presented to users are too similar, they do not present diverse items and so may not be of interest.",
              "Optimizing diversity in recommendation systems is crucial to provide users with a wide range of relevant items. Evaluating these new methods involves assessing their ability to balance diversity with accuracy and other desired properties."
            ]
          }
        ]
      }
    },
    "case_id": "b069a1248503c4caa98dab0014d1a55a",
    "annotator": "Annotator 2 Assignments",
    "agreement": true
  },
  {
    "initial_prompt": "In robotics, what are the leading methods for learning terrain traversibility costs automatically from robot experience?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "In robotics, what are the leading methods for learning terrain traversibility costs automatically from robot experience?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with providing a background on terrain traversibility in robotics.",
            "weight": 0.3,
            "evidence": [
              "Estimating the traversability of terrain in an unstructured outdoor environment is a core functionality for autonomous robot navigation. While general-purpose sensing can be used to identify the existence of terrain features such as vegetation and sloping ground, the traversability of these regions is a complex function of the terrain characteristics and vehicle capabilities, which makes it extremely difficult to characterize a priori.",
              "In robotics, learning terrain traversability costs automatically from robot experience is a crucial aspect of efficient and effective navigation."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide leading methods for learning terrain traversibility costs automatically from robot experience.",
            "weight": 0.3,
            "evidence": [
              " Our method is based on autonomous training data collection which exploits the robot's experience in navigating its environment to train classifiers without human intervention.",
              "There are two key elements of the traversability learning algorithm: a mechanism to autonomously collect and label training data as the robot interacts with its environment, and an online method to learn the concept space for traversable and non-traversable terrain.https://ieeexplore.ieee.org/abstract/document/1641763\"image data obtained in the past is associated with traversability labels obtained in the present. In order to exploit newly-acquired training data in making traversability predictions about unknown terrain, we propose an on-line learning method. The learned traversability concepts are incrementally updated with new data only, with the advantage that the updated classifier is immediately available for navigation.",
              "The online classifier must also be computationally efficient. Usually, multiple feature vectors accumulate over time in one grid cell, and we use simple majority voting to predict the traversability of each cell.",
              "Geometric features are obtained from stereo, which provides three dimensional coordinates (x, y, z) for a subset of image pixels. In order to reduce the dimensionality of our feature space and gain robustness to noise in the stereo measurements, we quantize the height estimates for each pixel using a histogram.",
              "A standard path planning algorithm is used to compute the best cost path to the goal that avoids colliding with any obstacles. The planner has two operational modes: conventional and learning-based. In the conventional planning mode, the cost map is based solely on the elevation map of terrain in the robot's environment which is computed through stereo ranging. Regions that exceed a slope threshold are marked as nontraversable.",
              "The LAGR (Learning Applied to Ground Vehicles) platform collects stereo pairs and then the human expert labels each explored cell as low, intermediate, high, or lethal with respect to the difficulty encountered while traversing this particular cell. After gathering a total of 4000 labeled cells, it is shown that height and slope were the most important features in terms of determining terrain's traversability. Subsequently, by integrating the classification from geometry features with color information, a cost map was generated for path planning purposes.",
              "the robot learns from human demonstration to calculate terrain costs, which indicate the probability of an obstacle's presence. Through the combination of using Bayesian estimates and geometric information collected by stereo vision, the final terrain costs follow a certain distribution and thus it can be determined whether the path is traversable by articulating that those cells with higher values of features ought to be less traversable.",
              "By performing graph optimization, local dense 3D maps are constructed and integrated to a global one that yields information for the robot's real time pose and can ultimately provide traversability costs for rough terrain navigation for each map cell.",
              "A fully differentiable and trained for 6 hours end-to-end Fully Convolutional network, including multiple stages of filtering, various CNN layer dimensions (64,128,256,512) and downsampling, is used as the understructure before an upsampled penultimate layer classifies the input raw image (orbital or ground). The output of the classification acts as the input to the cost function of the optimal route planner for landing site traversability analysis and also for building a robust slip prediction model.",
              "Palazzoet al.[44]design a supervised model that can analyze multiple traversability routes through the medium of the encoder-decoder architecture. Notably, while the problem is examined as a regression one, their aim is to estimate and predict the traversability costs of various routes even on scenarios that no labels are provided. Using collected RGB images as inputs, the utilized architecture consists of a fully convolutional network module for feature extraction, followed by two layers; a convolutional and a fully-connected respectively. The bottom line of their method lies on training a model to predict correct traversability scores on the source dataset, while carrying out unsupervised domain adaptation on the target data.",
              "Terrain traversability takes into account the map content and the capabilities of the robot to produce a cost/difficulty map, representing how difficult it may be for the robot to traverse particular areas of the terrain. This cost map is then passed on to the planner to determine the best compromise between its objectives and the associated costs.",
              "Traversability analysis: this task usually interprets the terrain at a more refined level than obstacle detection, contrasting its characteristics against the dynamics and kinematics capabilities of the robot, and generating a \"difficulty\" or cost map of the environment. Such a cost map is usually a 2D representation that indicates a cost value for each location (x, y) (optionally with an orientation).",
              "terrain traversability analysis based on regression aims at estimating thetraversal costof a terrain patch, somehow encoding the difficulty that the vehicle would encounter while crossing the considered terrain patch. Most of the time the goal is to identify a map of costs (or cost map) of the surrounding environment, on top of which path planning or motion control can be performed.",
              "a vector of terrain geometry features is determined by a local quadratic regression of terrain patches around footholds suggested by an expert. This vector is used to derive a utility function encoding the terrain cost function. The height map of a testbed rough terrain is a priori known and the position of the vehicle inside the testing scenario is acquired through a motion capture system.",
              "an unsupervised method is adopted by Faigl and Pragr [34] for traversability cost maps regression. In particular self-organizing maps are proposed, to predict three traversal costs, i.e., instantaneous power consumption, maximum achievable forward velocity, and attitude stability, based on the morphology and the appearance of the terrain, acquired through an RGB-D camera. Furthermore, during the learning phase the predicting model is also fed with the traversal costs actually experienced by the robot during the traversal. Once learned the model is deployed for predicting the cost from the exteroceptive data only.",
              "in the context of deep learning-based traversal cost regression most of the works belong to the learning from demonstration paradigm, especially to the recent wave of Deep Inverse Reinforcement Learning (DIRL) and maximum entropy DIRL (ME-DIRL). In this case, the traversability cost map is indirectly obtained as the negative reward function learned from the expert demonstrations.",
              "In literature some works providing a hybrid approach between terrain classification and traversal cost regression can be found. Typically, in these works a terrain classification is performed from which a traversal cost is derived as a side result, or vice versa, a map of traversal costs is built and after that a binary classification is obtained by thresholding.",
              "Using proprioceptive signals, such as energy consumption and robot stability, combined with exteroceptive information collected by an RGB-D camera, the authors manage to predict the traversal cost of seen but not yet visited terrain.",
              "One approach involves online incremental learning of terrain traversal cost using Bayesian Committee Machine (BCM) and Gaussian Processes (GPs). This method, as described in, enables robots to learn the traversal cost model incrementally as they explore unknown environments. The traversal cost is characterized by incrementally constructed GPs, which are updated based on the robot's proprioceptive experience and terrain descriptors.",
              "Incremental learning of traversability cost using exteroceptive and proprioceptive data collected by aerial and ground robots is another effective method.",
              "Self-supervised traversability prediction methods, such as WayFASTER, have also been developed. These methods, as described in, use the robot's own experience to predict traversability without requiring human-provided labels.",
              "CNNs have become a popular method for learning terrain traversibility costs from visual data. These networks can process raw image data and learn to associate visual features with traversibility costs.",
              "Robots can learn terrain properties by analyzing data from their IMUs, which measure acceleration and angular velocity.",
              "Self-supervised learning methods enable robots to automatically learn terrain traversability from their own experiences without human labeling. These approaches leverage various sensor data and robot interactions to build robust traversability models.",
              "One notable example is the STERLING system, which learns terrain representations from unconstrained, unlabeled robot experiences. STERLING uses self-supervised learning techniques to develop terrain representations and traversability costs, making it particularly relevant for autonomous navigation in unknown environments.",
              "Apprenticeship learning, also known as learning from demonstration or imitation learning, has been widely investigated to transfer human expertise in navigation control to autonomous robots. This method allows robots to learn traversability costs directly from human demonstrations, creating a mapping from the representation space to the terrain costs encountered during traversal.",
              "Proprioceptive feedback has been incorporated into traversal cost learning models. By combining the robot's experienced traversal cost with terrain appearance and geometry captured by the robot's sensors, more comprehensive traversability models can be created.",
              "In RL, robots interact with the environment and receive rewards based on their actions. For terrain traversibility, the reward might represent the ease or efficiency of traversing a particular terrain.",
              "The robot learns a policy to maximize cumulative rewards. Techniques like Q-learning or actor-critic methods can be employed to learn the optimal policy for minimizing traversal costs."
            ]
          }
        ]
      }
    },
    "case_id": "e2e6287d2687772bb02cfc5107047cb6",
    "annotator": "Annotator 2 Assignments",
    "agreement": true
  },
  {
    "initial_prompt": "What are some of the challenges associated with creating a training dataset for question answering in scientific domains? Describe some recent methods that try to overcome these challenges.",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are some of the challenges associated with creating a training dataset for question answering in scientific domains? Describe some recent methods that try to overcome these challenges.",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with challenges associated with creating a training dataset for question answering in scientific domains.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Besides, the mathematical reasoning abilities of external calculators and Python interpreters are evaluated using computation-intensive QA datasets. However, these evaluation benchmarks may not faithfully reflect the extent to which models leverage external tools, as some questions could still be correctly answered solely using the internal knowledge of the LLMs.",
              "The questions are designed to simulate real-world information-seeking inquiries. However, they cannot be answered directly with LLMs' internal knowledge, but instead require LLMs to obtain information from the reference corpora via tool use.",
              "While human experts can produce high-quality questions, the entire process is labor-intensive, timeconsuming, and hard to scale.",
              "Depending solely on LLMs may generate unanswerable questions or hallucinate information that does not exist in the reference data. Besides, some of the LLM-generated questions are too easy and can be directly answered with only LLMs' internal knowledge.",
              "We assume that we are given an LLM model M (e.g., GPT-3.5 or Llama-2-7B) that is pre-trained on some source dataset (e.g., Common Crawl). The goal is to adapt M (hereon called the student model) to a new target domain by using a small seed dataset D, where D potentially has unseen characteristics, compared to the pre-trained dataset (e.g., a medical dataset with specific terminology, or a private database with specific characteristics). In this case, the model's zero-shot or fine-tuned performance is likely to be unsatisfactory.",
              "To enrich D, AugGPT (Dai et al., 2023) has introduced a promising approach that generates additional augmented data by applying a prompted LLM to all available data points in the target training dataset. However, this method falls short by indiscriminately augmenting data without considering the student model's varying performance across different data points.",
              " However, such an endeavor demands considerable computing resources, maintenance efforts, and user traffic, all while carefully handling potential data privacy issues.",
              " However, dynamic data collection can be more expensive than its static predecessor as creating examples that elicit a certain model response (i.e., fooling the model) requires more annotator effort, resulting in more time spent, and therefore higher cost per example.",
              "building a credible QA system for patient-specific EMR QA requires largescale question and answer annotations that sufficiently capture the challenging nature of clinical narratives in the EMR. However, serious privacy concerns about sharing personal health information (Devereaux, 2013; Krumholz et al., 2016), and the tedious nature of assimilating answer annotations from across longitudinal clinical notes, makes this task impractical and possibly erroneous to do manually.",
              "By sampling many QA pairs per passage, we increase the chance of generating good samples. However, if we sample too many qa pairs the top ranked ones might be too similar.",
              "Scientific texts are often context-dependent and may leave crucial information implicit, requiring a good grasp of the surrounding context to understand fully.",
              "One of the primary challenges is the scarcity of annotated datasets in scientific domains. Annotating scientific texts is labor-intensive and requires experts, which restricts the availability of high-quality training datasets.",
              "Published scientific work may contain measurement errors, contradictory findings, or incomplete information, adding noise to the datasets.",
              "One of the primary challenges in creating scientific QA datasets is the need for domain expertise. Scientific fields often involve complex concepts, specialized terminology, and intricate relationships between ideas. This makes it difficult for non-experts to generate or validate questions and answers accurately.",
              "Scientific knowledge evolves rapidly, and datasets can quickly become outdated. Maintaining the relevance and accuracy of scientific QA datasets over time is an ongoing challenge."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should explaining the recent methods used to overcome the challenges associated with creating a training dataset for question answering in scientific domains.",
            "weight": 0.17142857142857143,
            "evidence": [
              "ToolQA attempts to mitigate these issues by selecting data from out-of-scope sources that have not been memorized by LLMs.",
              "ToolQA is unique in that it focuses on the open-ended use of tools for question-answering, rather than benchmarking the intermediate process of tool use.",
              " ToolQA creates tool-based question-answer pairs and assesses whether LLMs can arrive at the correct answer, regardless of the tool chains used.",
              "we propose an automatic three-phase process (Figure 2): (1) We first select data from public sources that are unmemorized by LLMs during Reference Data Collection; (2) We adopt Human-Guided Question Generation to steer LLMs to generate valid questions according to predefined templates; (3) We produce accurate answers for the generated questions with Programmatic Answer Generation.",
              "The question generation phase aims to generate questions that can be answered by using the available tools over the reference corpora. There are two straightforward strategies to generate questions: 1) letting human experts come up with questions about reference corpora, or 2) relying solely on LLMs to generate questions about the reference corpora.",
              "we propose a human-guided LLM generation approach that uses question templates to bridge human guidance and automatic LLM generation.",
              "\"the true arguments filled into the question template, we can run the tool chains with the corresponding arguments to programmatically extract answers from the reference data. This process enables automatic generation correct answers to questions, even for those questions that involve multi-step reasoning.",
              "Large Language Models (LLMs) have demonstrated impressive zero shot performance on a wide range of NLP tasks, demonstrating the ability to reason and apply commonsense. A relevant application is to use them for creating high quality synthetic datasets for downstream tasks.",
              "We develop a data generation pipeline that selects source passages, identifies candidate answers, generates questions, then finally filters or relabels them to improve quality. Using this approach, we amplify a smaller human-written adversarial dataset to a much larger set of synthetic question-answer pairs.",
              "In self-training, a model is trained to both predict correctly on labelled examples and increase its confidence on unlabelled examples. Self-training can yield complementary accuracy gains with pretraining (Du et al., 2020) and can improve robustness to domain shift (Kumar et al., 2020).",
              "Models like SciBERT and BioBERT [Beltagy et al., 2019; Lee et al., 2020] are adaptations of BERT (Bidirectional Encoder Representations from Transformers) specifically for scientific texts. These models leverage transfer learning by initially being pre-trained on large general text corpora and subsequently fine-tuned on scientific literature, making them more adept at understanding domain-specific language.",
              "Techniques like back-translation, paraphrasing, and other data augmentation methods are employed to artificially increase the size and diversity of training datasets.",
              "Combining crowdsourcing with expert annotation can strike a balance between scale and quality. Initial annotations by non-experts can be refined through expert review, thus enhancing the quality without fully depending on scarce expert time.",
              "Active learning involves iteratively selecting the most informative data points for annotation, thus making the most efficient use of expert annotators.",
              "Leveraging domain-specific knowledge graphs and ontologies can help models understand the context and relationships between different scientific terms."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide challenges and their solutions altogether.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Each question requires combining the seed core fact with additional knowledge. However, it is unclear how many additional facts are needed, or whether these facts can even be retrieved from any existing knowledge sources. QASC, on the other hand, explicitly identifies two facts deemed (by crowd workers) to be sufficient to answer a question.",
              "Many systems answer science questions by composing multiple facts from semi-structured and unstructured knowl8083 edge sources (Khashabi et al. 2016; Khot, Sabharwal, and Clark 2017; Jansen et al. 2017; Khashabi et al. 2018b). However, these often require careful manual tuning due to the large variety of reasoning techniques needed for these questions (Boratko et al. 2018) and the large number of facts that often must be composed together (Jansen 2018; Jansen et al. 2016). By limiting QASC to require exactly 2 hops (thereby avoiding semantic drift issues with longer paths (Fried et al. 2015; Khashabi et al. 2019)) and explicitly annotating these hops, we hope to constrain the problem enough so as to enable the development of supervised models for identifying and composing relevant knowledge."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should provide advantages of some of the tasks performed in creating a training dataset.",
            "weight": 0.08571428571428572,
            "evidence": [
              " Automating data annotation processes has the potential to save large amounts of time, money and effort that goes into manually labelling datasets."
            ]
          }
        ]
      }
    },
    "case_id": "0ed1770483ec64633a580366026dd16e",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "How does multiplexing enhance data transmission efficiency in communication networks?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "How does multiplexing enhance data transmission efficiency in communication networks?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should introduce multiplexing.",
            "weight": 0.17142857142857143,
            "evidence": [
              "A multiplexer is a device that allows digital information from several sources to be routed onto a single line for transmission to a single destination.",
              "Multiplexing is a fundamental technique employed in communication networks to enhance data transmission efficiency. It allows the transmission of multiple signals over a single communication channel, reducing the need for dedicated lines for each signal. This technique improves the overall utilization of network resources, reduces costs, and increases the capacity and flexibility of communication systems."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide reasons about the enhancement of data transmission efficiency in communication network using multiplexing.",
            "weight": 0.17142857142857143,
            "evidence": [
              "MIMO systems employ different kind of techniques such as Space-Time Coding (STC) and Space Division Multiplexing (SDM) to correctly retrieve the data at receiver end.",
              "the aim of SDM is to enhance the data rate.",
              "V-BLASTtechnique uses an antenna array with K transmit antennas. In total K information bits are multiplexed in an orthogonal fashion. The special property of the multiplexing technique is that, for each symbol duration, only one out of K antennas is transmitting using BPSK modulation.",
              "Furthermore, the symbol duration is equivalent to the bit-duration due to the parallelism introduced by the K antennas. The receiver is able to detect the transmitting antenna and by using this information the demultiplexing is carried out. It is demonstrated that the proposed method achieves about the same spectral efficiency as 8PSK, but with a relaxed bit-energy-to-noise ratio of about 2.5 dB. Furthermore, the linear channel capacity gain (error-free transmitted bits) over the single antenna BPSK transmission scheme is found to be in the range of 2.5-3.",
              "In optical time-division multiplexing (OTDM), a high bit-rate data stream is constructed directly by time-multiplexing several lower bit-rate optical streams. Similarly, at the receiver end of the system, the very high bit-rate optical signal is demultiplexed to several lower bit-rate optical signals before detection and conversion to the electrical domain.",
              "This system uses the discrete Fourier transform to orthogonally frequency multiplex many narrow subchannels, each signaling at a very low rate, into one high-rate channel. When this technique is used with pilot-based correction, the effects of flat Rayleigh fading can be reduced significantly.",
              "In addition, with each subchannel signaling at a low rate, this technique can provide added protection against delay spread.",
              "At present, Orthogonal Frequency Division Multiplexing (OFDM) [6] is a widely adopted solution mainly because of its robustness against multipath channels [7] and easy implementation based on Fast Fourier Transform (FFT) algorithms [8].",
              "Bi-orthogonal Frequency Division Multiplexing (BFDM) [16] employs well localized pulse shapes at the transmitter and receiver side that are bi-orthogonal to each other. The good frequency-localization of the transmit pulse makes the system robust against frequency dispersion (Doppler effect) while the good time-localization of the pulse provides robustness against time dispersion (multipath).",
              "GFDM is based on the modulation of independent blocks, where each block consist of a number of subcarriers and subsymbols. The subcarriers are filtered with a prototype filter that is circularly shifted in time and frequency domain. This process reduces the OOB emissions, making fragmented spectrum and dynamic spectrum allocation feasible without severe interference in incumbent services or other users. The subcarrier filtering can result in nonorthogonal subcarriers and both inter-symbol interference (ISI) and inter-carrier interference (ICI) might arise.",
              "Orthogonal frequency division multiplexing (OFDM) is a modulation technique which is now used in most new and emerging broadband wired and wireless communication systems because it is an effective solution to intersymbol interference caused by a dispersive channel.",
              "Orthogonal frequency-division multiplexing (OFDM) effectively mitigates intersymbol interference (ISI) caused by the delay spread of wireless channels.",
              "OFDM has high data rate capability with reasonable computational complexity.",
              "TDM is highly efficient in scenarios where the data sources transmit data at regular intervals. If sources are idle, the allocated time slots can be seen as wasted bandwidth, but TDM can be adapted to minimize this.",
              "FDM is advantageous for continuous data streams like television broadcasting and radio. It minimizes interference and maximizes the use of available bandwidth. Guard bands are often required to prevent overlap, which can slightly reduce the efficiency.",
              "WDM enhances the data transmission capacity enormously as it allows the transmission of multiple signals through the same fiber without significant interference.",
              "CDM is highly efficient for wireless communication systems as it allows multiple users to share the same frequency spectrum with minimal interference.",
              "Multiplexing allows for more efficient utilization of available bandwidth and physical media. By combining multiple signals into one, it reduces the need for multiple channels, effectively conserving resources.",
              "By enabling the transmission of multiple signals simultaneously, multiplexing increases the overall data transmission capacity of the network, which is essential for handling large volumes of data and multiple users.",
              "Multiplexing allows for easy integration of new signals and users into the communication network without requiring significant changes to the existing infrastructure.",
              "Multiplexing VoIP packets for payload size reduction significantly cuts overhead... The combination of the huge packet header with payload usually causes huge overhead, could be technically reduced by multiplexing the related payloads in one header... A Performance investigation on bandwidth efficiency for multiplexing 10 users in each stream showed that a cumulative level of up to 68% -72% of bandwidth is saved, depicting an improvement in network performance with respect to network traffic, overload and packet congestion."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide the types of multiplexing techniques used in communication networks.",
            "weight": 0.17142857142857143,
            "evidence": [
              "Analog multiplexing and digital multiplexing are the major classification.",
              "Frequency division multiplexing [1,2,3,4] is a networking technique which combines many signals into a single one and then transmitted the combined signal through a common communication channel.",
              "Fiber-optic communications require a different kind of multiplexer called a wavelength division multiplexer (WAD) [2,4]. It is an analog multiplexing technique. It is designed for high data rate capability fiber cable.",
              "FDM [1,2,3,4] is based on sharing of the available bandwidth of a communication channel among the signals to be transmitted. It is an analog multiplexing technique that uses a single transmission medium which is divided into several frequency channels.",
              "In time division multiplexing (TDM) [1,2,3,4], all signals operate with the same frequency at different times, i.e., it is a technique of transmitting several signals over a single communication channel by dividing the time frame into equal slots.",
              "Code division multiplexing (CDM) [3] is a form of multiplexing in which the transmitter encodes the signal by using a unique chip code which is generated by a pseudorandom sequence generator. It uses spread-spectrum communication, and a narrowband signal is spread over a large band of frequency; it allows multiple signals from multiple users to share a common communication channel.",
              "Orthogonal frequency division multiplexing (OFDM) [4,5] is a multiplexing technique used in broadband communication system. It is a multicarrier modulation scheme. Now it is used in 4G broadband communication system and next-generation systems. OFDM is popular in broadband wireless systems due to its resistance to multipath fading.",
              "Digital multiplexer [6,7,8] or data selector is a logic circuit that has several input lines and a single output line. It also consists of data selector switch which is used to select the inputs and permit the data into the device to output."
            ]
          },
          {
            "name": "nice_to_have_item_0",
            "criterion": "The answer should provide challenges associated with various types on Multiplexing.",
            "weight": 0.08571428571428572,
            "evidence": [
              "The main drawbacks of OFDM are its high peak to average power ratio and its sensitivity to phase noise and frequency offset.",
              "All the FDM channels get affected due to wideband fading.",
              "In asynchronous TDM, the capacity of the transmission link must be higher than the total capacity of input lines.",
              "An output slot in synchronous TDM is totally occupied by data, in statistical TDM; a slot needs to carry data as well as the address of the destination.",
              "In CDM, as the number of users increases, the overall quality of services decreases."
            ]
          }
        ]
      }
    },
    "case_id": "ba4b2561ecf64f1c1cc1e16e207312be",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are the leading approaches to automatic scientific paper review generation and what are their strengths and weaknesses?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are the leading approaches to automatic scientific paper review generation and what are their strengths and weaknesses?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with providing the leading approaches used in scientific paper review generation.",
            "weight": 0.19999999999999998,
            "evidence": [
              "it builds upon a relatively small knowledge base (some tens of reviews) while commonly used methods for text generation, such as Artificial Neural Networks (ANN), typically require a very large amount of data in order to build an effective generative model.",
              "From a broader point of view, our proposal is a form of Natural Language Generation (NLG), which is widely used in many different fields such as spoken dialogue systems [16], machine translation [17], and as a mean for creating editorial content by turning structured data into prose [18].",
              "A notable use of NLG for scientific purpose, which is particularly relevant to our work, is the software SCIgen.https://link.springer.com/chapter/10.1007/978-3-319-45507-5_2\"This tool generates pdf files consisting of syntactically correct random text which is formatted like a scientific publication, including randomly generated figures, plots, and code fragments.",
              "Our work proposes acorpus-basedNLG method. Corpus-based methods aim at training text generation rules automatically from text examples of the desired text generator output.",
              "When generating a review for a paperawith a specified recommendationo, our method performs 3 steps, described below in full detail: (i) it builds a setSof sentences from reviews inRand replaces each specific term in each sentence with a specific term ofa; (ii) it removes fromSthe sentences which express a sentiment which is not consistent witho; (iii) it reorders and concatenates the sentences inSobtaining a review fora.",
              "Sentiment analysis.In this step, we aim at selecting the sentences ofSwhich express a sentiment consistent with the specified overall recommendationo. To this end, we apply a pre-trained Naive Bayes sentiment classifier to each sentence s[?]S, basing on the assumption that a positive sentiment can be associated with an accept recommendation, a negative sentiment with a reject recommendation, and a neutral sentiment with a neutral recommendation.",
              "This method consists of two main parts. The first part identifies key papers in the area by their bibliometric parameters, such as a graph of co-citations. The second stage uses a BERT based architecture that we train on existing reviews for extractive summarization of these key papers.",
              "a novel method for automatic review generation that combines bibliometric analysis of a specific research area to identify key papers together with a BERT-based deep neural network trained to extract the most relevant sentence from these key papers.",
              "We preprocess the raw data into the following data tables. Each has a PMID (a unique identifier of the paper) as key: * Lists of sentences of each paper that we can use to quickly find all sentences of a paper or sentences under certain numbers. It contains sentence's number in the paper, and the text of the sentence itself. * Abstracts. * Image caption texts. * Table caption texts. * List of review paper PMIDs. Lists PMIDs of all review papers that cite this paper. * Citations. PMIDs of cited papers.",
              "The automatic review pipeline starts with a search query by keywords or phrases, where users can choose the most recent, the most cited, or the most relevant papers and limit the size of search results. A citation graph is built on-thefly and shows overall publication dynamics in time and is used to detect the most popular articles, authors, and journals. We use a hybrid approach for papers similarity determination - a combination of citation, co-citation graph features, bibliographic coupling, and text-based similarity based on the TF-IDF metric",
              "We employ one of the most popular algorithms for community detection - Louvain [28] algorithm to extract closely related groups of papers (topics) from the similarity graph. For each topic, the service shows a word cloud of topic-specific keywords and detailed information about included papers.",
              "The model introduced takes in several sentences separated by specific tags \"[CLS]\" and \"[SEP]\" that mark the beginning and end of the fragments of interest, i.e., the sentences. For each input sentence i the BERTSUM model produces a vector Ti, which will be further perceived as a vector representation of the original sentence. Then these vectors are transferred to the linear layer, which will give an estimate of the quality of the sentence for the summary Yi.",
              " Our system consists of two steps: sentence ranking and sentence selection. In the sentence ranking step, the system ranks sentences in the input papers by regarding aspects as queries. We use LexRank and also incorporate query expansion and word embedding to compensate for tersely expressed queries. In the sentence selection step, the system selects sentences that remain in the final output.",
              "Actually, LexRank was demonstrated as useful for query-focused summarization with a small modification to the algorithm [17], which we will call QLexRank. Q-LexRank adds query relevance to edge weights to value sentences. Overview of the proposed method. that are related to the query. The score p(s | q) of a sentence s given a query q.",
              "Add words that frequently co-occur with the query words in document Dithe system is processing (cooccur). - In addition to the words added to cooccur, add frequently co-occurring words in the entire document set D1, . . . , DK(cooccur+).",
              "In the sentence selection step, the system selects sentences from the rankings computed in the ranking step to reduce the redundancy of the resulting summaries. We use an ILP-based model.",
              "Generally,deep learningclassifiers usingCNNlayers,LSTMlayers, or a hybrid combination have achieved good accuracy results when applied to annotated datasets with human-generated fake reviews.",
              "One of the fundamental approaches to automatic review generation involves using advanced NLP techniques to summarize and analyze the content of scientific papers.",
              "Multi-modal ApproachesThese methods incorporate not just text but also figures, tables, and other visual elements in scientific papers.",
              "Template-based methods utilize pre-defined templates to generate reviews.",
              "These approaches rely on machine learning models, particularly NLP and deep learning techniques, to understand and generate human-like text. BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) are often employed.",
              "As AI and NLP technologies advance, hybrid and domain-specific models, augmented with explainable AI and human oversight, hold promise for significantly enhancing the scientific review process, creating a collaborative future between AI and human reviewers."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide the strengths of leading approaches to automatic scientific paper review generation.",
            "weight": 0.19999999999999998,
            "evidence": [
              "The goal of comparative summarization is to highlight differences among given documents.",
              "Automatic scientific paper review generation is an emerging field within natural language processing and artificial intelligence. This task aims to automatically produce comprehensive, critical evaluations of scientific papers, similar to those written by human peer reviewers.",
              "Strengths of NLP:- Can efficiently process large volumes of text- Capable of extracting key information and main ideas- Can identify structural elements of papers (e.g., abstract, methodology, results)",
              "Strengths of sentiment analysis:- Can assess the overall tone and stance of a paper- Useful for identifying potential biases or overly positive self-presentation",
              "Strengths of supervised learning model:- Can learn from a wide range of existing reviews- Capable of generating human-like text- Can be fine-tuned for specific scientific domains\"Strengths of transformer-based models:- Can generate coherent and fluent text- Capable of understanding and generating domain-specific language- Can be prompted to focus on specific aspects of paper review",
              "Strengths of multi-modal approaches:- Can provide a more holistic evaluation of the paper- Better suited for fields where visual elements are crucial (e.g., computer vision, medical imaging)"
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide the weaknesses of leading approaches to automatic scientific paper review generation.",
            "weight": 0.19999999999999998,
            "evidence": [
              "A scientific paper's text is generally much longer than BERT can process during one iteration. BERT model can accept 10-15 sentences per iteration.",
              "The task is more challenging than multi-document summarization of generic text since each scientific paper has its own characteristics, contributions, approach, and specific content to its own work.",
              "the candidate summary is compared with a reference summary where there is no single 'ideal.' Therefore, a good summary may be penalized for including relevant sentences that are not in the reference summary.",
              " They adopted the pyramid evaluation method (Nenkova and Passonneau, 2004) and compared their proposed approaches with state-of-the-art baselines such as LexRank (Erkan and Radev, 2004), C-LexRank (Qazvinian and Radev, 2008), and MMR (Carbonell and Goldstein, 1998). Their approach outperformed the other baselines for n-grams of sizes 1, 2, and 3. However, it produced low variation and more stable results with respect to summary quality for equivalent n-gram sizes.",
              "The main related issues include the unavailability of training and test benchmark corpora, gold standard summaries for comparison, suitable evaluation metrics, and baseline systems needed for comparison purposes.",
              "Most of the surveyed studies are centered around citation-based approaches and targeted a single article. Further research is needed to advance knowledge in this field by shifting from single-article to multi-article summarization and from extractive to abstractive in order to enhance the coherence and readability of the output.",
              "However, the ability of automatic generators to resemble truthful reviews has improved so much that they represent a real threat to currentdiscriminators.",
              "Weaknesses of NLP:- May struggle with understanding complex scientific concepts- Limited ability to provide critical analysis or identify flaws in methodology- Lacks domain-specific knowledge required for in-depth evaluation",
              "Weaknesses of Sentiment analysis:- May not capture nuanced scientific arguments- Can be misled by technical jargon or field-specific terminology",
              "Weaknesses of transformer-based models:- May generate plausible-sounding but factually incorrect content- Ethical concerns regarding the use of AI in peer review processes- High computational requirements",
              "Weaknesses of multi-modal approaches:- Increased complexity in data processing and model architecture- May struggle with interpreting complex visualizations or datasets"
            ]
          }
        ]
      }
    },
    "case_id": "aab38dd1282ff3387cc8bf9bdf13b3aa",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "Does active learning work well when fine-tuning large language models?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "Does active learning work well when fine-tuning large language models?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with providing a background on active learning.",
            "weight": 0.19999999999999998,
            "evidence": [
              "The standard active learning pipeline iterates over the following steps: (1) train a model on the labeled subset of data, (2) query this model to select unlabeled samples for annotation, and (3) label chosen samples and move them to the labeled split (Lewis and Gale, 1994).",
              ". Active learning can help us to identify the most informative samples that are then labelled manually, even if there are initially no labelled samples.",
              "The aim of active learning is to minimize the effort of labelling data, while simultaneously maximizing the model's performance. This is achieved by selecting a query strategy that chooses the most interesting samples from a set of unlabelled data points, which we refer to as most informative samples. These samples are then passed to, e.g., a human annotator for labelling. The usual setup consists of iteratively training a model on a small subset of manually labelled data that stems from a pool of unlabelled data.",
              "Active learning shows promise in improving the efficiency and performance of fine-tuning large language models, particularly when combined with task adaptation.",
              "Active learning (AL) has emerged as a promising approach to enhance the efficiency and effectiveness of fine-tuning large language models (LLMs).",
              "Active learning is a machine learning paradigm where the algorithm can interactively query a user or other information source to obtain the desired outputs at new data points [1]. The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns.",
              "Active learning is a subset of machine learning where the algorithm selectively queries the most informative data points from a pool of unlabeled data for labeling by an oracle (often a human annotator). The goal is to improve model performance with fewer labeled examples compared to passive learning, where data points are selected randomly for labeling."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide the benefits of active learning when fine-tuning large language models.",
            "weight": 0.19999999999999998,
            "evidence": [
              "Broadly speaking, Active Learning aims to reduce the expenses associated with human annotation, achieving this by strategically selecting the most informative data points for labeling.",
              " Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON achieves 91.0% and 92.1% of the fully supervised performance based on vanilla finetuning and prompt-based learning respectively.",
              "Second, we further propose a simple yet effective fine-tuning method that is robust in both low and high resource data settings for AL.",
              " We also observe that in all five datasets, the addition of our proposed pretraining step (TAPT) and fine-tuning technique (FT+) leads to large performance gains, especially in the first AL iterations. This is particularly evident in TREC-6, DBPEDIA and IMDB datasets, where after the first AL iteration (i.e. equivalent to 2% of training data) TAPT+FT+ with ENTROPY is 45, 30 and 12 points in accuracy higher than the ENTROPY baseline with BERT and SFT.",
              "an ablation study to evaluate the contribution of our two proposed steps to the AL pipeline; the pretraining step (TAPT) and finetuning method (FT+). We show that the addition of both methods provides large gains compared to standard fine-tuning (SFT) in terms of accuracy, data efficiency and uncertainty calibration.",
              "For TREC-6, training BERT with our fine-tuning approach FT+ provides large gains both in accuracy and uncertainty calibration, showing the importance of fine-tuning the LM for a larger number of epochs in low resource settings.",
              "For the larger dataset, AGNEWS, we see that BERT with SFT performs equally to FT+ which is the ideal scenario. We see that our fine-tuning approach does not deteriorate the performance of BERT given the large increase in warmup steps, showing that our simple strategy provides robust results in both high and low resource settings.",
              "We see that augmenting the finetuning dataset with synthetic data improves the performance of the stance detection model.",
              "we propose an approach to tune the representations generated by BERT-like transformer models during the active learning process, Adaptive Tuning Active Learning. Our experiments show that the limited label information acquired in active learning can not only be used for training a classifier but can also adaptively improve the embeddings generated by the BERT-like language models as well.",
              "Utilizing instruction-tuned LLMs presents several advantages for active learning with pre-trained transformers like BERT.",
              "When combined with pre-trained language models (PLMs), AL has shown potential to reduce label complexity and improve model performance.",
              "AL has demonstrated promise in enhancing label efficiency for novel tasks, addressing bottlenecks in label acquisition for LLMs.",
              "Various strategies have been developed to enhance active learning for LLMs, including specialized fine-tuning methods, self-training approaches, and novel sampling techniques. These improvements aim to increase label efficiency, reduce computational costs, and optimize performance across different tasks and resource settings.",
              " L-Tuning has shown promise as a scalable and efficient approach for optimizing LLMs, outperforming conventional prompt and prefix tuning in classification tasks.",
              "Yu et al. explored active fine-tuning of pre-trained language models to improve label efficiency, achieving comparable performance to fully-supervised methods with fewer annotated samples.",
              "Margatina et al. demonstrated that active learning algorithms can serve as effective demonstration selection methods for in-context learning across various GPT and OPT models.",
              "Active learning has been found to be particularly effective for tasks where the distribution of informative examples is highly skewed, such as in sentiment analysis or named entity recognition [4].",
              "Large language models (LLMs), such as OpenAI's GPT-3, Google's BERT, and their successors, have shown remarkable performance across a variety of natural language processing (NLP) tasks. Fine-tuning these pre-trained models on specific datasets can yield significant performance improvements in domain-specific applications."
            ]
          },
          {
            "name": "most_important_item_2",
            "criterion": "The answer should provide challenges associated with active learning in LLMs",
            "weight": 0.19999999999999998,
            "evidence": [
              "Similarly, Large Language Models (LLMs) such as GPT-3.5 provide an alternative for automated annotation but come with concerns regarding their reliability.",
              "However, Active Learning is not the only strategy employed to minimize human annotation costs.",
              "Moreover, cold-start data selection requires greater care to ensure the sample diversity compared to the traditional AL, as fine-tuning PLMs on few redundant data will lead to poor generalization.",
              "While there are several methods that leverage pretrained embeddings (Hacohen et al., 2022; Chang et al., 2021) or masked language modeling (MLM) loss (Yuan et al., 2020) to assist data selection, the mismatch between pre-training and fine-tuning tasks hurts their efficacy.",
              "Only recently, pretrained LMs such as BERT (Devlin et al., 2019) have been introduced in AL settings (Yuan et al., 2020; Ein-Dor et al., 2020; Shelmanov et al., 2021; Karamcheti et al., 2021; Margatina et al., 2021). Still, they are trained at each AL iteration with a standard fine-tuning approach that mainly includes a pre-defined number of training epochs, which has been demonstrated to be unstable, especially in small datasets.https://arxiv.org/pdf/2104.08320\"Fine-tuning transformer-based models (Vaswani et al., 2017) to solve stance detection is a common practice, but training these models requires a large amount of annotated data.",
              " Since these LLMs exhibit substantial zero-shot capabilities, they overcome the cold start problem and can be directly applied to a dataset. This is crucial for models like BERT, which already demonstrate strong performance in few-shot settings, rendering active learning strategies that face the cold start problem less useful.",
              "Fine-tuning Large Language Models (LLMs) is now a common approach for text classification in a wide range of applications. When labeled documents are scarce, active learning helps save annotation efforts but requires retraining of massive models on each acquisition iteration.",
              "However, in many practical scenarios, downstream text datasets are either scarcely labeled or are unlabeled at all, restricting supervised transfer learning. At the same time, manual labeling is often laborious and costly, which calls for a careful + work performed while interning at Bloomberg and targeted selection of examples for annotation using techniques such as active learning (Schroder and Niekler, 2020).",
              "When used together with transfer learning from LLMs, this procedure requires sequentially re-fine-tuning models with up to billions of parameters, which is very expensive, if feasible at all.",
              "However, the effectiveness of AL in fine-tuning LLMs is not uniform across all scenarios and depends on various factors.",
              "Active learning with LLMs faces challenges such as high computational costs, the need for large amounts of representative data, and difficulties in scenarios with limited labeled data. These limitations can impact the effectiveness of active learning strategies when fine-tuning large language models.",
              "Many traditional active learning algorithms may not scale well to the size of datasets typically used with LLMs [7]."
            ]
          }
        ]
      }
    },
    "case_id": "983e73defc06e6794a856330905dc787",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  },
  {
    "initial_prompt": "What are some tasks where fine tuning smaller models is beneficial over using LLMs?",
    "metric_config": {
      "name": "rubric_corpusqa_generic",
      "config": {
        "question": "What are some tasks where fine tuning smaller models is beneficial over using LLMs?",
        "low_length": 300,
        "high_length": 600,
        "length_weight": 0.05,
        "expertise_weight": 0.05,
        "citations_weight": 0.2,
        "excerpts_weight": 0.1,
        "other_properties": [
          {
            "name": "most_important_item_0",
            "criterion": "The answer should begin with limitations of the fine-tuning of LLMs.",
            "weight": 0.3,
            "evidence": [
              "However, fine-tuning LLMs on large-scale recommendation data demands substantial computational resources and time costs [26], thereby diminishing the practicality of LLM-based recommender models in real-world applications.",
              "Despite its efficiency, randomly sampled data may lack sufficient representativeness to enable LLMs to effectively comprehend new items and user behaviour."
            ]
          },
          {
            "name": "most_important_item_1",
            "criterion": "The answer should provide the tasks where fine tuning smaller models is beneficial over using LLMs.",
            "weight": 0.3,
            "evidence": [
              "Light, local, fine-tuned models should be considered rather than LLMs, especially for those who are sensitive to the cost or have strict latency requirements. ParameterEfficient tuning can be a viable option for model deployment and delivery.",
              " The zero-shot approach of LLMs prohibits the learning of shortcuts from task-specific datasets, which is prevalent in fine-tuned models. Nevertheless, LLMs still demonstrate a degree of shortcut learning issues.",
              "a novel data pruning method incorporating two scores, namely influence score and effort score, to efficiently identify the influential samples. Particularly, the influence score is introduced to accurately estimate the influence of removing each sample on the overall performance. To achieve low costs of the data pruning process, we employ a small-sized surrogate model to replace LLMs to obtain the influence score.",
              "To achieve high efficiency, one possible solution is to train a surrogate model for sample selection, e.g., using a small-sized traditional recommender model, which can drastically reduce the GPU memory usage and the training time compared to LLMs.",
              "To reduce the costs, we propose utilizing a surrogate model, e.g., a small-sized traditional recommender model, to compute the influence scores. Nevertheless, since LLMs acquire rich world knowledge during the pre-training stage, they intricately possess different learning abilities compared to the surrogate model. Therefore, the influential samples on LLMs might deviate from the ones for LLMs.",
              "If the group size is smaller than the average sampling budget, we select all users from this group and update the average sampling budget for the remaining groups.",
              "Fine-tuned smaller models often outperform larger language models on specific, well-defined tasks when sufficient training data is available. These tasks range from basic language understanding to more complex reasoning and information extraction tasks.",
              "Specifically for sentiment analysis, fine-tuned models have demonstrated better performance on datasets like SST-2.",
              " Smaller models fine-tuned with techniques like distilling step-by-step have outperformed larger language models on tasks such as ANLI (Adversarial Natural Language Inference), using over 700 times fewer parameters.",
              "Additionally, adapter-based PEFT approaches have enabled smaller models to surpass GPT-3.5 on simpler arithmetic reasoning tasks like MultiArith, AddSub, and SingleEq.",
              "Fine-tuned smaller models have shown notable advantages in Named Entity Recognition (NER) and Relation Extraction (RE) tasks, especially when domain-specific training data is available.",
              "For more complex tasks like text infilling, fine-tuned models, even with significantly fewer parameters, have tended to outperform few-shot learning with larger models.",
              "When sufficient in-domain data is available, fine-tuning smaller domain-specific models has proven to be particularly effective, especially for structure prediction tasks like NER.",
              "Fine-tuned smaller models have shown benefits in tasks involving counterfactual data generation, particularly for sentiment analysis, where they achieved significant performance improvements.",
              "Smaller language models that are fine-tuned on instructions, trained on dialog data, and/or trained on multiple dialog tasks outperform larger language models.",
              "Specialized smaller models can operate on low-cost, in-house computing infrastructure, offering advantages in speed and reduced training costs that outweigh any performance gains from large foundation LLMs."
            ]
          }
        ]
      }
    },
    "case_id": "a5d23eb3a2555db0a82f6b64fed85baa",
    "annotator": "Annotator 2 Assignments",
    "agreement": false
  }
]