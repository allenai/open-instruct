# Finegrained Rewards V1

## Overview

The V1 finegrained reward system provides span-level feedback for different reward components in the longform RL-RAG setting. Instead of just returning overall scores, it identifies specific text spans that contribute to each reward type, enabling more targeted training.

## Key Features

- **Span-based rewards**: Each reward component identifies specific character spans in the response
- **Multiple reward types**: Format, search turns, rubric, and citation rewards
- **Positive/negative span logic**: Different handling for positive vs negative rewards
- **Modular design**: Reusable span identification functions

## Reward Types & Span Logic

### 1. Format Rewards (Groups 0-2)
- **Answer format (Group 0)**: Rewards `<answer>...</answer>` tags when present, penalizes whole response when absent
- **Citation format (Group 1)**: Rewards each `<cite>...</cite>` tag when present, penalizes whole response when absent  
- **Query format (Group 2)**: Rewards each `<query>...</query>` tag when present, penalizes whole response when absent

### 2. Search Turns Reward (Group 3)
- **Positive**: Rewards content inside `<query>` tags (not the tags themselves)
- **Negative**: Penalizes whole response

### 3. Rubric Reward (Group 4)
- **Target**: Content between `<answer>` tags
- **Fallback**: Whole response if no answer tags found

### 4. Citation Reward (Group 5)
- **Target**: Content inside `<cite>` tags
- **Negative only**: Penalizes whole response if no citations and reward â‰¤ 0

## Output Format

```python
{
    "finegrained_scores": [
        (score, (start_char, end_char), reward_group_id, response_idx),
        # ... more tuples
    ],
    "log_values": {
        "format_reward": float,
        "num_search_turns_reward": float,
        "rubric_reward": float,
        "citation_reward": float,
        "overall_reward": float,
        # ... additional metrics
    }
}
```

## Usage

```python
from open_instruct.search_rewards.longform_finegrained_rewards_v1 import compute_longform_finegrained_reward

result = compute_longform_finegrained_reward(response, ground_truth, question)
```

## Files

- **`find_reward_spans.py`**: Core span identification functions (reusable)
- **`longform_finegrained_rewards_v1.py`**: Main reward computation with spans
- **Backward compatibility**: Original `compute_longform_averaged_outcome_reward()` maintained

## Reward Group IDs

| ID | Type | Description |
|----|------|-------------|
| 0 | Answer Format | `<answer>` tag presence |
| 1 | Citation Format | `<cite>` tag presence |
| 2 | Query Format | `<query>` tag presence |
| 3 | Search Turns | Query content quality |
| 4 | Rubric | Answer content quality |
| 5 | Citation | Citation content quality | 