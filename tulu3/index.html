
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://github.com/allenai/open-instruct/tulu3/">
      
      
      
      
      <link rel="icon" href="../assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.22">
    
    
      
        <title>Tulu3 Reproduction - Open Instruct</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="ai2-dark" data-md-color-primary="custom" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#tulu3-reproduction" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Open Instruct" class="md-header__button md-logo" aria-label="Open Instruct" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Open Instruct
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Tulu3 Reproduction
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="ai2-dark" data-md-color-primary="custom" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2a7 7 0 0 0-7 7c0 2.38 1.19 4.47 3 5.74V17a1 1 0 0 0 1 1h6a1 1 0 0 0 1-1v-2.26c1.81-1.27 3-3.36 3-5.74a7 7 0 0 0-7-7M9 21a1 1 0 0 0 1 1h4a1 1 0 0 0 1-1v-1H9z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="ai2" data-md-color-primary="custom" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2a7 7 0 0 1 7 7c0 2.38-1.19 4.47-3 5.74V17a1 1 0 0 1-1 1H9a1 1 0 0 1-1-1v-2.26C6.19 13.47 5 11.38 5 9a7 7 0 0 1 7-7M9 21v-1h6v1a1 1 0 0 1-1 1h-4a1 1 0 0 1-1-1m3-17a5 5 0 0 0-5 5c0 2.05 1.23 3.81 3 4.58V16h4v-2.42c1.77-.77 3-2.53 3-4.58a5 5 0 0 0-5-5"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/allenai/open-instruct" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    allenai/open-instruct
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Open Instruct" class="md-nav__button md-logo" aria-label="Open Instruct" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Open Instruct
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/allenai/open-instruct" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    allenai/open-instruct
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Get Started
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Get Started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../get_started/installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Installation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../get_started/ai2_internal_setup/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Ai2 Internal Setup
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../algorithms/dataset_transformation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dataset Transformations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../algorithms/trained_model.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    None
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../algorithms/finetune/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Supervised finetuning (SFT)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../algorithms/dpo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Direct Preference Optimization (DPO)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../algorithms/grpo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Grouped Relative Policy Optimization (GRPO)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../algorithms/ppo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Proximal Policy Optimization (PPO)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../algorithms/reward_modeling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reward Modeling (RM)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Not Maintained
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Not Maintained
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../algorithms/synthetic_preference_dataset/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Synthetic preference dataset
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#finetuning" class="md-nav__link">
    <span class="md-ellipsis">
      Finetuning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Finetuning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llama-31-tulu-3-8b-sft-reproduction" class="md-nav__link">
    <span class="md-ellipsis">
      Llama-3.1-Tulu-3-8B-SFT Reproduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-tulu-3-70b-sft-reproduction" class="md-nav__link">
    <span class="md-ellipsis">
      Llama-3.1-Tulu-3-70B-SFT Reproduction
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#preference-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      Preference Tuning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Preference Tuning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llama-31-tulu-3-8b-dpo-reproduction" class="md-nav__link">
    <span class="md-ellipsis">
      Llama-3.1-Tulu-3-8B-DPO Reproduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-tulu-3-70b-dpo-reproduction" class="md-nav__link">
    <span class="md-ellipsis">
      Llama-3.1-Tulu-3-70B-DPO Reproduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-tulu-3-405b-dpo-reproduction" class="md-nav__link">
    <span class="md-ellipsis">
      Llama-3.1-Tulu-3-405B-DPO Reproduction
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rlvr" class="md-nav__link">
    <span class="md-ellipsis">
      RLVR
    </span>
  </a>
  
    <nav class="md-nav" aria-label="RLVR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rlvr-for-if-note" class="md-nav__link">
    <span class="md-ellipsis">
      RLVR for IF Note:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-tulu-3-8b-rm-reproduction" class="md-nav__link">
    <span class="md-ellipsis">
      Llama-3.1-Tulu-3-8B-RM Reproduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-tulu-3-8b-reproduction" class="md-nav__link">
    <span class="md-ellipsis">
      Llama-3.1-Tulu-3-8B Reproduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-tulu-3-70b-reproduction" class="md-nav__link">
    <span class="md-ellipsis">
      Llama-3.1-Tulu-3-70B Reproduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-tulu-3-405b-reproduction" class="md-nav__link">
    <span class="md-ellipsis">
      Llama-3.1-Tulu-3-405B Reproduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#new-llama-31-tulu-31-8b-reproduction" class="md-nav__link">
    <span class="md-ellipsis">
      (NEW) Llama-3.1-Tulu-3.1-8B Reproduction
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="tulu3-reproduction">Tulu3 Reproduction</h1>
<p>This document details the commands and configs to reproduce the tulu3 models.</p>
<h2 id="finetuning">Finetuning</h2>
<h3 id="llama-31-tulu-3-8b-sft-reproduction">Llama-3.1-Tulu-3-8B-SFT Reproduction</h3>
<p>Below is (almost) the exact command which produced <a href="https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-SFT">Llama-3.1-Tulu-3-8B-SFT</a>. We deployed the command across 8 machines, each equipped with 8 NVIDIA H100 GPUs, for a total of 64 GPUs in the our setup.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># modify the following `MACHINE_RANK`, `MAIN_PROCESS_IP`,</span>
<span class="c1"># `NUM_MACHINES`, `NUM_PROCESSES`, `PER_DEVICE_TRAIN_BATCH_SIZE`,</span>
<span class="c1"># `GRADIENT_ACCUMULATION_STEPS` according to your setup</span>
<span class="nv">MACHINE_RANK</span><span class="o">=</span><span class="m">0</span>
<span class="nv">MAIN_PROCESS_IP</span><span class="o">=</span>localhost
<span class="nv">NUM_MACHINES</span><span class="o">=</span><span class="m">8</span>
<span class="nv">NUM_PROCESSES</span><span class="o">=</span><span class="m">64</span>
<span class="nv">PER_DEVICE_TRAIN_BATCH_SIZE</span><span class="o">=</span><span class="m">1</span>
<span class="nv">GRADIENT_ACCUMULATION_STEPS</span><span class="o">=</span><span class="m">2</span>
accelerate<span class="w"> </span>launch<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--mixed_precision<span class="w"> </span>bf16<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_machines<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_processes<span class="w"> </span><span class="m">64</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--machine_rank<span class="w"> </span><span class="nv">$MACHINE_RANK</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--main_process_ip<span class="w"> </span><span class="nv">$MAIN_PROCESS_IP</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--main_process_port<span class="w"> </span><span class="m">29400</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_deepspeed<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deepspeed_config_file<span class="w"> </span>configs/ds_configs/stage3_no_offloading_accelerate.conf<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deepspeed_multinode_launcher<span class="w"> </span>standard<span class="w"> </span>open_instruct/finetune.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_name_or_path<span class="w"> </span>meta-llama/Llama-3.1-8B<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tokenizer_name<span class="w"> </span>meta-llama/Llama-3.1-8B<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_slow_tokenizer<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_flash_attn<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_seq_length<span class="w"> </span><span class="m">4096</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--preprocessing_num_workers<span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_train_batch_size<span class="w"> </span><span class="nv">$PER_DEVICE_TRAIN_BATCH_SIZE</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_accumulation_steps<span class="w"> </span><span class="nv">$GRADIENT_ACCUMULATION_STEPS</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning_rate<span class="w"> </span>5e-06<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lr_scheduler_type<span class="w"> </span>linear<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--warmup_ratio<span class="w"> </span><span class="m">0</span>.03<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--weight_decay<span class="w"> </span><span class="m">0</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_train_epochs<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>output/sft_8b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with_tracking<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--report_to<span class="w"> </span>wandb<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--logging_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_revision<span class="w"> </span>main<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_list<span class="w"> </span>allenai/tulu-3-sft-mixture<span class="w"> </span><span class="m">1</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--checkpointing_steps<span class="w"> </span>epoch<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mix_dir<span class="w"> </span>output/sft_8b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--exp_name<span class="w"> </span>tulu-3-8b-sft<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--seed<span class="w"> </span><span class="m">123</span>
<span class="c1"># For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JBNTPW8TKG09B2XR832YB5S8</span>
</code></pre></div>
<blockquote>
<p>[!NOTE]
If you have different number of GPUs, please adjust the <code>NUM_MACHINES</code>, <code>NUM_PROCESSES</code>, <code>PER_DEVICE_TRAIN_BATCH_SIZE</code>, and <code>GRADIENT_ACCUMULATION_STEPS</code> accordingly to reproduce the same effective batch size.
The effective batch size is calculated by multiplying:
- Number of GPUs / processes (NUM_PROCESSES)
- Train batch size per GPU (PER_DEVICE_TRAIN_BATCH_SIZE) 
- Gradient accumulation steps (GRADIENT_ACCUMULATION_STEPS)
so we have
<div class="highlight"><pre><span></span><code>64 GPUs: 64 * 1 * 2 = 128 # from the example above
8 GPUs:   8 * 1 * 16 = 128 # if you only 
</code></pre></div>
You can achieve the same effective batch size with fewer GPUs by increasing gradient accumulation steps proportionally (e.g., <code>NUM_PROCESSES=8, PER_DEVICE_TRAIN_BATCH_SIZE=1, and GRADIENT_ACCUMULATION_STEPS=16</code>)</p>
</blockquote>
<h3 id="llama-31-tulu-3-70b-sft-reproduction">Llama-3.1-Tulu-3-70B-SFT Reproduction</h3>
<p>This is (almost) the exact command which produced <a href="https://huggingface.co/allenai/Llama-3.1-Tulu-3-70B-SFT">allenai/Llama-3.1-Tulu-3-70B-SFT</a></p>
<div class="highlight"><pre><span></span><code><span class="c1"># modify the following `MACHINE_RANK`, `MAIN_PROCESS_IP`,</span>
<span class="c1"># `NUM_MACHINES`, `NUM_PROCESSES`, `PER_DEVICE_TRAIN_BATCH_SIZE`,</span>
<span class="c1"># `GRADIENT_ACCUMULATION_STEPS` according to your setup</span>
<span class="nv">MACHINE_RANK</span><span class="o">=</span><span class="m">0</span>
<span class="nv">MAIN_PROCESS_IP</span><span class="o">=</span>localhost
<span class="nv">NUM_MACHINES</span><span class="o">=</span><span class="m">8</span>
<span class="nv">NUM_PROCESSES</span><span class="o">=</span><span class="m">64</span>
<span class="nv">PER_DEVICE_TRAIN_BATCH_SIZE</span><span class="o">=</span><span class="m">1</span>
<span class="nv">GRADIENT_ACCUMULATION_STEPS</span><span class="o">=</span><span class="m">2</span>
accelerate<span class="w"> </span>launch<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--mixed_precision<span class="w"> </span>bf16<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_machines<span class="w"> </span><span class="nv">$NUM_MACHINES</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_processes<span class="w"> </span><span class="nv">$NUM_PROCESSES</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--machine_rank<span class="w"> </span><span class="nv">$MACHINE_RANK</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--main_process_ip<span class="w"> </span><span class="nv">$MAIN_PROCESS_IP</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--main_process_port<span class="w"> </span><span class="m">29400</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_deepspeed<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deepspeed_config_file<span class="w"> </span>configs/ds_configs/stage3_no_offloading_accelerate.conf<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deepspeed_multinode_launcher<span class="w"> </span>standard<span class="w"> </span>open_instruct/finetune.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_name_or_path<span class="w"> </span>meta-llama/Llama-3.1-70B<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tokenizer_name<span class="w"> </span>meta-llama/Llama-3.1-70B<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_slow_tokenizer<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_flash_attn<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_seq_length<span class="w"> </span><span class="m">4096</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--preprocessing_num_workers<span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_train_batch_size<span class="w"> </span><span class="nv">$PER_DEVICE_TRAIN_BATCH_SIZE</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_accumulation_steps<span class="w"> </span><span class="nv">$GRADIENT_ACCUMULATION_STEPS</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning_rate<span class="w"> </span>2e-06<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lr_scheduler_type<span class="w"> </span>linear<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--warmup_ratio<span class="w"> </span><span class="m">0</span>.03<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--weight_decay<span class="w"> </span><span class="m">0</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_train_epochs<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>output/sft_70B<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with_tracking<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--report_to<span class="w"> </span>wandb<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--logging_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_revision<span class="w"> </span>main<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_list<span class="w"> </span>allenai/tulu-3-sft-mixture<span class="w"> </span><span class="m">1</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mix_dir<span class="w"> </span>output/sft_70B<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--checkpointing_steps<span class="w"> </span><span class="m">1000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--keep_last_n_checkpoints<span class="w"> </span><span class="m">20</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_checkpointing<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--exp_name<span class="w"> </span>tulu-3-70b-sft<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--seed<span class="w"> </span><span class="m">456</span>
<span class="c1"># For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JC5J4R80M18XQTDH47JSFRJY/</span>
</code></pre></div>
<h2 id="preference-tuning">Preference Tuning</h2>
<h3 id="llama-31-tulu-3-8b-dpo-reproduction">Llama-3.1-Tulu-3-8B-DPO Reproduction</h3>
<p>This is (almost) the exact command which produced <a href="https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-DPO">allenai/Llama-3.1-Tulu-3-8B-DPO</a></p>
<div class="highlight"><pre><span></span><code>accelerate<span class="w"> </span>launch<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--mixed_precision<span class="w"> </span>bf16<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_machines<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_processes<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_deepspeed<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deepspeed_config_file<span class="w"> </span>configs/ds_configs/stage3_no_offloading_accelerate.conf<span class="w"> </span>open_instruct/dpo_tune.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_name_or_path<span class="w"> </span>allenai/Llama-3.1-Tulu-3-8B-SFT<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_flash_attn<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tokenizer_name<span class="w"> </span>allenai/Llama-3.1-Tulu-3-8B-SFT<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_seq_length<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--preprocessing_num_workers<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_train_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning_rate<span class="w"> </span>5e-07<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lr_scheduler_type<span class="w"> </span>linear<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--warmup_ratio<span class="w"> </span><span class="m">0</span>.1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--weight_decay<span class="w"> </span><span class="m">0</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_train_epochs<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>output/dpo_8b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with_tracking<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--report_to<span class="w"> </span>wandb<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--logging_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_revision<span class="w"> </span>main<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_checkpointing<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_list<span class="w"> </span>allenai/llama-3.1-tulu-3-8b-preference-mixture<span class="w"> </span><span class="m">1</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_slow_tokenizer<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_lora<span class="w"> </span>False<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dpo_loss_type<span class="w"> </span>dpo_norm<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dpo_beta<span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--checkpointing_steps<span class="w"> </span><span class="m">1000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--exp_name<span class="w"> </span>tulu-3-8b-dpo
<span class="c1"># For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JCRXP0AR5312S8MD3XGCN0J7/</span>
</code></pre></div>
<h3 id="llama-31-tulu-3-70b-dpo-reproduction">Llama-3.1-Tulu-3-70B-DPO Reproduction</h3>
<p>This is (almost) the exact command which produced <a href="https://huggingface.co/allenai/Llama-3.1-Tulu-3-70B-DPO">allenai/Llama-3.1-Tulu-3-70B-DPO</a></p>
<div class="highlight"><pre><span></span><code><span class="c1"># modify the following `MACHINE_RANK`, `MAIN_PROCESS_IP`,</span>
<span class="c1"># `NUM_MACHINES`, `NUM_PROCESSES`, `PER_DEVICE_TRAIN_BATCH_SIZE`,</span>
<span class="c1"># `GRADIENT_ACCUMULATION_STEPS` according to your setup</span>
<span class="nv">MACHINE_RANK</span><span class="o">=</span><span class="m">0</span>
<span class="nv">MAIN_PROCESS_IP</span><span class="o">=</span>localhost
<span class="nv">NUM_MACHINES</span><span class="o">=</span><span class="m">8</span>
<span class="nv">NUM_PROCESSES</span><span class="o">=</span><span class="m">64</span>
<span class="nv">PER_DEVICE_TRAIN_BATCH_SIZE</span><span class="o">=</span><span class="m">1</span>
<span class="nv">GRADIENT_ACCUMULATION_STEPS</span><span class="o">=</span><span class="m">2</span>
accelerate<span class="w"> </span>launch<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--mixed_precision<span class="w"> </span>bf16<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_machines<span class="w"> </span><span class="nv">$NUM_MACHINES</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_processes<span class="w"> </span><span class="nv">$NUM_PROCESSES</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--machine_rank<span class="w"> </span><span class="nv">$MACHINE_RANK</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--main_process_ip<span class="w"> </span><span class="nv">$MAIN_PROCESS_IP</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--main_process_port<span class="w"> </span><span class="m">29400</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_deepspeed<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deepspeed_config_file<span class="w"> </span>configs/ds_configs/stage3_offloading_accelerate.conf<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deepspeed_multinode_launcher<span class="w"> </span>standard<span class="w"> </span>open_instruct/dpo_tune_cache.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_name_or_path<span class="w"> </span>allenai/Llama-3.1-Tulu-3-70B-SFT<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tokenizer_name<span class="w"> </span>allenai/Llama-3.1-Tulu-3-70B-SFT<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_flash_attn<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_seq_length<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--preprocessing_num_workers<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_train_batch_size<span class="w"> </span><span class="nv">$PER_DEVICE_TRAIN_BATCH_SIZE</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_accumulation_steps<span class="w"> </span><span class="nv">$GRADIENT_ACCUMULATION_STEPS</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning_rate<span class="w"> </span>2e-07<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lr_scheduler_type<span class="w"> </span>linear<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--warmup_ratio<span class="w"> </span><span class="m">0</span>.1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--weight_decay<span class="w"> </span><span class="m">0</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_train_epochs<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>output/dpo_70b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with_tracking<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--report_to<span class="w"> </span>wandb<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--logging_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_revision<span class="w"> </span>main<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_checkpointing<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_list<span class="w"> </span>allenai/llama-3.1-tulu-3-70b-preference-mixture<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_slow_tokenizer<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_lora<span class="w"> </span>False<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dpo_loss_type<span class="w"> </span>dpo_norm<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dpo_beta<span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--checkpointing_steps<span class="w"> </span>epoch<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--exp_name<span class="w"> </span>tulu-3-70b-dpo
<span class="c1"># For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JCSAYYHQYF9QDQDCV6KJ53M9/</span>
</code></pre></div>
<h3 id="llama-31-tulu-3-405b-dpo-reproduction">Llama-3.1-Tulu-3-405B-DPO Reproduction</h3>
<p>This is (almost) the exact command which produced <a href="https://huggingface.co/allenai/Llama-3.1-Tulu-3-405B-DPO">allenai/Llama-3.1-Tulu-3-405B-DPO</a></p>
<div class="highlight"><pre><span></span><code><span class="c1"># modify the following `MACHINE_RANK`, `MAIN_PROCESS_IP`,</span>
<span class="c1"># `NUM_MACHINES`, `NUM_PROCESSES`, `PER_DEVICE_TRAIN_BATCH_SIZE`,</span>
<span class="c1"># `GRADIENT_ACCUMULATION_STEPS` according to your setup</span>
<span class="nv">MACHINE_RANK</span><span class="o">=</span><span class="m">0</span>
<span class="nv">MAIN_PROCESS_IP</span><span class="o">=</span>localhost
<span class="nv">NUM_MACHINES</span><span class="o">=</span><span class="m">8</span>
<span class="nv">NUM_PROCESSES</span><span class="o">=</span><span class="m">64</span>
<span class="nv">PER_DEVICE_TRAIN_BATCH_SIZE</span><span class="o">=</span><span class="m">1</span>
<span class="nv">GRADIENT_ACCUMULATION_STEPS</span><span class="o">=</span><span class="m">2</span>
accelerate<span class="w"> </span>launch<span class="w"> </span>--mixed_precision<span class="w"> </span>bf16<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_machines<span class="w"> </span><span class="m">32</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_processes<span class="w"> </span><span class="m">256</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--machine_rank<span class="w"> </span><span class="nv">$BEAKER_REPLICA_RANK</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--main_process_ip<span class="w"> </span><span class="nv">$BEAKER_LEADER_REPLICA_HOSTNAME</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--main_process_port<span class="w"> </span><span class="m">29400</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_deepspeed<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deepspeed_config_file<span class="w"> </span>configs/ds_configs/stage3_no_offloading_accelerate.conf<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deepspeed_multinode_launcher<span class="w"> </span>standard<span class="w"> </span>open_instruct/dpo_tune_cache.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_name_or_path<span class="w"> </span>allenai/Llama-3.1-Tulu-3-405B-SFT<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tokenizer_name<span class="w"> </span>allenai/Llama-3.1-Tulu-3-70B-SFT<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_flash_attn<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_seq_length<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--preprocessing_num_workers<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_train_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning_rate<span class="w"> </span>2e-07<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lr_scheduler_type<span class="w"> </span>linear<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--warmup_ratio<span class="w"> </span><span class="m">0</span>.1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--weight_decay<span class="w"> </span><span class="m">0</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_train_epochs<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>output_405b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with_tracking<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--report_to<span class="w"> </span>wandb<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--logging_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_revision<span class="w"> </span>main<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_checkpointing<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_list<span class="w"> </span>ai2-adapt-dev/405b_preference_mix<span class="w"> </span><span class="m">1</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_slow_tokenizer<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_lora<span class="w"> </span>False<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dpo_loss_type<span class="w"> </span>dpo_norm<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dpo_beta<span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--checkpointing_steps<span class="w"> </span><span class="m">1000</span>
<span class="c1"># For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JJ4QRZ31SH79AHVM6WWDVJB4/</span>
</code></pre></div>
<h2 id="rlvr">RLVR</h2>
<h3 id="rlvr-for-if-note">RLVR for IF Note:</h3>
<p>We have since updated the RLVR verifier functions and judge for precise IF. If you want to reproduce Tulu3 results,
please use the IFEvalVerifierOld class in ground_truth_utils.py. The new IFEvalVerifier class is not compatible with
the old data format, so please use the new IF data format for the new verifier. The new verifier and the new data will
give better results.</p>
<h3 id="llama-31-tulu-3-8b-rm-reproduction">Llama-3.1-Tulu-3-8B-RM Reproduction</h3>
<p>This is (almost) the exact command which produced <a href="https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-RM">allenai/Llama-3.1-Tulu-3-8B-RM</a></p>
<div class="highlight"><pre><span></span><code>accelerate<span class="w"> </span>launch<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--config_file<span class="w"> </span>configs/ds_configs/deepspeed_zero3.yaml<span class="w"> </span>open_instruct/reward_modeling.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer<span class="w"> </span><span class="s1">&#39;{&quot;allenai/llama-3.1-tulu-3-8b-preference-mixture&quot;: 1.0}&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_train_splits<span class="w"> </span>train<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_eval_mixer<span class="w"> </span><span class="s1">&#39;{&quot;allenai/ultrafeedback_binarized_cleaned&quot;: 1.0}&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_eval_splits<span class="w"> </span>test_prefs<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_name_or_path<span class="w"> </span>allenai/Llama-3.1-Tulu-3-8B-SFT<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--chat_template<span class="w"> </span>tulu<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning_rate<span class="w"> </span>3e-6<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_train_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">32</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_token_length<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_prompt_token_length<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_train_epochs<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>output/rm_8b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_checkpointing<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--push_to_hub<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with_tracking
<span class="c1"># For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JCS01RFBQGFE5F1W3W96FFVM/</span>
</code></pre></div>
<h3 id="llama-31-tulu-3-8b-reproduction">Llama-3.1-Tulu-3-8B Reproduction</h3>
<p>This is (almost) the exact command which produced <a href="https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B">allenai/Llama-3.1-Tulu-3-8B</a></p>
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>open_instruct/ppo_vllm_thread_ray_gtrl.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--exp_name<span class="w"> </span>tulu-3-8b-rlvr<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_list<span class="w"> </span>allenai/RLVR-GSM-MATH-IF-Mixed-Constraints<span class="w"> </span><span class="m">1</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_list_splits<span class="w"> </span>train<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_eval_list<span class="w"> </span>allenai/RLVR-GSM-MATH-IF-Mixed-Constraints<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_eval_list_splits<span class="w"> </span>train<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_token_length<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_prompt_token_length<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--response_length<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_name_or_path<span class="w"> </span>allenai/Llama-3.1-Tulu-3-8B-DPO<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--reward_model_path<span class="w"> </span>allenai/Llama-3.1-Tulu-3-8B-RM<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--non_stop_penalty<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--stop_token<span class="w"> </span>eos<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--temperature<span class="w"> </span><span class="m">1</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--chat_template_name<span class="w"> </span>tulu<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning_rate<span class="w"> </span>3e-7<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--total_episodes<span class="w"> </span><span class="m">10000000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--penalty_reward_value<span class="w"> </span>-10.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deepspeed_stage<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_train_batch_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--local_rollout_forward_batch_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--local_mini_batch_size<span class="w"> </span><span class="m">32</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--local_rollout_batch_size<span class="w"> </span><span class="m">32</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--actor_num_gpus_per_node<span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_tensor_parallel_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--beta<span class="w"> </span><span class="m">0</span>.05<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--apply_verifiable_reward<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>output/rlvr_8b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--seed<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_evals<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_freq<span class="w"> </span><span class="m">100</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--reward_model_multiplier<span class="w"> </span><span class="m">0</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_checkpointing<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with_tracking
<span class="c1"># For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JCVTA10BQDVGGQKFYWEZ6KCQ/</span>
</code></pre></div>
<h3 id="llama-31-tulu-3-70b-reproduction">Llama-3.1-Tulu-3-70B Reproduction</h3>
<p>This is (almost) the exact command which produced <a href="https://huggingface.co/allenai/Llama-3.1-Tulu-3-70B">allenai/Llama-3.1-Tulu-3-70B</a></p>
<p>Couple of notes:
* Make sure to modify <code>configs/beaker_configs/ray_node_setup.sh</code> in our own cluster setup. The idea is to have the replicas join the main machines via <code>ray</code>.
* We had to use <code>--vllm_tensor_parallel_size 4</code> because <code>--vllm_tensor_parallel_size 8</code> errors out for some strange reason. This is a temporary workaround.
* Here the effective batch size is <code>sum(actor_num_gpus_per_node) * local_mini_batch_size = 40 * 16 = 640</code>. If you have less GPUs, you can adjust <code>actor_num_gpus_per_node</code> and <code>local_mini_batch_size</code> accordingly.</p>
<div class="highlight"><pre><span></span><code><span class="nb">source</span><span class="w"> </span>configs/beaker_configs/ray_node_setup.sh<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>python<span class="w"> </span>open_instruct/ppo_vllm_thread_ray_gtrl.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_list<span class="w"> </span>allenai/RLVR-GSM-MATH-IF-Mixed-Constraints<span class="w"> </span><span class="m">1</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_list_splits<span class="w"> </span>train<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_eval_list<span class="w"> </span>allenai/RLVR-GSM-MATH-IF-Mixed-Constraints<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_eval_list_splits<span class="w"> </span>train<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_token_length<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_prompt_token_length<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--response_length<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_name_or_path<span class="w"> </span>allenai/Llama-3.1-Tulu-3-70B-DPO<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--exp_name<span class="w"> </span>tulu-3-70b-rlvr<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--reward_model_path<span class="w"> </span>allenai/Llama-3.1-Tulu-3-8B-RM<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--beta<span class="w"> </span><span class="m">0</span>.07<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--warmup_ratio<span class="w"> </span><span class="m">0</span>.1<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--seed<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>output/rlvr_70b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--non_stop_penalty<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--stop_token<span class="w"> </span>eos<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--temperature<span class="w"> </span><span class="m">1</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--chat_template_name<span class="w"> </span>tulu<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning_rate<span class="w"> </span>1e-7<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--total_episodes<span class="w"> </span><span class="m">400000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--penalty_reward_value<span class="w"> </span>-10.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deepspeed_stage<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_train_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--local_rollout_forward_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--local_mini_batch_size<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--local_rollout_batch_size<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--actor_num_gpus_per_node<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_num_engines<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_tensor_parallel_size<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--apply_verifiable_reward<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--reward_model_multiplier<span class="w"> </span><span class="m">0</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--no_gather_whole_model<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_evals<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_freq<span class="w"> </span><span class="m">40</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_checkpointing<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with_tracking
<span class="c1"># For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JD3YEM4XGH2F2H10Y49GK441/</span>
</code></pre></div>
<h3 id="llama-31-tulu-3-405b-reproduction">Llama-3.1-Tulu-3-405B Reproduction</h3>
<p>This is (almost) the exact command which produced <a href="https://huggingface.co/allenai/Llama-3.1-Tulu-3-405B">allenai/Llama-3.1-Tulu-3-405B</a></p>
<p>Couple of notes:
* We had to set <code>TORCH_NCCL_ENABLE_MONITORING=0</code> to turn off NCCL heartbeat monitoring and avoid timeouts. Feel free to remove this.
* Make sure to modify <code>configs/beaker_configs/ray_node_setup.sh</code> in our own cluster setup. The idea is to have the replicas join the main machines via <code>ray</code>.
* Here the effective batch size is <code>sum(actor_num_gpus_per_node) * local_mini_batch_size = 40 * 16 = 640</code>. If you have less GPUs, you can adjust <code>actor_num_gpus_per_node</code> and <code>local_mini_batch_size</code> accordingly.</p>
<div class="highlight"><pre><span></span><code><span class="nv">TORCH_NCCL_ENABLE_MONITORING</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>python<span class="w"> </span>mason.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cluster<span class="w"> </span>ai2/jupiter<span class="w"> </span>--pure_docker_mode<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--workspace<span class="w"> </span>ai2/tulu-3-dev<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--priority<span class="w"> </span>urgent<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--preemptible<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_nodes<span class="w"> </span><span class="m">32</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--image<span class="w"> </span>nathanl/open_instruct_auto<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--budget<span class="w"> </span>ai2/oe-adapt<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpus<span class="w"> </span><span class="m">8</span><span class="w"> </span>--<span class="w"> </span><span class="nb">source</span><span class="w"> </span>configs/beaker_configs/ray_node_setup.sh<span class="w"> </span><span class="se">\&amp;\&amp;</span><span class="w"> </span><span class="nv">TORCH_DISTRIBUTED_DEBUG</span><span class="o">=</span>DETAIL<span class="w"> </span>python<span class="w"> </span>open_instruct/ppo_vllm_thread_ray_gtrl.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_list<span class="w"> </span>allenai/RLVR-MATH<span class="w"> </span><span class="m">1</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_list_splits<span class="w"> </span>train<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_eval_list<span class="w"> </span>allenai/RLVR-MATH<span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_eval_list_splits<span class="w"> </span>train<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_token_length<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_prompt_token_length<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--response_length<span class="w"> </span><span class="m">1024</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_name_or_path<span class="w"> </span>/weka/oe-adapt-default/hamishi/405b_dpo_v4<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--exp_name<span class="w"> </span><span class="s2">&quot;405b_rlvr_math_only_8b_valu_on_v4&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--reward_model_path<span class="w"> </span>allenai/Llama-3.1-Tulu-3-8B-RM<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--beta<span class="w"> </span><span class="m">0</span>.05<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span><span class="s2">&quot;/weka/oe-adapt-default/hamishi/405b_rlvr_math_only_8b_valu_on_v4&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--non_stop_penalty<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--stop_token<span class="w"> </span>eos<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--temperature<span class="w"> </span><span class="m">1</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--chat_template<span class="w"> </span>tulu<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning_rate<span class="w"> </span>1e-7<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--total_episodes<span class="w"> </span><span class="m">400000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_epochs<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--penalty_reward_value<span class="w"> </span>-10.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deepspeed_stage<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_train_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--local_rollout_forward_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--local_mini_batch_size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--local_rollout_batch_size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--actor_num_gpus_per_node<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_num_engines<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_tensor_parallel_size<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_enforce_eager<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--apply_verifiable_reward<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--reward_model_multiplier<span class="w"> </span><span class="m">0</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--no_gather_whole_model<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--seed<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_evals<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--no_try_launch_beaker_eval_jobs<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_freq<span class="w"> </span><span class="m">25</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--try_launch_beaker_eval_jobs_on_weka<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_checkpointing<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with_tracking
<span class="c1"># For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JJA31S20XAFR82YPFKSMMYZV/</span>
</code></pre></div>
<h3 id="new-llama-31-tulu-31-8b-reproduction">(NEW) Llama-3.1-Tulu-3.1-8B Reproduction</h3>
<p>This is the exact command which produced <a href="https://huggingface.co/allenai/Llama-3.1-Tulu-3.1-8B">allenai/Llama-3.1-Tulu-3.1-8B</a>, which uses 2 nodes (16 GPUs)</p>
<div class="highlight"><pre><span></span><code><span class="k">for</span><span class="w"> </span>learning_rate<span class="w"> </span><span class="k">in</span><span class="w"> </span>5e-7<span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="k">for</span><span class="w"> </span>beta<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">0</span>.01<span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="k">for</span><span class="w"> </span>nspp<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">16</span><span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="k">for</span><span class="w"> </span>m<span class="w"> </span><span class="k">in</span><span class="w"> </span>half-m<span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="k">for</span><span class="w"> </span>kl_estimator<span class="w"> </span><span class="k">in</span><span class="w"> </span>kl3<span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="nv">local_rollout_batch_size</span><span class="o">=</span><span class="m">4</span>
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="nv">$m</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s2">&quot;half-m&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="nv">local_mini_batch_size</span><span class="o">=</span><span class="k">$((</span><span class="nv">$local_rollout_batch_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nv">$nspp</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="m">2</span><span class="k">))</span>
<span class="k">else</span>
<span class="w">    </span><span class="nv">local_mini_batch_size</span><span class="o">=</span><span class="k">$((</span><span class="nv">$local_rollout_batch_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nv">$nspp</span><span class="k">))</span>
<span class="k">fi</span>
<span class="nv">exp_name</span><span class="o">=</span><span class="s2">&quot;0204_lr_scan_grpo_math_lr_</span><span class="si">${</span><span class="nv">learning_rate</span><span class="si">}</span><span class="s2">_</span><span class="si">${</span><span class="nv">kl_estimator</span><span class="si">}</span><span class="s2">_</span><span class="si">${</span><span class="nv">beta</span><span class="si">}</span><span class="s2">_</span><span class="si">${</span><span class="nv">nspp</span><span class="si">}</span><span class="s2">_</span><span class="si">${</span><span class="nv">m</span><span class="si">}</span><span class="s2">_</span><span class="si">${</span><span class="nv">RANDOM</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="nv">full_bsz</span><span class="o">=</span><span class="k">$((</span><span class="nv">$local_rollout_batch_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nv">nspp</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="o">(</span><span class="m">7</span><span class="o">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">2</span><span class="k">))</span>
<span class="nb">echo</span><span class="w"> </span><span class="nv">$exp_name</span>:
<span class="nb">echo</span><span class="w"> </span>---<span class="w"> </span><span class="nv">local_mini_batch_size</span><span class="o">=</span><span class="nv">$local_mini_batch_size</span>
<span class="nb">echo</span><span class="w"> </span>---<span class="w"> </span><span class="nv">full_bsz</span><span class="o">=</span><span class="nv">$full_bsz</span>
<span class="nb">echo</span><span class="w"> </span>---<span class="w"> </span><span class="nv">num_gradient_updates</span><span class="o">=</span><span class="k">$((</span><span class="nv">$local_rollout_batch_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nv">$nspp</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nv">$local_mini_batch_size</span><span class="k">))</span>
python<span class="w"> </span>mason.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cluster<span class="w"> </span>ai2/jupiter<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--workspace<span class="w"> </span>ai2/tulu-3-dev<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--priority<span class="w"> </span>high<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--preemptible<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_nodes<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_retries<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--budget<span class="w"> </span>ai2/oe-adapt<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpus<span class="w"> </span><span class="m">8</span><span class="w"> </span>--<span class="w"> </span><span class="nb">source</span><span class="w"> </span>configs/beaker_configs/ray_node_setup.sh<span class="w"> </span><span class="se">\&amp;\&amp;</span><span class="w"> </span>uv<span class="w"> </span>run<span class="w"> </span>python<span class="w"> </span>open_instruct/grpo_vllm_thread_ray_gtrl.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--exp_name<span class="w"> </span><span class="nv">$exp_name</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--beta<span class="w"> </span><span class="nv">$beta</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--local_mini_batch_size<span class="w"> </span><span class="nv">$local_mini_batch_size</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--number_samples_per_prompt<span class="w"> </span><span class="nv">$nspp</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>/weka/oe-adapt-default/costah/models/<span class="nv">$exp_name</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--local_rollout_batch_size<span class="w"> </span><span class="nv">$local_rollout_batch_size</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--kl_estimator<span class="w"> </span><span class="nv">$kl_estimator</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning_rate<span class="w"> </span><span class="nv">$learning_rate</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_list<span class="w"> </span>allenai/RLVR-GSM-MATH-IF-Mixed-Constraints<span class="w"> </span><span class="m">1</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_list_splits<span class="w"> </span>train<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_eval_list<span class="w"> </span>allenai/RLVR-GSM-MATH-IF-Mixed-Constraints<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_mixer_eval_list_splits<span class="w"> </span>train<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_token_length<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max_prompt_token_length<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--response_length<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_name_or_path<span class="w"> </span>allenai/Llama-3.1-Tulu-3-8B-DPO<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--non_stop_penalty<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--stop_token<span class="w"> </span>eos<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--temperature<span class="w"> </span><span class="m">1</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--chat_template_name<span class="w"> </span>tulu<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--total_episodes<span class="w"> </span><span class="m">10000000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--penalty_reward_value<span class="w"> </span><span class="m">0</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--deepspeed_stage<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_train_batch_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--local_rollout_forward_batch_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--actor_num_gpus_per_node<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_epochs<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--vllm_tensor_parallel_size<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lr_scheduler_type<span class="w"> </span>constant<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--apply_verifiable_reward<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--seed<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_evals<span class="w"> </span><span class="m">30</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_freq<span class="w"> </span><span class="m">40</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--reward_model_multiplier<span class="w"> </span><span class="m">0</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--no_try_launch_beaker_eval_jobs<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--try_launch_beaker_eval_jobs_on_weka<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_checkpointing<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--with_tracking
<span class="k">done</span>
<span class="k">done</span>
<span class="k">done</span>
<span class="k">done</span>
<span class="k">done</span>
<span class="c1"># For Ai2 internal members, this was the experiment URL: https://beaker.allen.ai/orgs/ai2/workspaces/tulu-3-dev/work/01JKA7CSDGG3YA84X89C5HJPXR?taskId=01JKA7CSDQMVBDNAWF5T7ZXDSA&amp;jobId=01JKH4KYJTR2Y2NYNCCQ63ZQHE</span>
</code></pre></div>
<p>If you are running on a single node (8 GPUs), consider adjusting the commands as follows. Basically, the idea is to simulate the same batch size. In the two nodes setup, we used <code>--actor_num_gpus_per_node 4 8</code> (12 GPUs) for training, so we multiply it with <code>local_rollout_batch_size=4</code> to get the rollout batch size <code>12 * 4 = 48</code>. Now assume we used <code>--actor_num_gpus_per_node 6</code> (6 GPUs) for training, so we get <code>48 / 6 = 8</code>, which is the new <code>local_rollout_batch_size</code>.</p>
<div class="highlight"><pre><span></span><code><span class="w"> </span>for learning_rate in 5e-7; do
<span class="w"> </span>for beta in 0.01; do
<span class="w"> </span>for nspp in 16; do
<span class="w"> </span>for m in half-m ; do
<span class="w"> </span>for kl_estimator in kl3; do
<span class="gd">-local_rollout_batch_size=4</span>
<span class="gi">+local_rollout_batch_size=8</span>
<span class="w"> </span>if [ $m == &quot;half-m&quot; ]; then
<span class="w"> </span>    local_mini_batch_size=$(($local_rollout_batch_size * $nspp / 2))
<span class="w"> </span>else
<span class="w"> </span>    local_mini_batch_size=$(($local_rollout_batch_size * $nspp))
<span class="w"> </span>fi
<span class="w"> </span>exp_name=&quot;0204_lr_scan_grpo_math_lr_${learning_rate}_${kl_estimator}_${beta}_${nspp}_${m}_${RANDOM}&quot;
<span class="w"> </span>full_bsz=$(($local_rollout_batch_size * nspp * (7) * 2))
<span class="w"> </span>echo $exp_name:
<span class="w"> </span>echo --- local_mini_batch_size=$local_mini_batch_size
<span class="w"> </span>echo --- full_bsz=$full_bsz
<span class="w"> </span>echo --- num_gradient_updates=$(($local_rollout_batch_size * $nspp / $local_mini_batch_size))
<span class="w"> </span>python mason.py \
<span class="w"> </span>    --cluster ai2/jupiter \
<span class="w"> </span>    --workspace ai2/tulu-3-dev \
<span class="w"> </span>    --priority high \
<span class="w"> </span>    --preemptible \
<span class="w"> </span>    --num_nodes 2 \
<span class="w"> </span>    --max_retries 1 \
<span class="w"> </span>    --budget ai2/oe-adapt \
<span class="w"> </span>    --gpus 8 -- source configs/beaker_configs/ray_node_setup.sh \&amp;\&amp; uv run python open_instruct/grpo_vllm_thread_ray_gtrl.py \
<span class="w"> </span>    --exp_name $exp_name \
<span class="w"> </span>    --beta $beta \
<span class="w"> </span>    --local_mini_batch_size $local_mini_batch_size \
<span class="w"> </span>    --number_samples_per_prompt $nspp \
<span class="w"> </span>    --output_dir /weka/oe-adapt-default/costah/models/$exp_name \
<span class="w"> </span>    --local_rollout_batch_size $local_rollout_batch_size \
<span class="w"> </span>    --kl_estimator $kl_estimator \
<span class="w"> </span>    --learning_rate $learning_rate \
<span class="w"> </span>    --dataset_mixer_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 1.0 \
<span class="w"> </span>    --dataset_mixer_list_splits train \
<span class="w"> </span>    --dataset_mixer_eval_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 16 \
<span class="w"> </span>    --dataset_mixer_eval_list_splits train \
<span class="w"> </span>    --max_token_length 2048 \
<span class="w"> </span>    --max_prompt_token_length 2048 \
<span class="w"> </span>    --response_length 2048 \
<span class="w"> </span>    --model_name_or_path allenai/Llama-3.1-Tulu-3-8B-DPO \
<span class="w"> </span>    --non_stop_penalty \
<span class="w"> </span>    --stop_token eos \
<span class="w"> </span>    --temperature 1.0 \
<span class="w"> </span>    --chat_template_name tulu \
<span class="w"> </span>    --total_episodes 10000000 \
<span class="w"> </span>    --penalty_reward_value 0.0 \
<span class="gd">-    --deepspeed_stage 2 \</span>
<span class="gi">+    --deepspeed_stage 3 \</span>
<span class="w"> </span>    --per_device_train_batch_size 2 \
<span class="w"> </span>    --local_rollout_forward_batch_size 2 \
<span class="gd">-    --actor_num_gpus_per_node 4 8 \</span>
<span class="gi">+    --actor_num_gpus_per_node 6 \</span>
<span class="w"> </span>    --num_epochs 1 \
<span class="gd">-    --vllm_tensor_parallel_size 4 \</span>
<span class="gi">+    --vllm_tensor_parallel_size 2 \</span>
<span class="w"> </span>    --lr_scheduler_type constant \
<span class="w"> </span>    --apply_verifiable_reward true \
<span class="w"> </span>    --seed 1 \
<span class="w"> </span>    --num_evals 30 \
<span class="w"> </span>    --save_freq 40 \
<span class="w"> </span>    --reward_model_multiplier 0.0 \
<span class="w"> </span>    --no_try_launch_beaker_eval_jobs \
<span class="w"> </span>    --try_launch_beaker_eval_jobs_on_weka \
<span class="w"> </span>    --gradient_checkpointing \
<span class="w"> </span>    --with_tracking
<span class="w"> </span>done
<span class="w"> </span>done
<span class="w"> </span>done
<span class="w"> </span>done
<span class="w"> </span>done
</code></pre></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tracking", "navigation.sections", "navigation.expand", "navigation.top", "search.suggest", "search.highlight"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>