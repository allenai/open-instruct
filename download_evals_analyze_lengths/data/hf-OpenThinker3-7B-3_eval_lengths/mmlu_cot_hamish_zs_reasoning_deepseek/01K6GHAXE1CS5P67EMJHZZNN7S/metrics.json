{"all_primary_scores": ["mmlu:cot::hamish_zs_reasoning_deepseek: 0.773139", "mmlu_abstract_algebra:cot::hamish_zs_reasoning_deepseek: 0.87", "mmlu_anatomy:cot::hamish_zs_reasoning_deepseek: 0.622222", "mmlu_astronomy:cot::hamish_zs_reasoning_deepseek: 0.848684", "mmlu_business_ethics:cot::hamish_zs_reasoning_deepseek: 0.71", "mmlu_clinical_knowledge:cot::hamish_zs_reasoning_deepseek: 0.777358", "mmlu_college_biology:cot::hamish_zs_reasoning_deepseek: 0.840278", "mmlu_college_chemistry:cot::hamish_zs_reasoning_deepseek: 0.67", "mmlu_college_computer_science:cot::hamish_zs_reasoning_deepseek: 0.81", "mmlu_college_mathematics:cot::hamish_zs_reasoning_deepseek: 0.89", "mmlu_college_medicine:cot::hamish_zs_reasoning_deepseek: 0.734104", "mmlu_college_physics:cot::hamish_zs_reasoning_deepseek: 0.95098", "mmlu_computer_security:cot::hamish_zs_reasoning_deepseek: 0.82", "mmlu_conceptual_physics:cot::hamish_zs_reasoning_deepseek: 0.914894", "mmlu_econometrics:cot::hamish_zs_reasoning_deepseek: 0.649123", "mmlu_electrical_engineering:cot::hamish_zs_reasoning_deepseek: 0.724138", "mmlu_elementary_mathematics:cot::hamish_zs_reasoning_deepseek: 0.973545", "mmlu_formal_logic:cot::hamish_zs_reasoning_deepseek: 0.793651", "mmlu_global_facts:cot::hamish_zs_reasoning_deepseek: 0.49", "mmlu_high_school_biology:cot::hamish_zs_reasoning_deepseek: 0.887097", "mmlu_high_school_chemistry:cot::hamish_zs_reasoning_deepseek: 0.871921", "mmlu_high_school_computer_science:cot::hamish_zs_reasoning_deepseek: 0.95", "mmlu_high_school_european_history:cot::hamish_zs_reasoning_deepseek: 0.787879", "mmlu_high_school_geography:cot::hamish_zs_reasoning_deepseek: 0.833333", "mmlu_high_school_government_and_politics:cot::hamish_zs_reasoning_deepseek: 0.88601", "mmlu_high_school_macroeconomics:cot::hamish_zs_reasoning_deepseek: 0.828205", "mmlu_high_school_mathematics:cot::hamish_zs_reasoning_deepseek: 0.97037", "mmlu_high_school_microeconomics:cot::hamish_zs_reasoning_deepseek: 0.857143", "mmlu_high_school_physics:cot::hamish_zs_reasoning_deepseek: 0.874172", "mmlu_high_school_psychology:cot::hamish_zs_reasoning_deepseek: 0.897248", "mmlu_high_school_statistics:cot::hamish_zs_reasoning_deepseek: 0.847222", "mmlu_high_school_us_history:cot::hamish_zs_reasoning_deepseek: 0.769608", "mmlu_high_school_world_history:cot::hamish_zs_reasoning_deepseek: 0.772152", "mmlu_human_aging:cot::hamish_zs_reasoning_deepseek: 0.748879", "mmlu_human_sexuality:cot::hamish_zs_reasoning_deepseek: 0.801527", "mmlu_international_law:cot::hamish_zs_reasoning_deepseek: 0.735537", "mmlu_jurisprudence:cot::hamish_zs_reasoning_deepseek: 0.731481", "mmlu_logical_fallacies:cot::hamish_zs_reasoning_deepseek: 0.754601", "mmlu_machine_learning:cot::hamish_zs_reasoning_deepseek: 0.669643", "mmlu_management:cot::hamish_zs_reasoning_deepseek: 0.815534", "mmlu_marketing:cot::hamish_zs_reasoning_deepseek: 0.897436", "mmlu_medical_genetics:cot::hamish_zs_reasoning_deepseek: 0.85", "mmlu_miscellaneous:cot::hamish_zs_reasoning_deepseek: 0.819923", "mmlu_moral_disputes:cot::hamish_zs_reasoning_deepseek: 0.653179", "mmlu_moral_scenarios:cot::hamish_zs_reasoning_deepseek: 0.589944", "mmlu_nutrition:cot::hamish_zs_reasoning_deepseek: 0.77451", "mmlu_philosophy:cot::hamish_zs_reasoning_deepseek: 0.672026", "mmlu_prehistory:cot::hamish_zs_reasoning_deepseek: 0.75", "mmlu_professional_accounting:cot::hamish_zs_reasoning_deepseek: 0.617021", "mmlu_professional_law:cot::hamish_zs_reasoning_deepseek: 0.447849", "mmlu_professional_medicine:cot::hamish_zs_reasoning_deepseek: 0.716912", "mmlu_professional_psychology:cot::hamish_zs_reasoning_deepseek: 0.699346", "mmlu_public_relations:cot::hamish_zs_reasoning_deepseek: 0.663636", "mmlu_security_studies:cot::hamish_zs_reasoning_deepseek: 0.726531", "mmlu_sociology:cot::hamish_zs_reasoning_deepseek: 0.766169", "mmlu_us_foreign_policy:cot::hamish_zs_reasoning_deepseek: 0.78", "mmlu_virology:cot::hamish_zs_reasoning_deepseek: 0.493976", "mmlu_world_religions:cot::hamish_zs_reasoning_deepseek: 0.77193"], "tasks": [{"alias": "mmlu:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple_micro": 0.7381427147130039, "exact_match_simple_macro": 0.7723990636162857, "primary_score_micro": 0.7392821535393819, "primary_score_macro": 0.7731390961215917, "exact_match_micro": 0.7392821535393819, "exact_match_macro": 0.7731390961215917, "primary_score": 0.7731390961215917, "extra_metrics": {"num_tokens_micro": 2925.364620424441, "num_tokens_macro": 2886.7573889483047, "answer_format_correct_micro": 0.9437402079475858, "answer_format_correct_macro": 0.9403112516294174}}, "num_instances": 14042, "processing_time": 3.2024433612823486, "task_config": {"task_name": "mmlu:cot::hamish_zs_reasoning_deepseek", "task_core": "mmlu_abstract_algebra", "split": "test", "primary_metric": "macro", "context_kwargs": {"description": "The following are multiple choice questions about abstract algebra. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "abstract_algebra", "use_chat_format": true, "version": 0, "metadata": {"num_tasks": 57, "description": "Aggregate metric", "alias": "mmlu:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_abstract_algebra:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.86, "exact_match": 0.87, "primary_score": 0.87, "extra_metrics": {"num_tokens": 3461.48, "answer_format_correct": 0.7}}, "num_instances": 100, "processing_time": 0.04318642616271973, "task_config": {"task_name": "mmlu_abstract_algebra:cot", "task_core": "mmlu_abstract_algebra", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about abstract algebra. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "abstract_algebra", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_abstract_algebra:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_anatomy:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.6222222222222222, "exact_match": 0.6222222222222222, "primary_score": 0.6222222222222222, "extra_metrics": {"num_tokens": 3275.9555555555557, "answer_format_correct": 0.9666666666666667}}, "num_instances": 135, "processing_time": 0.03899884223937988, "task_config": {"task_name": "mmlu_anatomy:cot", "task_core": "mmlu_anatomy", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about anatomy. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "anatomy", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_anatomy:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_astronomy:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.8486842105263158, "exact_match": 0.8486842105263158, "primary_score": 0.8486842105263158, "extra_metrics": {"num_tokens": 2100.1315789473683, "answer_format_correct": 0.9802631578947368}}, "num_instances": 152, "processing_time": 0.03882336616516113, "task_config": {"task_name": "mmlu_astronomy:cot", "task_core": "mmlu_astronomy", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about astronomy. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "astronomy", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_astronomy:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_business_ethics:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.71, "exact_match": 0.71, "primary_score": 0.71, "extra_metrics": {"num_tokens": 2746.95, "answer_format_correct": 0.97}}, "num_instances": 100, "processing_time": 0.03666377067565918, "task_config": {"task_name": "mmlu_business_ethics:cot", "task_core": "mmlu_business_ethics", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about business ethics. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "business_ethics", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_business_ethics:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_clinical_knowledge:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.7773584905660378, "exact_match": 0.7773584905660378, "primary_score": 0.7773584905660378, "extra_metrics": {"num_tokens": 2580.841509433962, "answer_format_correct": 0.969811320754717}}, "num_instances": 265, "processing_time": 0.05087137222290039, "task_config": {"task_name": "mmlu_clinical_knowledge:cot", "task_core": "mmlu_clinical_knowledge", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about clinical knowledge. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "clinical_knowledge", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_clinical_knowledge:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_college_biology:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.8402777777777778, "exact_match": 0.8402777777777778, "primary_score": 0.8402777777777778, "extra_metrics": {"num_tokens": 2818.284722222222, "answer_format_correct": 0.9756944444444444}}, "num_instances": 144, "processing_time": 0.045989036560058594, "task_config": {"task_name": "mmlu_college_biology:cot", "task_core": "mmlu_college_biology", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about college biology. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "college_biology", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_college_biology:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_college_chemistry:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.66, "exact_match": 0.67, "primary_score": 0.67, "extra_metrics": {"num_tokens": 6613.46, "answer_format_correct": 0.905}}, "num_instances": 100, "processing_time": 0.03865933418273926, "task_config": {"task_name": "mmlu_college_chemistry:cot", "task_core": "mmlu_college_chemistry", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about college chemistry. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "college_chemistry", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_college_chemistry:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_college_computer_science:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.81, "exact_match": 0.81, "primary_score": 0.81, "extra_metrics": {"num_tokens": 4133.24, "answer_format_correct": 0.9}}, "num_instances": 100, "processing_time": 0.03578352928161621, "task_config": {"task_name": "mmlu_college_computer_science:cot", "task_core": "mmlu_college_computer_science", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about college computer science. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "college_computer_science", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_college_computer_science:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_college_mathematics:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.89, "exact_match": 0.89, "primary_score": 0.89, "extra_metrics": {"num_tokens": 5262.28, "answer_format_correct": 0.66}}, "num_instances": 100, "processing_time": 0.038274526596069336, "task_config": {"task_name": "mmlu_college_mathematics:cot", "task_core": "mmlu_college_mathematics", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about college mathematics. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "college_mathematics", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_college_mathematics:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_college_medicine:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.7341040462427746, "exact_match": 0.7341040462427746, "primary_score": 0.7341040462427746, "extra_metrics": {"num_tokens": 3264.277456647399, "answer_format_correct": 0.9797687861271677}}, "num_instances": 173, "processing_time": 0.04235720634460449, "task_config": {"task_name": "mmlu_college_medicine:cot", "task_core": "mmlu_college_medicine", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about college medicine. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "college_medicine", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_college_medicine:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_college_physics:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.9509803921568627, "exact_match": 0.9509803921568627, "primary_score": 0.9509803921568627, "extra_metrics": {"num_tokens": 3512.5196078431372, "answer_format_correct": 0.8529411764705882}}, "num_instances": 102, "processing_time": 0.03500247001647949, "task_config": {"task_name": "mmlu_college_physics:cot", "task_core": "mmlu_college_physics", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about college physics. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "college_physics", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_college_physics:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_computer_security:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.82, "exact_match": 0.82, "primary_score": 0.82, "extra_metrics": {"num_tokens": 2146.15, "answer_format_correct": 0.96}}, "num_instances": 100, "processing_time": 0.04373598098754883, "task_config": {"task_name": "mmlu_computer_security:cot", "task_core": "mmlu_computer_security", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about computer security. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "computer_security", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_computer_security:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_conceptual_physics:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.9148936170212766, "exact_match": 0.9148936170212766, "primary_score": 0.9148936170212766, "extra_metrics": {"num_tokens": 1808.9659574468085, "answer_format_correct": 0.9361702127659575}}, "num_instances": 235, "processing_time": 0.04547476768493652, "task_config": {"task_name": "mmlu_conceptual_physics:cot", "task_core": "mmlu_conceptual_physics", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about conceptual physics. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "conceptual_physics", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_conceptual_physics:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_econometrics:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.6491228070175439, "exact_match": 0.6491228070175439, "primary_score": 0.6491228070175439, "extra_metrics": {"num_tokens": 4792.578947368421, "answer_format_correct": 0.8903508771929824}}, "num_instances": 114, "processing_time": 0.03580784797668457, "task_config": {"task_name": "mmlu_econometrics:cot", "task_core": "mmlu_econometrics", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about econometrics. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "econometrics", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_econometrics:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_electrical_engineering:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.7241379310344828, "exact_match": 0.7241379310344828, "primary_score": 0.7241379310344828, "extra_metrics": {"num_tokens": 3189.5172413793102, "answer_format_correct": 0.9551724137931035}}, "num_instances": 145, "processing_time": 0.03685617446899414, "task_config": {"task_name": "mmlu_electrical_engineering:cot", "task_core": "mmlu_electrical_engineering", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about electrical engineering. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "electrical_engineering", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_electrical_engineering:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_elementary_mathematics:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.9735449735449735, "exact_match": 0.9735449735449735, "primary_score": 0.9735449735449735, "extra_metrics": {"num_tokens": 1495.3915343915344, "answer_format_correct": 0.9232804232804233}}, "num_instances": 378, "processing_time": 0.06170320510864258, "task_config": {"task_name": "mmlu_elementary_mathematics:cot", "task_core": "mmlu_elementary_mathematics", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about elementary mathematics. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "elementary_mathematics", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_elementary_mathematics:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_formal_logic:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.7936507936507936, "exact_match": 0.7936507936507936, "primary_score": 0.7936507936507936, "extra_metrics": {"num_tokens": 4501.039682539683, "answer_format_correct": 0.9087301587301587}}, "num_instances": 126, "processing_time": 0.051721811294555664, "task_config": {"task_name": "mmlu_formal_logic:cot", "task_core": "mmlu_formal_logic", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about formal logic. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "formal_logic", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_formal_logic:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_global_facts:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.49, "exact_match": 0.49, "primary_score": 0.49, "extra_metrics": {"num_tokens": 3178.62, "answer_format_correct": 0.97}}, "num_instances": 100, "processing_time": 0.03946399688720703, "task_config": {"task_name": "mmlu_global_facts:cot", "task_core": "mmlu_global_facts", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about global facts. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "global_facts", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_global_facts:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_high_school_biology:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.8870967741935484, "exact_match": 0.8870967741935484, "primary_score": 0.8870967741935484, "extra_metrics": {"num_tokens": 2065.025806451613, "answer_format_correct": 0.9854838709677419}}, "num_instances": 310, "processing_time": 0.05376434326171875, "task_config": {"task_name": "mmlu_high_school_biology:cot", "task_core": "mmlu_high_school_biology", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about high school biology. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "high_school_biology", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_high_school_biology:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_high_school_chemistry:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.8719211822660099, "exact_match": 0.8719211822660099, "primary_score": 0.8719211822660099, "extra_metrics": {"num_tokens": 2946.5320197044334, "answer_format_correct": 0.9507389162561576}}, "num_instances": 203, "processing_time": 0.04310202598571777, "task_config": {"task_name": "mmlu_high_school_chemistry:cot", "task_core": "mmlu_high_school_chemistry", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about high school chemistry. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "high_school_chemistry", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_high_school_chemistry:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_high_school_computer_science:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.95, "exact_match": 0.95, "primary_score": 0.95, "extra_metrics": {"num_tokens": 1784.11, "answer_format_correct": 0.975}}, "num_instances": 100, "processing_time": 0.03246164321899414, "task_config": {"task_name": "mmlu_high_school_computer_science:cot", "task_core": "mmlu_high_school_computer_science", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about high school computer science. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "high_school_computer_science", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_high_school_computer_science:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_high_school_european_history:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.7878787878787878, "exact_match": 0.7878787878787878, "primary_score": 0.7878787878787878, "extra_metrics": {"num_tokens": 2058.072727272727, "answer_format_correct": 0.9727272727272728}}, "num_instances": 165, "processing_time": 0.04411649703979492, "task_config": {"task_name": "mmlu_high_school_european_history:cot", "task_core": "mmlu_high_school_european_history", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about high school european history. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "high_school_european_history", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_high_school_european_history:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_high_school_geography:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.8333333333333334, "exact_match": 0.8333333333333334, "primary_score": 0.8333333333333334, "extra_metrics": {"num_tokens": 2338.813131313131, "answer_format_correct": 0.9797979797979798}}, "num_instances": 198, "processing_time": 0.05676579475402832, "task_config": {"task_name": "mmlu_high_school_geography:cot", "task_core": "mmlu_high_school_geography", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about high school geography. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "high_school_geography", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_high_school_geography:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_high_school_government_and_politics:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.8860103626943006, "exact_match": 0.8860103626943006, "primary_score": 0.8860103626943006, "extra_metrics": {"num_tokens": 2265.139896373057, "answer_format_correct": 0.9844559585492227}}, "num_instances": 193, "processing_time": 0.04799962043762207, "task_config": {"task_name": "mmlu_high_school_government_and_politics:cot", "task_core": "mmlu_high_school_government_and_politics", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about high school government and politics. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "high_school_government_and_politics", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_high_school_government_and_politics:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_high_school_macroeconomics:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.8282051282051283, "exact_match": 0.8282051282051283, "primary_score": 0.8282051282051283, "extra_metrics": {"num_tokens": 3119.9, "answer_format_correct": 0.95}}, "num_instances": 390, "processing_time": 0.06596112251281738, "task_config": {"task_name": "mmlu_high_school_macroeconomics:cot", "task_core": "mmlu_high_school_macroeconomics", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about high school macroeconomics. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "high_school_macroeconomics", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_high_school_macroeconomics:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_high_school_mathematics:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.9666666666666667, "exact_match": 0.9703703703703703, "primary_score": 0.9703703703703703, "extra_metrics": {"num_tokens": 3441.3074074074075, "answer_format_correct": 0.6018518518518519}}, "num_instances": 270, "processing_time": 0.07518196105957031, "task_config": {"task_name": "mmlu_high_school_mathematics:cot", "task_core": "mmlu_high_school_mathematics", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about high school mathematics. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "high_school_mathematics", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_high_school_mathematics:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_high_school_microeconomics:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.8571428571428571, "exact_match": 0.8571428571428571, "primary_score": 0.8571428571428571, "extra_metrics": {"num_tokens": 3112.659663865546, "answer_format_correct": 0.9453781512605042}}, "num_instances": 238, "processing_time": 0.05023622512817383, "task_config": {"task_name": "mmlu_high_school_microeconomics:cot", "task_core": "mmlu_high_school_microeconomics", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about high school microeconomics. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "high_school_microeconomics", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_high_school_microeconomics:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_high_school_physics:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.8675496688741722, "exact_match": 0.8741721854304636, "primary_score": 0.8741721854304636, "extra_metrics": {"num_tokens": 3641.4503311258277, "answer_format_correct": 0.8609271523178808}}, "num_instances": 151, "processing_time": 0.04349803924560547, "task_config": {"task_name": "mmlu_high_school_physics:cot", "task_core": "mmlu_high_school_physics", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about high school physics. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "high_school_physics", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_high_school_physics:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_high_school_psychology:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.8972477064220183, "exact_match": 0.8972477064220183, "primary_score": 0.8972477064220183, "extra_metrics": {"num_tokens": 1689.2385321100917, "answer_format_correct": 0.9788990825688073}}, "num_instances": 545, "processing_time": 0.07729864120483398, "task_config": {"task_name": "mmlu_high_school_psychology:cot", "task_core": "mmlu_high_school_psychology", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about high school psychology. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "high_school_psychology", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_high_school_psychology:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_high_school_statistics:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.8472222222222222, "exact_match": 0.8472222222222222, "primary_score": 0.8472222222222222, "extra_metrics": {"num_tokens": 3352.3518518518517, "answer_format_correct": 0.8842592592592593}}, "num_instances": 216, "processing_time": 0.048651695251464844, "task_config": {"task_name": "mmlu_high_school_statistics:cot", "task_core": "mmlu_high_school_statistics", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about high school statistics. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "high_school_statistics", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_high_school_statistics:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_high_school_us_history:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.7696078431372549, "exact_match": 0.7696078431372549, "primary_score": 0.7696078431372549, "extra_metrics": {"num_tokens": 2856.0343137254904, "answer_format_correct": 0.9779411764705882}}, "num_instances": 204, "processing_time": 0.06911993026733398, "task_config": {"task_name": "mmlu_high_school_us_history:cot", "task_core": "mmlu_high_school_us_history", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about high school us history. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "high_school_us_history", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_high_school_us_history:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_high_school_world_history:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.7679324894514767, "exact_match": 0.7721518987341772, "primary_score": 0.7721518987341772, "extra_metrics": {"num_tokens": 2726.4599156118143, "answer_format_correct": 0.9662447257383966}}, "num_instances": 237, "processing_time": 0.06836557388305664, "task_config": {"task_name": "mmlu_high_school_world_history:cot", "task_core": "mmlu_high_school_world_history", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about high school world history. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "high_school_world_history", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_high_school_world_history:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_human_aging:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.7488789237668162, "exact_match": 0.7488789237668162, "primary_score": 0.7488789237668162, "extra_metrics": {"num_tokens": 2333.9282511210763, "answer_format_correct": 0.9820627802690582}}, "num_instances": 223, "processing_time": 0.04734373092651367, "task_config": {"task_name": "mmlu_human_aging:cot", "task_core": "mmlu_human_aging", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about human aging. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "human_aging", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_human_aging:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_human_sexuality:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.8015267175572519, "exact_match": 0.8015267175572519, "primary_score": 0.8015267175572519, "extra_metrics": {"num_tokens": 2163.587786259542, "answer_format_correct": 0.9847328244274809}}, "num_instances": 131, "processing_time": 0.036748647689819336, "task_config": {"task_name": "mmlu_human_sexuality:cot", "task_core": "mmlu_human_sexuality", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about human sexuality. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "human_sexuality", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_human_sexuality:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_international_law:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.7355371900826446, "exact_match": 0.7355371900826446, "primary_score": 0.7355371900826446, "extra_metrics": {"num_tokens": 2415.528925619835, "answer_format_correct": 0.9545454545454546}}, "num_instances": 121, "processing_time": 0.041237592697143555, "task_config": {"task_name": "mmlu_international_law:cot", "task_core": "mmlu_international_law", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about international law. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "international_law", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_international_law:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_jurisprudence:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.7314814814814815, "exact_match": 0.7314814814814815, "primary_score": 0.7314814814814815, "extra_metrics": {"num_tokens": 2546.0555555555557, "answer_format_correct": 0.9722222222222222}}, "num_instances": 108, "processing_time": 0.03743433952331543, "task_config": {"task_name": "mmlu_jurisprudence:cot", "task_core": "mmlu_jurisprudence", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about jurisprudence. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "jurisprudence", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_jurisprudence:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_logical_fallacies:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.754601226993865, "exact_match": 0.754601226993865, "primary_score": 0.754601226993865, "extra_metrics": {"num_tokens": 2779.570552147239, "answer_format_correct": 0.9447852760736196}}, "num_instances": 163, "processing_time": 0.05046582221984863, "task_config": {"task_name": "mmlu_logical_fallacies:cot", "task_core": "mmlu_logical_fallacies", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about logical fallacies. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "logical_fallacies", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_logical_fallacies:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_machine_learning:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.6696428571428571, "exact_match": 0.6696428571428571, "primary_score": 0.6696428571428571, "extra_metrics": {"num_tokens": 2903.8214285714284, "answer_format_correct": 0.8839285714285714}}, "num_instances": 112, "processing_time": 0.03455948829650879, "task_config": {"task_name": "mmlu_machine_learning:cot", "task_core": "mmlu_machine_learning", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about machine learning. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "machine_learning", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_machine_learning:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_management:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.8155339805825242, "exact_match": 0.8155339805825242, "primary_score": 0.8155339805825242, "extra_metrics": {"num_tokens": 2105.009708737864, "answer_format_correct": 0.9854368932038835}}, "num_instances": 103, "processing_time": 0.04600667953491211, "task_config": {"task_name": "mmlu_management:cot", "task_core": "mmlu_management", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about management. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "management", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_management:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_marketing:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.8974358974358975, "exact_match": 0.8974358974358975, "primary_score": 0.8974358974358975, "extra_metrics": {"num_tokens": 1559.7521367521367, "answer_format_correct": 0.9743589743589743}}, "num_instances": 234, "processing_time": 0.046041011810302734, "task_config": {"task_name": "mmlu_marketing:cot", "task_core": "mmlu_marketing", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about marketing. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "marketing", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_marketing:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_medical_genetics:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.85, "exact_match": 0.85, "primary_score": 0.85, "extra_metrics": {"num_tokens": 2790.33, "answer_format_correct": 0.975}}, "num_instances": 100, "processing_time": 0.04847407341003418, "task_config": {"task_name": "mmlu_medical_genetics:cot", "task_core": "mmlu_medical_genetics", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about medical genetics. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "medical_genetics", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_medical_genetics:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_miscellaneous:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.8199233716475096, "exact_match": 0.8199233716475096, "primary_score": 0.8199233716475096, "extra_metrics": {"num_tokens": 2161.537675606641, "answer_format_correct": 0.9763729246487867}}, "num_instances": 783, "processing_time": 0.12232089042663574, "task_config": {"task_name": "mmlu_miscellaneous:cot", "task_core": "mmlu_miscellaneous", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about miscellaneous. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "miscellaneous", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_miscellaneous:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_moral_disputes:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.653179190751445, "exact_match": 0.653179190751445, "primary_score": 0.653179190751445, "extra_metrics": {"num_tokens": 2911.8901734104047, "answer_format_correct": 0.9710982658959537}}, "num_instances": 346, "processing_time": 0.06252408027648926, "task_config": {"task_name": "mmlu_moral_disputes:cot", "task_core": "mmlu_moral_disputes", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about moral disputes. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "moral_disputes", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_moral_disputes:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_moral_scenarios:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.5888268156424581, "exact_match": 0.5899441340782123, "primary_score": 0.5899441340782123, "extra_metrics": {"num_tokens": 2794.7240223463687, "answer_format_correct": 0.9659217877094972}}, "num_instances": 895, "processing_time": 0.14552974700927734, "task_config": {"task_name": "mmlu_moral_scenarios:cot", "task_core": "mmlu_moral_scenarios", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about moral scenarios. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "moral_scenarios", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_moral_scenarios:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_nutrition:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.7745098039215687, "exact_match": 0.7745098039215687, "primary_score": 0.7745098039215687, "extra_metrics": {"num_tokens": 2608.6307189542486, "answer_format_correct": 0.9754901960784313}}, "num_instances": 306, "processing_time": 0.0637664794921875, "task_config": {"task_name": "mmlu_nutrition:cot", "task_core": "mmlu_nutrition", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about nutrition. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "nutrition", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_nutrition:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_philosophy:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.6720257234726688, "exact_match": 0.6720257234726688, "primary_score": 0.6720257234726688, "extra_metrics": {"num_tokens": 2616.906752411576, "answer_format_correct": 0.9903536977491961}}, "num_instances": 311, "processing_time": 0.05569648742675781, "task_config": {"task_name": "mmlu_philosophy:cot", "task_core": "mmlu_philosophy", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about philosophy. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "philosophy", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_philosophy:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_prehistory:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.75, "exact_match": 0.75, "primary_score": 0.75, "extra_metrics": {"num_tokens": 2375.9506172839506, "answer_format_correct": 0.9830246913580247}}, "num_instances": 324, "processing_time": 0.06999373435974121, "task_config": {"task_name": "mmlu_prehistory:cot", "task_core": "mmlu_prehistory", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about prehistory. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "prehistory", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_prehistory:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_professional_accounting:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.6170212765957447, "exact_match": 0.6170212765957447, "primary_score": 0.6170212765957447, "extra_metrics": {"num_tokens": 4131.992907801418, "answer_format_correct": 0.9343971631205674}}, "num_instances": 282, "processing_time": 0.08073067665100098, "task_config": {"task_name": "mmlu_professional_accounting:cot", "task_core": "mmlu_professional_accounting", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about professional accounting. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "professional_accounting", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_professional_accounting:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_professional_law:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.44132985658409385, "exact_match": 0.44784876140808344, "primary_score": 0.44784876140808344, "extra_metrics": {"num_tokens": 4797.628422425033, "answer_format_correct": 0.8895045632333768}}, "num_instances": 1534, "processing_time": 0.2708699703216553, "task_config": {"task_name": "mmlu_professional_law:cot", "task_core": "mmlu_professional_law", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about professional law. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "professional_law", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_professional_law:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_professional_medicine:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.7169117647058824, "exact_match": 0.7169117647058824, "primary_score": 0.7169117647058824, "extra_metrics": {"num_tokens": 3640.768382352941, "answer_format_correct": 0.9761029411764706}}, "num_instances": 272, "processing_time": 0.05981898307800293, "task_config": {"task_name": "mmlu_professional_medicine:cot", "task_core": "mmlu_professional_medicine", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about professional medicine. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "professional_medicine", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_professional_medicine:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_professional_psychology:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.6993464052287581, "exact_match": 0.6993464052287581, "primary_score": 0.6993464052287581, "extra_metrics": {"num_tokens": 2536.1944444444443, "answer_format_correct": 0.9820261437908496}}, "num_instances": 612, "processing_time": 0.09485745429992676, "task_config": {"task_name": "mmlu_professional_psychology:cot", "task_core": "mmlu_professional_psychology", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about professional psychology. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "professional_psychology", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_professional_psychology:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_public_relations:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.6636363636363637, "exact_match": 0.6636363636363637, "primary_score": 0.6636363636363637, "extra_metrics": {"num_tokens": 2203.4454545454546, "answer_format_correct": 0.9772727272727273}}, "num_instances": 110, "processing_time": 0.03336501121520996, "task_config": {"task_name": "mmlu_public_relations:cot", "task_core": "mmlu_public_relations", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about public relations. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "public_relations", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_public_relations:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_security_studies:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.726530612244898, "exact_match": 0.726530612244898, "primary_score": 0.726530612244898, "extra_metrics": {"num_tokens": 2590.8897959183673, "answer_format_correct": 0.9775510204081632}}, "num_instances": 245, "processing_time": 0.054990291595458984, "task_config": {"task_name": "mmlu_security_studies:cot", "task_core": "mmlu_security_studies", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about security studies. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "security_studies", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_security_studies:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_sociology:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.7661691542288557, "exact_match": 0.7661691542288557, "primary_score": 0.7661691542288557, "extra_metrics": {"num_tokens": 1925.0199004975125, "answer_format_correct": 0.9925373134328358}}, "num_instances": 201, "processing_time": 0.043313026428222656, "task_config": {"task_name": "mmlu_sociology:cot", "task_core": "mmlu_sociology", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about sociology. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "sociology", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_sociology:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_us_foreign_policy:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.78, "exact_match": 0.78, "primary_score": 0.78, "extra_metrics": {"num_tokens": 2192.59, "answer_format_correct": 0.97}}, "num_instances": 100, "processing_time": 0.03415107727050781, "task_config": {"task_name": "mmlu_us_foreign_policy:cot", "task_core": "mmlu_us_foreign_policy", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about us foreign policy. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "us_foreign_policy", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_us_foreign_policy:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_virology:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.4939759036144578, "exact_match": 0.4939759036144578, "primary_score": 0.4939759036144578, "extra_metrics": {"num_tokens": 2679.8253012048194, "answer_format_correct": 0.9819277108433735}}, "num_instances": 166, "processing_time": 0.039575815200805664, "task_config": {"task_name": "mmlu_virology:cot", "task_core": "mmlu_virology", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about virology. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "virology", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_virology:cot::hamish_zs_reasoning_deepseek"}}}, {"alias": "mmlu_world_religions:cot::hamish_zs_reasoning_deepseek", "metrics": {"exact_match_simple": 0.7719298245614035, "exact_match": 0.7719298245614035, "primary_score": 0.7719298245614035, "extra_metrics": {"num_tokens": 2500.812865497076, "answer_format_correct": 0.97953216374269}}, "num_instances": 171, "processing_time": 0.04673147201538086, "task_config": {"task_name": "mmlu_world_religions:cot", "task_core": "mmlu_world_religions", "split": "test", "primary_metric": "exact_match", "context_kwargs": {"description": "The following are multiple choice questions about world religions. Summarize your reasoning concisely, then conclude with 'Therefore, the answer is: X' where X is one of A, B, C, or D.\n\n", "fewshot_as_multiturn": false}, "generation_kwargs": {"max_gen_toks": 131072, "do_sample": true, "temperature": 0.6, "stop_sequences": [], "truncate_context": false, "top_p": 0.95}, "metric_kwargs": {"answer_regexes_templates": ["(?i)therefore,?\\s*the\\s*answer\\s*is:?\\s*\\(?($ANS$)\\b", "(?i)so\\s+the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)the\\s+correct\\s+answer\\s+is:?\\s*($ANS$)", "(?i)the\\s+answer\\s+is\\s+($ANS$)\\.?", "(?i)a:\\s*($ANS$)", "(?i)answer:\\s*($ANS$)", "(?i)\\b($ANS$)\\)?\\s+is\\s+correct", "(?i)\\(($ANS$)\\)", "(?i)\\b($ANS$)\\b", "(?i)\\b($ANS$)\\b", "(?i).*\\b($ANS$)\\b"], "answer_regexes": ["\\(?([A-D])\\)?"]}, "native_id_field": "question_id", "dataset_path": "cais/mmlu", "dataset_name": "world_religions", "use_chat_format": true, "version": 0, "metadata": {"alias": "mmlu_world_religions:cot::hamish_zs_reasoning_deepseek"}}}], "model_config": {"model": "hf-OpenThinker3-7B-3", "trust_remote_code": "true", "max_length": 32768, "model_path": "open-thoughts/OpenThinker3-7B", "model_type": "vllm", "process_output": "r1_style"}}