#!/bin/bash
#PBS -P CFP01-CF-060
#PBS -j oe
#PBS -k oed
#PBS -N openseal_sft
#PBS -q auto
#PBS -l select=1:ngpus=8
#PBS -l walltime=96:00:00

cd $PBS_O_WORKDIR

image="/app1/common/singularity-img/hopper/pytorch/pytorch_2.3.0_cuda_12.4_ngc_24.04.sif"

module load singularity

LOG_DIR='./logs/sft/parallel_7b'
mkdir -p $LOG_DIR

singularity exec -e \
--env HF_HOME=/scratch/e1583535/cache \
--env HF_DATASETS_CACHE=/scratch/e1583535/cache/datasets \
--env TRITON_CACHE_DIR=/scratch/e1583535/cache/triton \
$image bash << EOF > $LOG_DIR/stdout.$PBS_JOBID.log 2> $LOG_DIR/stderr.$PBS_JOBID.log

# Ensure the open_instruct package (one level up from the package dir) is on PYTHONPATH
export PYTHONPATH=/scratch/e1583535/open-instruct:${PYTHONPATH}

if [ -d "/hpctmp/e1583535/virtualenvs/open-instruct" ]; then
    source /hpctmp/e1583535/virtualenvs/open-instruct/bin/activate
fi

echo "Starting training at $(date)"
echo "Running on host: $(hostname)"
echo "GPU info:"
nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits || echo "No GPU detected"

# Memory optimization for CUDA
export PYTORCH_ALLOC_CONF=expandable_segments:True

# Set NCCL timeout to 1 hour (from default 30 min)
export NCCL_DEBUG=INFO
export TORCH_NCCL_BLOCKING_WAIT=1
export NCCL_TIMEOUT=3600

export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

accelerate launch \
    --mixed_precision bf16 \
    --multi_gpu \
    --num_processes 8 \
    --num_machines 1 \
    --dynamo_backend no \
    /scratch/e1583535/open-instruct/open_instruct/finetune.py \
    --exp_name ConvertedSeaInstruct_stage1_step1440 \
    --model_name_or_path /scratch/e1583535/llm/nus-olmo/para-only-7B-34B-checkpoints/step8290-unsharded-hf-para-only-7B-34.7B \
    --tokenizer_name allenai/OLMo-2-1124-7B \
    --tokenizer_revision main \
    --output_dir /scratch/e1583535/llm/openseal-sft \
    --use_slow_tokenizer False \
    --chat_template_name olmo \
    --dataset_mixer_list /scratch/e1583535/data/sea-instruct-dataset-messages/sft_stage_1 1.00 \
    --use_flash_attn \
    --gradient_checkpointing \
    --max_seq_length 4096 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 256 \
    --learning_rate 2e-5 \
    --lr_scheduler_type linear \
    --warmup_ratio 0.3 \
    --weight_decay 0.0 \
    --max_train_steps 1440 \
    --logging_steps 100 \
    --seed 42 \
    --verbose True \
    --packing True \
    --timeout 3600 \
    --checkpointing_steps 1000 \
    --keep_last_n_checkpoints 2 \
    --push_to_hub False \
    --try_launch_beaker_eval_jobs False \
    --with_tracking True \
    --report_to wandb \
    --wandb_project_name openseal-posttraining \
    --wandb_entity tsangb34-national-university-of-singapore-students-union 
    
echo "Training completed at $(date)"
EOF