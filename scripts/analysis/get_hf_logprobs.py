"""
Score sequences with HuggingFace and compute logprobs.

This script loads vLLM-generated sequences from JSON files and computes
HuggingFace logprobs with matched precision (bf16 sequences → bf16 HF,
fp32 sequences → fp32 HF).

Usage:
    # Score sequences from vLLM
    uv run python scripts/analysis/get_hf_logprobs.py \
        --bf16-input ~/dev/logprobs_data/vllm_bf16.json \
        --fp32-input ~/dev/logprobs_data/vllm_fp32.json \
        --output ~/dev/logprobs_data/results.json

    # Or use the wrapper script:
    ./scripts/analysis/run_logprobs_comparison.sh --model hamishivi/qwen3_openthoughts2
"""
import argparse
import gc
import json
import time
from pathlib import Path
from typing import List, Dict, Any

import numpy as np
import torch
import transformers

from open_instruct import model_utils


def setup_precision(disable_tf32: bool = True):
    """Configure precision settings."""
    if disable_tf32:
        torch.backends.cuda.matmul.allow_tf32 = False
        torch.backends.cudnn.allow_tf32 = False
        torch.set_float32_matmul_precision("highest")
        print("TF32 disabled for precise fp32 comparisons")


def load_vllm_results(input_path: Path) -> Dict[str, Any]:
    """Load vLLM results from JSON."""
    with open(input_path) as f:
        return json.load(f)


def compute_hf_logprobs_single(
    model,
    query: List[int],
    response: List[int],
    use_fp32_lm_head: bool,
) -> List[float]:
    """Compute logprobs for a single sequence."""
    # Concatenate query and response
    full_sequence = query + response
    input_ids = torch.tensor(full_sequence, dtype=torch.long, device="cuda")[None, :]

    # Create attention mask and position ids
    attention_mask = torch.ones_like(input_ids)
    position_ids = torch.arange(len(full_sequence), device="cuda")[None, :]

    with torch.inference_mode():
        # Get hidden states from base model
        base_output = model.model(
            input_ids=input_ids[:, :-1],
            attention_mask=attention_mask[:, :-1],
            position_ids=position_ids[:, :-1],
            return_dict=True,
        )
        hidden_states = base_output.last_hidden_state

        # Compute LM head
        if use_fp32_lm_head:
            hidden_fp32 = hidden_states.float()
            w = model.lm_head.weight.float()
            b = model.lm_head.bias.float() if getattr(model.lm_head, "bias", None) is not None else None
            logits = torch.nn.functional.linear(hidden_fp32, w, b)
        else:
            # BF16 head
            w = model.lm_head.weight
            b = model.lm_head.bias if getattr(model.lm_head, "bias", None) is not None else None
            logits = torch.nn.functional.linear(hidden_states, w, b)

        # Always do log_softmax in fp32
        logits = logits.to(torch.float32)
        logprobs_all = model_utils.log_softmax_and_gather(logits, input_ids[:, 1:])

        # Slice exactly len(response) tokens starting after query
        start = len(query) - 1  # -1 because we predict next token
        end = start + len(response)
        logprobs = logprobs_all[:, start:end]

    return logprobs[0].tolist()


def get_hf_logprobs(
    model,
    vllm_results: List[Dict],
    use_fp32_lm_head: bool,
) -> List[List[float]]:
    """
    Compute HF logprobs for sequences generated by vLLM.

    Args:
        model: HuggingFace model
        vllm_results: List of vLLM results with query, response, logprobs
        use_fp32_lm_head: If True, compute LM head in fp32
    """
    head_mode = "fp32" if use_fp32_lm_head else "bf16"
    all_hf_logprobs = []

    for i, result in enumerate(vllm_results):
        print(f"  [{i+1}/{len(vllm_results)}] Computing HF logprobs ({head_mode})...")

        query = result["query"]
        response = result["response"]

        hf_logprobs = compute_hf_logprobs_single(
            model, query, response, use_fp32_lm_head
        )

        # Verify length match
        if len(hf_logprobs) != len(response):
            raise RuntimeError(
                f"Length mismatch: HF={len(hf_logprobs)}, vLLM={len(response)}"
            )

        all_hf_logprobs.append(hf_logprobs)

        # Show per-prompt stats
        vllm_lp = np.array(result["logprobs"])
        hf_lp = np.array(hf_logprobs)
        diff = np.abs(vllm_lp - hf_lp)
        print(f"    Tokens: {len(response)}, Mean |diff|: {diff.mean():.6f}, Max: {diff.max():.6f}")

    return all_hf_logprobs


def save_combined_results(
    output_path: Path,
    bf16_data: Dict[str, Any],
    fp32_data: Dict[str, Any],
    hf_bf16_logprobs: List[List[float]],
    hf_fp32_logprobs: List[List[float]],
    metadata: Dict[str, Any],
):
    """Save combined results in format compatible with plot_logprobs.py."""
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # Extract prompts from results
    prompts = [r["prompt"] for r in bf16_data["results"]]

    # Build comparisons dict matching original format
    comparisons = {
        "vllm_bf16": [r["logprobs"] for r in bf16_data["results"]],
        "vllm_fp32": [r["logprobs"] for r in fp32_data["results"]],
        "hf_bf16": hf_bf16_logprobs,
        "hf_fp32": hf_fp32_logprobs,
    }

    data = {
        "config": bf16_data["config"],  # Use bf16 config (should be same except mode)
        "metadata": metadata,
        "prompts": prompts,
        "vllm_bf16_results": bf16_data["results"],
        "vllm_fp32_results": fp32_data["results"],
        "comparisons": comparisons,
    }

    with open(output_path, "w") as f:
        json.dump(data, f, indent=2)

    print(f"\nSaved combined results to {output_path}")
    return output_path


def main():
    parser = argparse.ArgumentParser(description="Score vLLM sequences with HuggingFace")
    parser.add_argument("--bf16-input", type=str, required=True,
                        help="Input JSON from vLLM bf16 generation")
    parser.add_argument("--fp32-input", type=str, required=True,
                        help="Input JSON from vLLM fp32 generation")
    parser.add_argument("--output", type=str, required=True,
                        help="Output JSON with combined results")
    parser.add_argument("--attn-implementation", type=str, default="sdpa")
    parser.add_argument("--allow-tf32", action="store_true", help="Allow TF32 (less precise)")
    args = parser.parse_args()

    setup_precision(disable_tf32=not args.allow_tf32)

    # Load vLLM results
    print("Loading vLLM results...")
    bf16_data = load_vllm_results(Path(args.bf16_input))
    fp32_data = load_vllm_results(Path(args.fp32_input))

    # Verify configs match (except mode)
    bf16_config = bf16_data["config"]
    fp32_config = fp32_data["config"]
    if bf16_config["model"] != fp32_config["model"]:
        raise ValueError(f"Model mismatch: {bf16_config['model']} vs {fp32_config['model']}")

    model_name = bf16_config["model"]
    dtype = bf16_config["dtype"]

    print("=" * 60)
    print("HuggingFace Logprobs Scoring")
    print(f"Model: {model_name}")
    print(f"BF16 sequences: {len(bf16_data['results'])}")
    print(f"FP32 sequences: {len(fp32_data['results'])}")
    print("=" * 60)

    # Load HF model once
    print(f"\nLoading HF model...")
    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)
    model = transformers.AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=getattr(torch, dtype),
        device_map="cuda",
        attn_implementation=args.attn_implementation,
        use_cache=False,
    )

    metadata = {
        "torch_version": torch.__version__,
        "transformers_version": transformers.__version__,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "bf16_source": args.bf16_input,
        "fp32_source": args.fp32_input,
    }

    # Score bf16 sequences with bf16 HF head
    print("\n" + "=" * 60)
    print("Scoring BF16 sequences with BF16 HF head")
    print("=" * 60)
    hf_bf16_logprobs = get_hf_logprobs(model, bf16_data["results"], use_fp32_lm_head=False)

    # Score fp32 sequences with fp32 HF head
    print("\n" + "=" * 60)
    print("Scoring FP32 sequences with FP32 HF head")
    print("=" * 60)
    hf_fp32_logprobs = get_hf_logprobs(model, fp32_data["results"], use_fp32_lm_head=True)

    # Cleanup
    del model
    gc.collect()
    torch.cuda.empty_cache()

    # Save combined results
    save_combined_results(
        Path(args.output),
        bf16_data,
        fp32_data,
        hf_bf16_logprobs,
        hf_fp32_logprobs,
        metadata,
    )

    # Print summary
    print("\n" + "=" * 60)
    print("SUMMARY")
    print("=" * 60)

    vllm_bf16_lp = np.concatenate([r["logprobs"] for r in bf16_data["results"]])
    vllm_fp32_lp = np.concatenate([r["logprobs"] for r in fp32_data["results"]])
    hf_bf16_lp = np.concatenate(hf_bf16_logprobs)
    hf_fp32_lp = np.concatenate(hf_fp32_logprobs)

    diff_bf16 = np.abs(vllm_bf16_lp - hf_bf16_lp)
    diff_fp32 = np.abs(vllm_fp32_lp - hf_fp32_lp)

    print(f"\nvLLM BF16 vs HF BF16 (baseline):")
    print(f"  Mean |diff|: {diff_bf16.mean():.6f}")
    print(f"  Max |diff|:  {diff_bf16.max():.6f}")
    print(f"  n_tokens:    {len(vllm_bf16_lp)}")

    print(f"\nvLLM FP32 vs HF FP32 (with fix):")
    print(f"  Mean |diff|: {diff_fp32.mean():.6f}")
    print(f"  Max |diff|:  {diff_fp32.max():.6f}")
    print(f"  n_tokens:    {len(vllm_fp32_lp)}")

    if diff_bf16.mean() > 0:
        improvement = (diff_bf16.mean() - diff_fp32.mean()) / diff_bf16.mean() * 100
        print(f"\nImprovement: {improvement:.1f}% reduction in mean |diff| with FP32")


if __name__ == "__main__":
    main()
