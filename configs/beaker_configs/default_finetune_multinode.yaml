# DEPRECATED: This config uses the old HF Accelerate-based finetune.py.
# The new finetune.py uses OLMo-core and has built-in Beaker launch support:
#   python open_instruct/finetune.py launch <run_name> <checkpoint> <cluster> \
#       --dataset_path <path> --num_nodes 8 --budget <budget> --workspace <workspace>
#
# See scripts/train/olmo2/*.sh for example usage.

version: v2
description: open-instruct-finetune-multinode-DEPRECATED
budget: ai2/oe-adapt
tasks:
  - name: open-instruct-finetune-multinode-DEPRECATED
    replicas: 4
    leaderSelection: true
    hostNetworking: true
    propagateFailure: true
    propagatePreemption: true
    synchronizedStartTimeout: 60m
    image:
      beaker: nathanl/open_instruct_auto
    command: [
      '/bin/sh', '-c'
    ]
    arguments: ['echo "This config is deprecated. Use: python open_instruct/finetune.py launch" && exit 1']
    envVars:
      - name: CUDA_DEVICE_ORDER
        value: PCI_BUS_ID
      - name: WANDB_API_KEY
        secret: WANDB_API_KEY
      - name: HF_TOKEN
        secret: HF_TOKEN
    datasets:
      - mountPath: /oe-adapt-default
        source:
          weka: oe-adapt-default
    result:
      path: /output
    resources:
      gpuCount: 8
    context:
      priority: normal
      preemptible: true
